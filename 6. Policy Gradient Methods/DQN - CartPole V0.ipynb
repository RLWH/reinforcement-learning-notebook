{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Cartpole v0 by DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0228608  -0.18481121 -0.00115526  0.3098936 ]\n",
      "[-0.02655703  0.01032718  0.00504261  0.01684656]\n",
      "[-0.02635048  0.20537645  0.00537954 -0.27424111]\n",
      "[-2.22429525e-02  4.00421238e-01 -1.05280207e-04 -5.65222474e-01]\n",
      "[-0.01423453  0.59554467 -0.01140973 -0.85793857]\n",
      "[-0.00232363  0.79082018 -0.0285685  -1.15418716]\n",
      "[ 0.01349277  0.59608222 -0.05165224 -0.87059742]\n",
      "[ 0.02541441  0.40169946 -0.06906419 -0.59459115]\n",
      "[ 0.0334484   0.20760868 -0.08095602 -0.32443777]\n",
      "[ 0.03760058  0.40378441 -0.08744477 -0.64151269]\n",
      "[ 0.04567626  0.20998326 -0.10027502 -0.3775977 ]\n",
      "[ 0.04987593  0.01641779 -0.10782698 -0.11813949]\n",
      "[ 0.05020429  0.2129062  -0.11018977 -0.44280013]\n",
      "[ 0.05446241  0.40940077 -0.11904577 -0.76808472]\n",
      "[ 0.06265042  0.60594272 -0.13440747 -1.09572794]\n",
      "[ 0.07476928  0.80255385 -0.15632202 -1.42738167]\n",
      "[ 0.09082036  0.60967006 -0.18486966 -1.18735559]\n",
      "[ 0.10301376  0.8066435  -0.20861677 -1.53182377]\n",
      "[ 0.11914663  1.00357906 -0.23925324 -1.88171169]\n",
      "Episode finished\n",
      "[ 0.01659897  0.16223202  0.02203078 -0.31599195]\n",
      "[ 0.01984361  0.35703335  0.01571094 -0.60164651]\n",
      "[ 0.02698428  0.55193205  0.00367801 -0.88933965]\n",
      "[ 0.03802292  0.7470039  -0.01410878 -1.18086411]\n",
      "[ 0.052963    0.94230613 -0.03772607 -1.47793614]\n",
      "[ 0.07180912  0.74766459 -0.06728479 -1.19727042]\n",
      "[ 0.08676241  0.94358977 -0.0912302  -1.51025998]\n",
      "[ 0.10563421  1.13969102 -0.1214354  -1.8299731 ]\n",
      "[ 0.12842803  1.33593099 -0.15803486 -2.15777964]\n",
      "[ 0.15514665  1.14267329 -0.20119045 -1.91777462]\n",
      "[ 0.17800011  1.33931175 -0.23954594 -2.26553459]\n",
      "Episode finished\n",
      "[ 0.042726   -0.23202821  0.04395938  0.33851093]\n",
      "[ 0.03808543 -0.42774722  0.0507296   0.64472573]\n",
      "[ 0.02953049 -0.23336757  0.06362412  0.36843945]\n",
      "[ 0.02486314 -0.03920463  0.07099291  0.0964772 ]\n",
      "[ 0.02407905 -0.23526848  0.07292245  0.41068655]\n",
      "[ 0.01937368 -0.04125201  0.08113618  0.1418555 ]\n",
      "[ 0.01854864 -0.23743659  0.08397329  0.45899215]\n",
      "[ 0.0137999  -0.04359582  0.09315313  0.19391469]\n",
      "[ 0.01292799  0.15007869  0.09703143 -0.06799009]\n",
      "[ 0.01592956 -0.04629083  0.09567163  0.25366113]\n",
      "[ 0.01500374  0.14734412  0.10074485 -0.00737847]\n",
      "[ 0.01795063  0.34088775  0.10059728 -0.26665298]\n",
      "[ 0.02476838  0.53444083  0.09526422 -0.52598905]\n",
      "[ 0.0354572   0.33811655  0.08474444 -0.20487256]\n",
      "[ 0.04221953  0.53193082  0.08064699 -0.4696656 ]\n",
      "[ 0.05285815  0.72582645  0.07125368 -0.73587922]\n",
      "[ 0.06737468  0.52979638  0.05653609 -0.42164939]\n",
      "[ 0.0779706   0.72407366  0.0481031  -0.69598695]\n",
      "[ 0.09245208  0.52831875  0.03418336 -0.38855737]\n",
      "[ 0.10301845  0.33272869  0.02641222 -0.08529572]\n",
      "[ 0.10967302  0.52746228  0.0247063  -0.36952994]\n",
      "[ 0.12022227  0.72222464  0.0173157  -0.65432141]\n",
      "[ 0.13466676  0.91710127  0.00422928 -0.94150197]\n",
      "[ 0.15300879  1.11216597 -0.01460076 -1.232853  ]\n",
      "[ 0.17525211  1.30747259 -0.03925782 -1.53007417]\n",
      "[ 0.20140156  1.11284537 -0.06985931 -1.24989712]\n",
      "[ 0.22365847  0.91868488 -0.09485725 -0.9798887 ]\n",
      "[ 0.24203216  1.11494152 -0.11445502 -1.30079647]\n",
      "[ 0.26433099  1.31131469 -0.14047095 -1.62700532]\n",
      "[ 0.29055729  1.50778142 -0.17301106 -1.95996544]\n",
      "[ 0.32071292  1.31486463 -0.21221037 -1.72552226]\n",
      "Episode finished\n",
      "[-0.01451825  0.22732954  0.02250775 -0.30299676]\n",
      "[-0.00997166  0.4221236   0.01644781 -0.58849713]\n",
      "[-0.00152919  0.61701141  0.00467787 -0.87595386]\n",
      "[ 0.01081104  0.42182618 -0.01284121 -0.58180394]\n",
      "[ 0.01924756  0.61712568 -0.02447729 -0.87850424]\n",
      "[ 0.03159007  0.42234474 -0.04204737 -0.59361595]\n",
      "[ 0.04003697  0.22783579 -0.05391969 -0.3144687 ]\n",
      "[ 0.04459369  0.42368271 -0.06020906 -0.62365685]\n",
      "[ 0.05306734  0.61959133 -0.0726822  -0.93467806]\n",
      "[ 0.06545917  0.81561445 -0.09137576 -1.24928619]\n",
      "[ 0.08177145  0.62177483 -0.11636149 -0.98656688]\n",
      "[ 0.09420695  0.42838714 -0.13609282 -0.73258011]\n",
      "[ 0.10277469  0.23538187 -0.15074443 -0.48563698]\n",
      "[ 0.10748233  0.04267266 -0.16045716 -0.24400248]\n",
      "[ 0.10833578 -0.14983711 -0.16533721 -0.00591943]\n",
      "[ 0.10533904 -0.3422495  -0.1654556   0.23037525]\n",
      "[ 0.09849405 -0.14519739 -0.1608481  -0.10958863]\n",
      "[ 0.0955901  -0.33769273 -0.16303987  0.12834154]\n",
      "[ 0.08883625 -0.14065623 -0.16047304 -0.21101533]\n",
      "[ 0.08602313  0.05635285 -0.16469335 -0.54970898]\n",
      "[ 0.08715018 -0.13611942 -0.17568753 -0.31310844]\n",
      "[ 0.08442779  0.06101325 -0.18194969 -0.65564244]\n",
      "[ 0.08564806  0.25813926 -0.19506254 -0.999649  ]\n",
      "[ 0.09081084  0.06608335 -0.21505552 -0.7740139 ]\n",
      "Episode finished\n",
      "[-0.01562147  0.22004666 -0.04551235 -0.29594721]\n",
      "[-0.01122054  0.02560208 -0.05143129 -0.01795838]\n",
      "[-0.01070849  0.22142245 -0.05179046 -0.32641467]\n",
      "[-0.00628005  0.41724204 -0.05831875 -0.63496981]\n",
      "[ 0.00206479  0.22297994 -0.07101815 -0.36120803]\n",
      "[ 0.00652439  0.41903571 -0.07824231 -0.67541217]\n",
      "[ 0.01490511  0.61515274 -0.09175055 -0.9916675 ]\n",
      "[ 0.02720816  0.81137472 -0.1115839  -1.31169931]\n",
      "[ 0.04343566  1.00771865 -0.13781789 -1.63712122]\n",
      "[ 0.06359003  1.20416166 -0.17056031 -1.96938209]\n",
      "[ 0.08767326  1.01120314 -0.20994796 -1.73404607]\n",
      "Episode finished\n",
      "[ 0.00451468  0.16420736  0.01115175 -0.32318058]\n",
      "[ 0.00779883  0.35916875  0.00468814 -0.61232594]\n",
      "[ 0.0149822   0.16398159 -0.00755838 -0.31817011]\n",
      "[ 0.01826183  0.35921038 -0.01392178 -0.61322707]\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "    env.render()\n",
    "    action = env.action_space.sample() # your agent here (this takes random actions)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    \n",
    "    print(observation)\n",
    "\n",
    "    if done:\n",
    "        print(\"Episode finished\")\n",
    "        observation = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 - Solving by using DQN with $\\epsilon$-greedy policy\n",
    "\n",
    "Checklist:\n",
    "1. Objective function\n",
    "2. Preprocess data\n",
    "3. Samples generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space Box(4,)\n",
      "Action space Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "# Environment understanding\n",
    "print(\"State space\", env.observation_space)\n",
    "print(\"Action space\", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The policy network\n",
    "The network takes in the state of the game and decide what we should do. \n",
    "\n",
    "For simplicity, use a simple 2-layer NN that takes in the observations and then produce a single number indicating the probability of pushing LEFT or RIGHT. It is standard to use a stochastic policy, meaning that the NN will only produce a probability of each action. \n",
    "\n",
    "We are going to train our model with a single experience:\n",
    "1. Let the model estimate Q values of the old state\n",
    "2. Let the model estimate Q values of the new state\n",
    "3. Calculate the new target Q value for the action, using the known reward\n",
    "4. Train the model with input = (old state), output = (target Q values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.01\n",
    "EPSILON = 0.9\n",
    "GAMMA = 0.9\n",
    "C = 100  # Update the network parameters every C iteration\n",
    "MEMORY_CAPACITY = 2000  # Capacity of experience replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\"Experience\", \"s a r s_ done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = deque(maxlen=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list.append(Experience(1, 2, 1, 3, False))\n",
    "test_list.append(Experience(1, 2, 1, 3, False))\n",
    "test_list.append(Experience(1, 2, 1, 3, False))\n",
    "test_list.append(Experience(1, 2, 1, 3, False))\n",
    "test_list.append(Experience(1, 2, 1, 3, False))\n",
    "test_list.append(Experience(1, 2, 2, 4, False))\n",
    "test_list.append(Experience(1, 3, 1, 5, False))\n",
    "test_list.append(Experience(1, 4, 1, 6, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_ls, a_ls, r_ls, s__ls, done_list = zip(*sample(test_list, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://morvanzhou.github.io/tutorials/machine-learning/torch/4-05-DQN/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\"Experience\", \"s a r s_ done\")\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    A Deep Q learning agent\n",
    "    https://towardsdatascience.com/reinforcement-learning-tutorial-part-3-basic-deep-q-learning-186164c3bf4\n",
    "    https://morvanzhou.github.io/tutorials/machine-learning/torch/4-05-DQN/\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env, network, learning_rate=0.1, gamma=0.95, epsilon=1.0):\n",
    "        \n",
    "        # Environment parameters\n",
    "        n_actions = env.action_space.n\n",
    "        n_state = env.observation_space.shape[0]\n",
    "        \n",
    "        # NN - Q function with random parameters\n",
    "        self.eval_net, self.target_net = network(), network()\n",
    "        \n",
    "        # Experience replay\n",
    "        self.experience_memory = deque(maxlen=MEMORY_CAPACITY)\n",
    "        \n",
    "        # Other parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def choose_action(self, x)\n",
    "        \"\"\"\n",
    "        Epsilon Greedy Policy\n",
    "        if eps = 0 -> Greedy policy\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Variable): features\n",
    "        \"\"\"\n",
    "        if random.uniform(0, 1) < self.eps:\n",
    "            return self.action_space.sample()\n",
    "        else:\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            action = torch.max(action_value, 1)[1].data.numpy()[0, 0]\n",
    "            return action\n",
    "    \n",
    "    def store_experience(self, s, a, r, s_pi):\n",
    "        self.experience_memory.append(Experience(s, a, r, s_pi))\n",
    "    \n",
    "    def learn(self):\n",
    "        \n",
    "        # Sample the data from experience memory\n",
    "        sample_values = sample(self.experience_memory, BATCH_SIZE)\n",
    "        b_s, b_a, b_r, b_s_pi, b_done = zip(*sample_values)\n",
    "        \n",
    "        b_s_torch = torch.FloatTensor(b_s)\n",
    "        b_a_torch = torch.FloatTensor(b_a)\n",
    "        b_r_torch = torch.FloatTensor(b_r)\n",
    "        b_s_pi_torch = torch.FloatTensor(b_s_pi)\n",
    "        \n",
    "        # Calculate the new Q value\n",
    "        \n",
    "        \n",
    "        pass\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(qs, env, eps):\n",
    "    \"\"\"\n",
    "    Epsilon Greedy Policy\n",
    "    if eps = 0 -> Greedy policy\n",
    "    \"\"\"\n",
    "    if random.uniform(0, 1) < eps:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return np.argmax(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \"\"\"\n",
    "    Policy Network\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(4, 12)\n",
    "        self.fc2 = nn.Linear(12, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        Essentially, the forward pass return the Q value\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "def loss_fn(output, labels):\n",
    "    \"\"\"\n",
    "    Compute the loss given outputs and labels\n",
    "    \n",
    "    Args:\n",
    "        outputs (Variable)\n",
    "        labels (Variable)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def some_measurement(outputs, labels):\n",
    "    \"\"\"\n",
    "    Compute the performance measurement, given the outputs and labels for all images\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(old_state, action, reward, new_state):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
