{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Policy Gradient Methods.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RLWH/reinforcement-learning-notebook/blob/master/6.%20Policy%20Gradient%20Methods/Policy_Gradient_Methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XqtpSFdtv-A",
        "colab_type": "text"
      },
      "source": [
        "# Policy Gradient Methods\n",
        "\n",
        "In previous chapters, we have learnt almost all the methods that have been used action-value methods. They learned the values of actions and then selected actions based on their estimated action values. \n",
        "\n",
        "Instead, we canlearn a *parameterized policy* that can select actions without consulting a value function. Why?\n",
        "\n",
        "- Some of the optimal policies are not a deterministic policy. It can be a stochastic policy.\n",
        "- Using a parameterized policy can solve large-scale problem, as it uses approximation method.\n",
        "\n",
        "#### What is a Parameterized policy?\n",
        "A parameterized policy is a policy that takes in a parameter vector, denote as $\\vec{\\theta} \\in \\mathop{\\mathbb{R}}^{d'}$ for approximation. \n",
        "\n",
        "##### The Parameterized policy formal representation\n",
        "- Denote $\\vec{\\theta} \\in \\mathop{\\mathbb{R}}^{d'} $ for the policy's parameter vector, and \n",
        "- Denote\n",
        "$\\pi(a|s, \\vec{\\theta}) = \\Pr\\{A_t=a | S_t=s, \\vec{\\theta}_t=\\vec{\\theta}\\}$\n",
        "for the probability that action $a$ is taken at time $t$, given that the environment is in state $s$ at time $t$ with parameter $\\vec{\\theta}$. \n",
        "- If a method uses a learned value function as well, then the value function's weight vector is denoted $\\vec{w} \\in \\mathop{\\mathbb{R}}^d$, as in $\\hat{v}(s, \\vec{w})$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX5ixgzk5-6F",
        "colab_type": "text"
      },
      "source": [
        "## Why policy approximation?\n",
        "In practice, to ensure exploration, we generally require that the policy never becomes deterministic. \n",
        "\n",
        "If a policy can be parameterized in any way, as long as the policy $\\pi(a|s, \\vec{\\theta})$ is differentiable w.r.t its parameters, then some optimality can be found. \n",
        "\n",
        "#### Advantages\n",
        "- Better convergence properties\n",
        "- Effective in high-dimensional or continuous action spaces\n",
        "- Can learn stochastic policies\n",
        "\n",
        "#### Disadvantages\n",
        "- Typically converge to a local rather than global optimum\n",
        "- Evaluating a policy is typically inefficient and high variance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDdyAsHYawxf",
        "colab_type": "text"
      },
      "source": [
        "# 1. The objective function - Performance Measure $J(\\vec{\\theta})$\n",
        "How we learn and what are we learning for the policy?\n",
        "\n",
        "The key element of learning the policy parameter is based on the gradient of performance measure $J(\\vec{\\theta})$\n",
        "\n",
        "The policy learning parameter is based on the gradient of some **scalar performance measure**, $J(\\vec{\\theta})$ w.r.t the policy parameters $\\vec{\\theta}$. \n",
        "\n",
        "#### Optimisation base on Gradient Ascent\n",
        "These methods seek to maximise performance, so their updates approximate gradient *ascent* in $J$:\n",
        "\\begin{equation}\n",
        "\\vec{\\theta}_{t+1} = \\vec{\\theta}_t + \\alpha \\widehat{\\nabla J(\\vec{\\theta}_t)}\n",
        "\\end{equation},\n",
        "where $\\widehat{\\nabla J(\\vec{\\theta}_t)} \\in \\mathop{\\mathbb{R}}^d'$ is a stochastic estimate whose expectation approximates the gradient of the performance measure w.r.t its argument $\\vec{\\theta}_t$\n",
        "\n",
        "For all the methods that follow this general schema, we call them *policy gradient methods*. \n",
        "\n",
        "#### Actor-Critic?\n",
        "For the methods that learn approximations to both policy and value functions are often called *actor-critic methods*, where *'actor'* is a reference to the learned policy, and *'critic'* refers to the learned value function, usually a state-value function. This will be covered later\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjaTyOy7OdJ2",
        "colab_type": "text"
      },
      "source": [
        "# 2. The Policy Objective Functions\n",
        "\n",
        "The policy performance measurement function $J(\\vec{\\theta})$ is different in episodic environments and continuing environments. \n",
        "\n",
        "## Problem definition\n",
        "- Goal: Given policy $\\pi_{\\theta}(s, a)$ and parameter $\\vec{\\theta}$, find the best $\\vec{\\theta}$ -> Performance measure function. i.e. Miximising the performance measure $J(\\vec{\\theta})$\n",
        "- The quality of the policy $\\pi_{\\theta}$ is measured by the performance measure function $J(\\vec{\\theta})$\n",
        "- The setup of function $J(\\vec{\\theta})$ is different for episodic environment and continuing environment\n",
        "\n",
        "## The formulation of $J(\\vec{\\theta})$\n",
        "### Episodic environments\n",
        "For Episodic environments, we define the *performance measure* $J(\\vec{\\theta})$ as the value of the start state of the episode\n",
        "\\begin{equation}\n",
        "J_1(\\theta) = V_{\\pi_{\\theta}}(s_1) = \\mathop{\\mathbb{E}}[v_1]\n",
        "\\end{equation}\n",
        "where $v_{\\pi_{\\theta}}(s_1)$ is the true value function for $\\pi_{\\theta}$, the policy determined by $\\vec{\\theta}$. \n",
        "\n",
        "This essentially means that we want the value from the beginning state to be as high as possible, so from this start state I can have a good policy\n",
        "\n",
        "### Continuing environments\n",
        "For Continuing environments, we define *performance measure* $J(\\vec{\\theta})$ as the **average value**\n",
        "\\begin{equation}\n",
        "J_{\\text{avV}}(\\theta) = \\sum_{s}\\mu^{\\pi_{\\theta}}(s) V^{\\pi_{\\theta}}(s)\n",
        "\\end{equation}\n",
        "where $\\mu_{\\pi}(s) = p(S_t=s | \\pi)$ is the probability of being in state s in the long run\n",
        "\n",
        "OR the **average reward per time-step**\n",
        "\\begin{equation}\n",
        "J_{avR}(\\theta) = \\sum_s{\\mu_{\\pi_{\\theta}}(s)} \\sum_{a}\\pi_{\\theta}(s, a) \\sum_{r}p(r|s, a)r\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOQpQ5R0lgxk",
        "colab_type": "text"
      },
      "source": [
        "# 3. Policy Optimisation\n",
        "\n",
        "- Let $J(\\vec{\\theta})$ be any policy objective function\n",
        "- Policy gradient algorithms search for a local maximum in $J(\\vec{\\theta})$ by gradient ascent of the policy w.r.t to parameters $\\vec{\\theta}$\n",
        "\n",
        "\\begin{equation}\n",
        "\\Delta \\vec{\\theta} = \\alpha \\nabla_{\\theta} J(\\vec{\\theta})\n",
        "\\end{equation}\n",
        "- where $\\nabla_{\\theta}J(\\theta)$ is the policy gradient\n",
        "- and $\\alpha$ is a step-size parameter\n",
        "\n",
        "#### Computing an estimate of the policy gradient\n",
        "- Assume policy $\\pi_{\\theta}$ is differentiable almost everywhere\n",
        "- The goal is to compute $\\nabla_{\\theta}J(\\vec{\\theta}) = \\nabla_{\\theta} \\mathop{\\mathbb{E_\\mu}}[v_{\\pi_{\\theta}}(S)]$\n",
        "- We will use Monte Carlo samples to compute this gradient\n",
        "- Gradient ascent can be optimised by optimizers in TF in practice\n",
        "\n",
        "The policy gradient can be calculated computationally or analytically.\n",
        "\n",
        "#### Computing Gradients By Finite Differences\n",
        "- If there is no access to the policy gradient, we can use computational method $J(\\vec{\\theta} + \\mu) - J(\\vec{\\theta})$ to approximate the gradient\n",
        "\n",
        "#### Evaluating the differentiable policy gradients analytically\n",
        "\n",
        "As we defined that $J(\\vec{\\theta})$ depends on the state-value function $v_{\\pi_{\\theta}}$, which means it depends on both the action selections and the distribution of states in which those selections are made, and that both of these are affected by the policy parameter. \n",
        "\n",
        "But how can we differentiate the state-value function? The effect of the policy on the state distribution is a function of the environment, and it is typically unknown. So, how can we estimate the performance gradient $\\nabla J(\\vec{\\theta})$ w.r.t to the policy parameter when the gradient depends on the unknown effect of policy changes on the state distribution?\n",
        "\n",
        "Fortunately, as long as the policy itself is differentiable, we can use the *policy gradient theorem* to obtain an analytic expression for the gradient of performance measure. \n",
        "\n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGN1-Rk4hM7j",
        "colab_type": "text"
      },
      "source": [
        "## One -step MDP (Contextual Bandit problem) Gradient of performance measure\n",
        "Consider a one-step MDP (Contextual Bandit problem),  the performance measure is the expected reward that we can get $J(\\vec{\\theta}) = \\mathop{\\mathbb{E}}[R(S,A)]$\n",
        "- We want to calculate the gradient of $J(\\theta)$, where $\\nabla_{\\theta}J(\\vec{\\theta}) = \\nabla_{\\theta} \\mathop{\\mathbb{E_\\mu}}[v_{\\pi_{\\theta}}(S)]$. \n",
        "- Now, we use the identity $\\nabla_{\\theta} \\mathop{\\mathbb{E}}[R(S, A)] = \\mathop{\\mathbb{E}}[\\nabla_{\\theta} \\log \\pi_{\\theta} (A|S) R(S,A)]$\n",
        "- Then, the right-hand side gives an expected gradient that can be sampled\n",
        "- Then, under stochastic policy-gradient update, we then have\n",
        "\n",
        "\\begin{equation}\n",
        "\\theta_{t+1} = \\theta_t + \\alpha R_{t+1} \\nabla_{\\theta} \\log \\pi_{\\theta_t}(A_t | S_t)\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "## Proof of the identity - The score function trick\n",
        "- Assume the policy $\\pi(\\theta)$ is differentiable whenever it is non-zero\n",
        "- Assume the gradient $\\nabla_{\\theta}\\pi_{\\theta}$ is known\n",
        "- The likelihood ratios exploit the following identity\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "\\nabla_{\\theta} \\mathop{\\mathbb{E}}[R(S, A)] &= \\nabla_{\\theta} \\sum_s{\\mu (s)}\\sum_a \\pi_{\\theta}(a|s)R(s,a) \\\\\n",
        "& = \\sum_s{\\mu(s)}\\sum_a \\nabla_{\\theta}\\pi_{\\theta}(a|s)R(s,a) \\\\\n",
        "& = \\sum_s \\mu(s) \\sum_a \\pi_{\\theta}(a|s) \\frac{\\nabla_{\\theta} \\pi_{\\theta} (a|s)}{\\pi_{\\theta}(a|s)}R(s,a) \\\\\n",
        "& = \\sum_s \\mu(s) \\sum_a \\pi_{\\theta}(a|s) \\nabla_{\\theta} \\log \\pi_{\\theta}(a|s) R(s,a) \\\\\n",
        "& = \\mathop{\\mathbb{E}}[\\nabla_{\\theta} \\log \\pi_{\\theta} (A|S) R(S,A)]\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "- And the score function is $\\nabla_{\\theta} \\log \\pi_{\\theta}(s, a)$, which is the log likelihood"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9I0hih80srv",
        "colab_type": "text"
      },
      "source": [
        "## Score function under the softmax policy\n",
        "One of the differentiable policies is the softmax policy\n",
        "\n",
        "The actions with the highest preferences in each state are given the highest probabilities of being selected, for example, according to a softmax distribution:\n",
        "\\begin{equation}\n",
        "\\pi(a|s, \\vec{\\theta}) = \\frac{e^{h(s, a, \\vec{\\theta})}}{\\sum_{b}{e^{h(s, b, \\vec{\\theta})}}}\n",
        "\\end{equation}\n",
        ", where the function $h(s, a, \\vec{\\theta})$ is the parameterized state-action preference.\n",
        "\n",
        "We call that this kind of policy parameterization as softmax in action preferences\n",
        "\n",
        "The action preferences themselves can be parameterized arbitrarily, for example using a deep artificial neural network (ANN), or it could simply be linear in features. For example:\n",
        "\n",
        "\\begin{equation}\n",
        "h(s, a, \\vec{\\theta}) = \\vec{\\theta}^T \\vec{x}(s, a)\n",
        "\\end{equation},\n",
        "where $\\vec{x}(s, a) \\in \\mathop{\\mathbb{R}}^d'$ constructed by any methods described in Section 9.5. \n",
        "\n",
        "Since softmax policy is differentiable, we can find out its score function is\n",
        "\\begin{equation}\n",
        "\\nabla_{\\theta} \\log \\pi_{\\theta}(s, a) = \\vec{x}(s, a) - \\mathop{\\mathbb{E}}_{\\pi_{\\theta}}[\\vec{x}(s, \\cdot)]\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "#### Advantages of parameterising policies by softmax in action preference\n",
        "\n",
        "1. The approximate policy can approach a determistic policy or a $\\epsilon$-greedy action selection (i.e. having a probability of $\\epsilon$ of randomly choosing an action). \n",
        "\n",
        "    - If the softmax distribution included a temperature parameter, then the temperature could be reduced over time to approach determinism, but in practice it would be difficult to choose the reduction schedule, or even the initial temperature, without more prior knowledge of the true action values than we would like to assume. \n",
        "\n",
        "    - Action preferences are different because they do not approach specific values. Instead, they are diven to product the optimal stochastic policy. If the optimal policy is deterministic, then the preferences of the optimal actions will be driven infinitely higher than all suboptimal actions (if permitted by parameterization). \n",
        "\n",
        "2. It enables the selection of actions with arbitrary probabilities. This is particularly useful when the optimal policy is not a deterministic policy.\n",
        "\n",
        "3. The policy parameterization may have over action-value parameterization is that the policy may be a simpler function to approximate. For this we will cover later.\n",
        "\n",
        "4. The last advantage is that policy parameterization is sometimes a good way of injecting prior knowledge about the desired form of the policy into the reinforcement learning system. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASVIF4r3Txyl",
        "colab_type": "text"
      },
      "source": [
        "## Theorem\n",
        "- To extend the porlicy gradient approach in one-step MDP, we can replace the instant reward from on-step MDP to long term action-value $Q_{\\pi_{\\theta}}$, we have\n",
        "> For any differentiable policy $\\pi_{\\theta}(s, a)$ \\\\\n",
        "> For any of the policy objective functions $J = J_1, J_{avR}, \\frac{1}{1-\\gamma}J_{avV}$\n",
        "> The policy gradient is\n",
        "> \\begin{equation}\n",
        "\\nabla_{\\theta}J(\\vec{\\theta}) = \\mathop{\\mathbb{E_{\\pi_{\\theta}}}}[\\nabla_{\\theta} \\log \\pi_{\\theta} (A|S) Q_{\\pi_{\\theta}}(S, A)]\n",
        "\\end{equation}\n",
        "- The policy gradient theorem applies to start state objective, average reward and average value objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU8zeye7mGnt",
        "colab_type": "text"
      },
      "source": [
        "# 4. Monte-Carlo Policy Gradient (REINFORCE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9OwE6pefDum",
        "colab_type": "text"
      },
      "source": [
        "## Algorithms\n",
        "\n",
        "#### Steps:\n",
        "- Update parameters by stochastic gradient ascent\n",
        "- Using policy gradient theorem\n",
        "- Using return $G_t$ as an unbiased sample of $Q_{\\pi_{\\theta}}(s_t, a_t)$\n",
        "\\begin{equation}\n",
        "\\Delta \\theta_t = \\alpha \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t, a_t)G_t\n",
        "\\end{equation}\n",
        "\n",
        "#### Algorithm\n",
        "---\n",
        "```\n",
        "Input: a differentiable policy parameterization pi(a|s, theta)\n",
        "Algorithm parameter: step size alpha > 0\n",
        "Initialise policy parameter theta with dimension d'\n",
        "\n",
        "Loop forever for each episode:\n",
        "        Generate an episode S0, A0, R1, ..., ST-1, AT-1, RT, following pi(.|., theta)\n",
        "        Loop for each step of the episode t = 0, 1, ..., T-1\n",
        "        G = sum(t+1:T)(gamma^(k-t-1))Rk\n",
        "        theta = theta + alpha * gamma^t * grad of ln pi(At|St, theta) * Gt\n",
        "```\n",
        "---\n",
        "\n",
        "The problem of Monte-Carlo policy is that the gradient still has high variance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usmPlLJMk7o7",
        "colab_type": "text"
      },
      "source": [
        "# 5. Actor-Critic methods\n",
        "\n",
        "- Simple actor-critic algorithm based on action-value critic\n",
        "- Using linear value function approximation $Q_w(s, a) = \\vec{x}(s, a)^T \\vec{w}$\n",
        "- The critic updates $w$ by linear TD(0), and the actor updates $\\theta$ by policy gradient\n",
        "---\n",
        "```\n",
        "Input: a differentiable policy parameterization pi(a|s, theta)\n",
        "Input: a differentiable state-value function parameterization Q_w(s, a, w)\n",
        "Parameters: step sizes alpha_theta > 0; alpha_w > 0\n",
        "\n",
        "Loop forever for each episode:\n",
        "\n",
        "        Initialise S, theta\n",
        "        Sample a in pi_theta\n",
        "        \n",
        "        Loop while S is not terminal for each time step:\n",
        "                A = pi(.|S, theta)\n",
        "                Take action A, observe S', R\n",
        "                delta = R + gamma * Q_w(S', A', w) - Q_w(S, A, w)  [TD(0) error, or advantage]\n",
        "                theta = theta + alpha_theta * grad_pi log pi_theta(s,a) Q_w(S,A)     [policy gradient update]\n",
        "                w = w + alpha_w * delta * x(s, a)    [TD(0)]\n",
        "                A = A', S = S'\n",
        "```\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNnSlHJPwZ8n",
        "colab_type": "text"
      },
      "source": [
        "## Reduce Variance\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVSrvfucwiJP",
        "colab_type": "text"
      },
      "source": [
        "## Full advantage actor critic agent\n",
        "\n",
        "Advantage actor critic includes:\n",
        "- A representation (e.g. LSTM): $(S_{t-1}, O_t) \\mapsto S_t$\n",
        "- A network $v_{w}: S \\mapsto v$\n",
        "- A network $\\pi_{\\theta} \\mapsto \\pi$\n",
        "- Copies/varients $\\pi^m$ of $\\pi_{\\theta}$ to use as policies: $S_{t}^{m} \\mapsto A_{t}^{m}$\n",
        "- A n-step TD loss on $v_{w}$\n",
        "\n",
        "\\begin{equation}\n",
        "L(w) = \\frac{1}{2}(G_{t}^{n} - v_{w}(S_t))^2\n",
        "\\end{equation}\n",
        "where $G_t^(n) = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...$\n",
        "\n",
        "- A n-step REINFORECE loss on $\\pi_{theta}$\n",
        "\\begin{equation}\n",
        "L(\\theta) = \\big[ G_{t}^{(n)} - v_{w}(S_t) \\big] \\log \\pi_{theta}(A_t | S_t)\n",
        "\\end{equation}\n",
        "\n",
        "- And use optimizers to minimize the losses"
      ]
    }
  ]
}