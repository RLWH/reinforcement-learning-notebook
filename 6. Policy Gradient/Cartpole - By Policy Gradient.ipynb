{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole solving by Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import sys\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    The policy network\n",
    "    Args:\n",
    "        n_inputs (int)\n",
    "        n_outputs (int)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super().__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        \n",
    "        self.reward_history = []\n",
    "        self.loss_history = []\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.n_inputs, 64)\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(64, self.n_outputs)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        # save log probs history and rewards history\n",
    "        self.saved_log_probs_list = []\n",
    "        self.saved_log_probs = torch.Tensor([])\n",
    "        self.rewards = []\n",
    "        \n",
    "        # Logs\n",
    "        self.loss_history = []\n",
    "        self.reward_history = []\n",
    "        \n",
    "    def reset(self):\n",
    "        self.saved_log_probs_list = []\n",
    "        self.saved_log_probs = torch.Tensor([])\n",
    "        self.rewards = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        Args:\n",
    "            x (torch.Tensor)\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preview the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "next_obs, reward, done, info = env.step(env.action_space.sample())\n",
    "env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'next_obs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9ccaf8600efb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext_obs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'next_obs' is not defined"
     ]
    }
   ],
   "source": [
    "next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preview the policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms\n",
    "\n",
    "---\n",
    "```\n",
    "Input: a differentiable policy parameterization pi(a|s, theta)\n",
    "Algorithm parameter: step size alpha > 0\n",
    "Initialise policy parameter theta with dimension d'\n",
    "\n",
    "Loop forever for each episode:\n",
    "        Generate an episode S0, A0, R1, ..., ST-1, AT-1, RT, following pi(.|., theta) (def generate())\n",
    "        Loop for each step of the episode t = 0, 1, ..., T-1\n",
    "        G = sum(t+1:T)(gamma^(k-t-1))Rk (def calculate_reward)\n",
    "        theta = theta + alpha * gamma^t * grad of ln pi(At|St, theta) * Gt\n",
    "```\n",
    "---\n",
    "####  Policy Gradient as a supervised learning problem\n",
    "http://karpathy.github.io/2016/05/31/rl/   \n",
    "https://amoudgl.github.io/blog/policy-gradient/\n",
    "\n",
    "> Okay, but what do we do if we do not have the correct label in the Reinforcement Learning setting?  \n",
    "Here is the Policy Gradients solution (again refer to diagram below). Our policy network calculated probability of going UP as 30% (logprob -1.2) and DOWN as 70% (logprob -0.36). We will now sample an action from this distribution; E.g. suppose we sample DOWN, and we will execute it in the game. **At this point notice one interesting fact: We could immediately fill in a gradient of 1.0 for DOWN as we did in supervised learning, and find the gradient vector that would encourage the network to be slightly more likely to do the DOWN action in the future.**\n",
    "\n",
    "> So we can immediately evaluate this gradient and that’s great, but the problem is that at least for now we do not yet know if going DOWN is good.   \n",
    "But the critical point is that that’s okay, because we can simply wait a bit and see!  \n",
    "For example in Pong we could wait until the end of the game, then take the reward we get (either +1 if we won or -1 if we lost), and enter that scalar as the gradient for the action we have taken (DOWN in this case). In the example below, going DOWN ended up to us losing the game (-1 reward). So if we fill in -1 for log probability of DOWN and do backprop we will find a gradient that discourages the network to take the DOWN action for that input in the future (and rightly so, since taking that action led to us losing the game)\n",
    "\n",
    "> Policy gradients is exactly the same as supervised learning with two minor differences:  \n",
    "    1) We don’t have the correct labels yi so as a “fake label” we substitute the action we happened to sample from the policy when it saw xi, and  \n",
    "    2) We modulate the loss for each example multiplicatively based on the eventual outcome, since we want to increase the log probability for actions that worked and decrease it for those that didn’t.  \n",
    "    So in summary our loss now looks like $\\sum_{i}A_{i}\\log p(yi∣xi)$, where yi is the action we happened to sample and Ai is a number that we call an advantage. \n",
    " \n",
    "i.e., using the sampled outcome of the SARSA sequence as the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = PolicyNetwork(env.observation_space.shape[0], env.action_space.n)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(Variable(state))\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs_list.append(m.log_prob(action))\n",
    "\n",
    "    policy.saved_log_probs = torch.cat([\n",
    "        policy.saved_log_probs,\n",
    "        m.log_prob(action).reshape(1)\n",
    "    ])\n",
    "\n",
    "    return action.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env, policy):\n",
    "    \"\"\"\n",
    "    Generate an episode\n",
    "    Args:\n",
    "        env (gym.env)\n",
    "    \"\"\"\n",
    "    \n",
    "    obs = env.reset()\n",
    "    ep_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while True:\n",
    "        action = act(obs)\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        policy.rewards.append(reward)\n",
    "        ep_reward += reward\n",
    "        obs = next_obs\n",
    "        \n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_discount_return(rewards, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Calculate the discount return by given rewards series\n",
    "    Args:\n",
    "        rewards (np.array)\n",
    "    \"\"\"\n",
    "    G = 0\n",
    "    T = len(rewards)\n",
    "    returns = []\n",
    "    for r in rewards[::-1]:\n",
    "        G = gamma * G + r\n",
    "        returns.insert(0, G)\n",
    "    \n",
    "    # Flip the returns list\n",
    "#     reversed_returns = reversed(returns)\n",
    "    \n",
    "    return torch.FloatTensor(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_update(episode_series, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Calculate the discounted return by a given episode series\n",
    "    We use the start value of the episode as the performance measure function\n",
    "    Recall the return of a monte carlo policy is\n",
    "    G = sum(t+1:T)(gamma^(k-t-1))Rk\n",
    "    Args:\n",
    "        episode_series (list)\n",
    "        gamma (float)\n",
    "    Return:\n",
    "        Return (float)\n",
    "    \"\"\"\n",
    "\n",
    "    advantage_torch = calculate_discount_return(policy.rewards)\n",
    "    advantage_torch = (advantage_torch - advantage_torch.mean()) / \\\n",
    "        (advantage_torch.std() + np.finfo(np.float32).eps)\n",
    "    probs_torch = torch.cat(policy.saved_log_probs_list)\n",
    "    \n",
    "    # Calculate performance measure (~loss) function\n",
    "    # Use expected average reward as the advantage At\n",
    "\n",
    "#     loss = (torch.sum(torch.mul(probs_torch, advantage_torch).mul(-1), -1))\n",
    "    loss = torch.neg(torch.matmul(probs_torch, advantage_torch))\n",
    "\n",
    "    # Update network weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Log the records\n",
    "    policy.loss_history.append(loss.item())\n",
    "    policy.reward_history.append(np.sum(policy.rewards))\n",
    "    \n",
    "    return loss, np.sum(policy.rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PolicyNetwork(\n",
      "  (fc1): Linear(in_features=4, out_features=64, bias=True)\n",
      "  (dropout1): Dropout(p=0.5)\n",
      "  (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (softmax): Softmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_episode(env, policy)\n",
    "# ep_loss, ep_rewards = gradient_update(policy.rewards)\n",
    "# # running_rewards.append(ep_rewards)\n",
    "# print(policy.saved_log_probs)\n",
    "# policy.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.0705e+00, -7.6322e-02, -1.4538e-01, -8.1001e-01, -2.2771e-01,\n",
       "        -5.3254e-01, -3.7689e-01, -3.0863e-01, -7.3684e-01, -3.4998e-01,\n",
       "        -8.5891e-01, -1.0231e-01, -2.6553e-01, -4.8735e-01, -6.9904e-01,\n",
       "        -4.9561e-01, -8.2223e-01, -3.9971e-01, -4.6303e-01, -6.5191e-01,\n",
       "        -4.7847e-01, -7.6780e-01, -1.9605e-01, -1.0580e+00, -2.1703e+00,\n",
       "        -1.6707e-02, -4.6623e-02, -9.2916e-02, -1.7382e-01, -2.4002e-01,\n",
       "        -3.8414e-01, -7.2857e-01, -7.2172e-01, -1.1740e+00, -1.7711e-01,\n",
       "        -1.0957e-01, -7.9081e-02, -6.4588e-02, -6.4630e-02, -7.7180e-02,\n",
       "        -1.5326e+00, -7.1395e-02, -1.6835e+00, -6.7068e-02, -1.7517e-01,\n",
       "        -6.2209e-01, -1.6373e+00, -8.6785e-02, -2.0552e-01, -5.2546e-01,\n",
       "        -2.8488e-01, -1.1472e+00, -5.2788e-02, -1.8358e+00, -2.1829e-02,\n",
       "        -6.2698e-02, -1.7169e-01, -3.4354e-01, -4.8131e-01, -6.8473e-01,\n",
       "        -4.2328e-01, -4.3539e-01, -9.1629e-01, -3.2329e-01, -3.7226e-01,\n",
       "        -5.1660e-01, -3.2137e-01, -6.6739e-01, -1.5608e+00, -3.6454e-02,\n",
       "        -2.2668e+00, -1.6838e-02, -4.2676e-02, -8.6103e-02, -1.3695e-01,\n",
       "        -1.9939e-01, -3.2363e-01, -7.9433e-01, -6.3255e-01, -3.9465e-01,\n",
       "        -1.2225e+00, -1.6427e-01, -2.1582e+00, -5.8698e-02, -3.2936e+00,\n",
       "        -1.8571e-02, -1.1288e-02, -7.4967e-03, -5.5426e-03, -4.7299e-03,\n",
       "        -5.8282e-03, -1.5807e-02, -6.8309e-02, -3.8644e-01, -2.1835e-01,\n",
       "        -6.5555e-01, -1.0768e-01, -3.5678e-01, -4.5842e-01, -1.7081e-01,\n",
       "        -9.2744e-01, -6.7942e-02, -1.8352e-01, -5.2623e-01, -1.0236e+00,\n",
       "        -3.1393e-01, -5.6871e-01, -2.1329e-01, -9.8157e-01, -9.9140e-02,\n",
       "        -1.6290e+00, -4.2236e-02, -8.4190e-02, -1.4499e-01, -1.8419e-01,\n",
       "        -2.6339e-01, -1.0574e+00, -1.1408e+00, -2.8838e-01, -1.0357e+00,\n",
       "        -3.3287e-01, -5.1278e-01, -5.6579e-01, -8.1542e-01, -3.2367e-01,\n",
       "        -3.1847e-01, -1.1773e+00, -2.0695e-01, -2.2965e-01, -1.3289e+00,\n",
       "        -1.8456e-01, -2.4378e-01, -5.2205e-01, -2.0854e-01, -7.3953e-01,\n",
       "        -1.3278e-01, -4.0807e-01, -7.0787e-01, -4.6826e-01, -7.4487e-01,\n",
       "        -1.0357e+00, -2.5527e-01, -2.8837e-01, -3.9223e-01, -7.7037e-01,\n",
       "        -1.2867e-01, -3.8326e-01, -6.4043e-01, -5.2346e-01, -6.6770e-01,\n",
       "        -4.9697e-01, -6.8273e-01, -1.2690e+00, -6.3431e-02, -1.6766e-01,\n",
       "        -2.9143e-01, -4.0910e-01, -6.4401e-01, -4.4129e-01, -1.0487e+00,\n",
       "        -2.3852e-01, -1.6696e+00, -1.1084e-01, -9.0602e-02, -8.6763e-02,\n",
       "        -1.0649e-01, -1.7539e-01, -7.0854e-01, -2.1570e-01, -5.9786e-01,\n",
       "        -2.8107e-01, -4.6554e-01, -3.8683e-01, -3.2929e-01, -8.4822e-01,\n",
       "        -4.0486e-01, -7.4882e-01, -9.7720e-01, -3.0119e-01, -4.0954e-01,\n",
       "        -7.4132e-01, -4.7528e-01, -6.3958e-01, -8.4350e-01, -3.6837e-01,\n",
       "        -9.2950e-01, -3.2142e-01, -4.3545e-01, -6.8805e-01, -1.6868e-01,\n",
       "        -1.0390e+00, -9.5974e-02, -2.2891e-01, -3.1528e-01, -1.0188e+00,\n",
       "        -2.9758e-01, -1.0625e+00, -1.4086e+00, -1.3008e-01, -1.8514e-01,\n",
       "        -1.4741e+00, -1.6136e-01, -2.2635e-01, -3.6665e-01, -6.4189e-01,\n",
       "        -4.2386e-01, -1.2478e+00, -1.8937e+00, -6.2361e-02, -3.6138e-02,\n",
       "        -2.3201e-02, -1.6362e-02, -4.3554e+00, -7.0758e-03, -5.2042e-03,\n",
       "        -4.2841e-03, -4.7447e-03, -7.7028e-03, -2.0657e-02, -9.4051e-02,\n",
       "        -8.7271e-01, -1.3834e-01, -6.0764e-01, -2.3754e-01, -1.1886e+00,\n",
       "        -4.9812e-02, -1.3203e-01, -1.1899e+00, -5.1905e-02, -1.2571e-01,\n",
       "        -2.8537e-01, -8.2162e-01, -1.2649e-01, -2.4084e-01, -3.0747e-01,\n",
       "        -1.1974e+00, -2.1214e-01, -2.3712e-01, -3.1550e-01, -4.7337e-01,\n",
       "        -6.7046e-01, -7.5587e-01, -1.1262e+00, -1.9463e-01, -1.4198e-01,\n",
       "        -1.1640e-01, -1.1268e-01, -1.2719e-01, -1.9255e-01, -5.3407e-01,\n",
       "        -1.9918e-01, -5.5402e-01, -4.5173e-01, -3.9444e-01, -6.5568e-01,\n",
       "        -2.5656e-01, -9.7197e-01, -1.9351e+00, -2.8694e-02, -6.2691e-02,\n",
       "        -1.1077e-01, -1.1677e-01, -1.4758e-01, -2.2906e-01, -1.1254e+00,\n",
       "        -3.6639e-01, -6.1715e-01, -1.0435e+00, -1.8773e-01, -2.1980e+00,\n",
       "        -4.4798e-02, -2.5337e-02, -1.5600e-02, -1.0505e-02, -7.9090e-03,\n",
       "        -6.7740e-03, -7.5177e-03, -1.2443e-02, -3.3743e-02, -1.6736e-01,\n",
       "        -5.6542e-01, -1.4590e+00, -6.4371e-02, -3.5939e-01, -2.6246e-01,\n",
       "        -6.6263e-01, -1.2414e-01, -3.4239e-01, -8.1577e-01, -2.7113e-01,\n",
       "        -5.0659e-01, -4.9208e-01, -2.7079e-01, -8.8648e-01, -1.2559e-01,\n",
       "        -1.5424e+00, -4.9566e-02, -8.8369e-02, -1.0731e-01, -1.0314e-01,\n",
       "        -1.1851e-01, -1.6807e-01, -2.6422e-01, -4.6037e-01, -5.3662e-01,\n",
       "        -3.8118e-01, -2.9559e-01, -2.6523e-01, -2.7391e-01, -3.3273e-01,\n",
       "        -5.2008e-01, -3.8218e-01, -6.8696e-01, -4.9619e-01, -6.4122e-01,\n",
       "        -5.4590e-01, -8.1082e-01, -2.3641e-01, -4.0014e-01, -5.0023e-01,\n",
       "        -7.2187e-01, -1.0580e+00, -1.5982e+00, -1.0035e-01, -6.7480e-02,\n",
       "        -5.0985e-02, -4.5080e-02, -4.5430e-02, -5.7346e-02, -1.0568e-01,\n",
       "        -4.0907e-01, -1.4463e+00, -4.1664e-02, -2.2394e+00, -1.6392e-02,\n",
       "        -3.6399e-02, -8.5556e-02, -1.8530e+00, -3.4479e-02, -6.2792e-02,\n",
       "        -7.9120e-02, -7.4566e-02, -7.7959e-02, -2.3099e+00, -7.7055e-02,\n",
       "        -1.0431e-01, -1.6479e-01, -3.0870e-01, -6.2153e-01, -3.2760e-01,\n",
       "        -1.8669e-01, -1.1383e-01, -7.6774e-02, -5.9336e-02, -5.1950e-02,\n",
       "        -5.1582e-02, -6.7481e-02, -1.2437e-01, -4.1811e-01, -2.7127e-01,\n",
       "        -6.4589e-01, -1.6436e-01, -1.0382e+00, -8.0364e-02, -2.0673e-01,\n",
       "        -1.0484e+00, -2.3051e+00, -1.5701e-02, -3.3665e-02, -5.9862e-02,\n",
       "        -7.0981e-02, -6.5327e-02, -6.7869e-02, -8.3079e-02, -1.2291e-01,\n",
       "        -2.1558e-01, -4.4750e-01, -4.7352e-01, -2.8281e-01, -1.7896e-01,\n",
       "        -1.2671e-01, -1.0239e-01, -9.2891e-02, -9.5737e-02, -1.2779e-01,\n",
       "        -2.2896e-01, -6.7420e-01, -1.6065e-01, -4.4227e-01, -8.5681e-01,\n",
       "        -1.0958e+00, -3.1734e-01, -1.1163e+00, -3.0824e-01, -3.8536e-01,\n",
       "        -7.7707e-01, -9.6842e-01, -4.0143e-01, -9.6374e-01, -4.0611e-01,\n",
       "        -4.8356e-01, -7.6420e-01, -1.7182e+00, -3.1801e-02, -7.4674e-02,\n",
       "        -1.4170e-01, -1.5173e-01, -1.5025e-01, -1.8684e+00, -1.3086e-01,\n",
       "        -1.4688e-01, -1.8331e-01, -1.3846e+00, -2.8690e-01, -4.8182e-01,\n",
       "        -5.1422e-01, -1.0740e+00, -1.8284e-01, -1.2956e-01, -1.0361e-01,\n",
       "        -2.4220e+00, -4.4958e-02, -3.7242e-02, -3.4524e-02, -4.0602e-02,\n",
       "        -2.7715e+00, -3.3323e-02, -5.3686e-02, -1.2079e-01, -1.0171e+00,\n",
       "        -1.6178e-01, -5.9937e-01, -1.5442e-01, -4.3803e-01, -4.4501e-01,\n",
       "        -1.5349e+00, -3.4261e-02, -7.8700e-02, -1.6528e-01, -2.4222e-01,\n",
       "        -2.2088e-01, -2.1608e-01, -2.2334e-01, -2.8427e-01, -4.4542e-01,\n",
       "        -5.9551e-01, -4.8640e-01, -4.4637e-01, -1.0009e+00, -1.4709e+00,\n",
       "        -1.1906e-01, -9.4871e-02, -8.4817e-02, -8.6007e-02, -1.0066e-01,\n",
       "        -1.5883e-01, -3.2400e-01, -3.6136e-01, -5.1064e-01, -1.6344e+00,\n",
       "        -3.4606e+00, -4.3426e-03, -8.0429e-03, -1.4973e-02, -2.7510e-02,\n",
       "        -4.1342e-02, -3.6655e-02, -3.0346e-02, -2.8430e-02, -3.0725e-02,\n",
       "        -3.5674e-02, -5.0450e-02, -9.3230e-02, -2.0298e-01, -5.4838e-01,\n",
       "        -1.3833e+00, -7.5214e-02, -2.4743e-02, -9.3907e-03, -3.7773e-03],\n",
       "       grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.saved_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.0705e+00, -7.6322e-02, -1.4538e-01, -8.1001e-01, -2.2771e-01,\n",
       "        -5.3254e-01, -3.7689e-01, -3.0863e-01, -7.3684e-01, -3.4998e-01,\n",
       "        -8.5891e-01, -1.0231e-01, -2.6553e-01, -4.8735e-01, -6.9904e-01,\n",
       "        -4.9561e-01, -8.2223e-01, -3.9971e-01, -4.6303e-01, -6.5191e-01,\n",
       "        -4.7847e-01, -7.6780e-01, -1.9605e-01, -1.0580e+00, -2.1703e+00,\n",
       "        -1.6707e-02, -4.6623e-02, -9.2916e-02, -1.7382e-01, -2.4002e-01,\n",
       "        -3.8414e-01, -7.2857e-01, -7.2172e-01, -1.1740e+00, -1.7711e-01,\n",
       "        -1.0957e-01, -7.9081e-02, -6.4588e-02, -6.4630e-02, -7.7180e-02,\n",
       "        -1.5326e+00, -7.1395e-02, -1.6835e+00, -6.7068e-02, -1.7517e-01,\n",
       "        -6.2209e-01, -1.6373e+00, -8.6785e-02, -2.0552e-01, -5.2546e-01,\n",
       "        -2.8488e-01, -1.1472e+00, -5.2788e-02, -1.8358e+00, -2.1829e-02,\n",
       "        -6.2698e-02, -1.7169e-01, -3.4354e-01, -4.8131e-01, -6.8473e-01,\n",
       "        -4.2328e-01, -4.3539e-01, -9.1629e-01, -3.2329e-01, -3.7226e-01,\n",
       "        -5.1660e-01, -3.2137e-01, -6.6739e-01, -1.5608e+00, -3.6454e-02,\n",
       "        -2.2668e+00, -1.6838e-02, -4.2676e-02, -8.6103e-02, -1.3695e-01,\n",
       "        -1.9939e-01, -3.2363e-01, -7.9433e-01, -6.3255e-01, -3.9465e-01,\n",
       "        -1.2225e+00, -1.6427e-01, -2.1582e+00, -5.8698e-02, -3.2936e+00,\n",
       "        -1.8571e-02, -1.1288e-02, -7.4967e-03, -5.5426e-03, -4.7299e-03,\n",
       "        -5.8282e-03, -1.5807e-02, -6.8309e-02, -3.8644e-01, -2.1835e-01,\n",
       "        -6.5555e-01, -1.0768e-01, -3.5678e-01, -4.5842e-01, -1.7081e-01,\n",
       "        -9.2744e-01, -6.7942e-02, -1.8352e-01, -5.2623e-01, -1.0236e+00,\n",
       "        -3.1393e-01, -5.6871e-01, -2.1329e-01, -9.8157e-01, -9.9140e-02,\n",
       "        -1.6290e+00, -4.2236e-02, -8.4190e-02, -1.4499e-01, -1.8419e-01,\n",
       "        -2.6339e-01, -1.0574e+00, -1.1408e+00, -2.8838e-01, -1.0357e+00,\n",
       "        -3.3287e-01, -5.1278e-01, -5.6579e-01, -8.1542e-01, -3.2367e-01,\n",
       "        -3.1847e-01, -1.1773e+00, -2.0695e-01, -2.2965e-01, -1.3289e+00,\n",
       "        -1.8456e-01, -2.4378e-01, -5.2205e-01, -2.0854e-01, -7.3953e-01,\n",
       "        -1.3278e-01, -4.0807e-01, -7.0787e-01, -4.6826e-01, -7.4487e-01,\n",
       "        -1.0357e+00, -2.5527e-01, -2.8837e-01, -3.9223e-01, -7.7037e-01,\n",
       "        -1.2867e-01, -3.8326e-01, -6.4043e-01, -5.2346e-01, -6.6770e-01,\n",
       "        -4.9697e-01, -6.8273e-01, -1.2690e+00, -6.3431e-02, -1.6766e-01,\n",
       "        -2.9143e-01, -4.0910e-01, -6.4401e-01, -4.4129e-01, -1.0487e+00,\n",
       "        -2.3852e-01, -1.6696e+00, -1.1084e-01, -9.0602e-02, -8.6763e-02,\n",
       "        -1.0649e-01, -1.7539e-01, -7.0854e-01, -2.1570e-01, -5.9786e-01,\n",
       "        -2.8107e-01, -4.6554e-01, -3.8683e-01, -3.2929e-01, -8.4822e-01,\n",
       "        -4.0486e-01, -7.4882e-01, -9.7720e-01, -3.0119e-01, -4.0954e-01,\n",
       "        -7.4132e-01, -4.7528e-01, -6.3958e-01, -8.4350e-01, -3.6837e-01,\n",
       "        -9.2950e-01, -3.2142e-01, -4.3545e-01, -6.8805e-01, -1.6868e-01,\n",
       "        -1.0390e+00, -9.5974e-02, -2.2891e-01, -3.1528e-01, -1.0188e+00,\n",
       "        -2.9758e-01, -1.0625e+00, -1.4086e+00, -1.3008e-01, -1.8514e-01,\n",
       "        -1.4741e+00, -1.6136e-01, -2.2635e-01, -3.6665e-01, -6.4189e-01,\n",
       "        -4.2386e-01, -1.2478e+00, -1.8937e+00, -6.2361e-02, -3.6138e-02,\n",
       "        -2.3201e-02, -1.6362e-02, -4.3554e+00, -7.0758e-03, -5.2042e-03,\n",
       "        -4.2841e-03, -4.7447e-03, -7.7028e-03, -2.0657e-02, -9.4051e-02,\n",
       "        -8.7271e-01, -1.3834e-01, -6.0764e-01, -2.3754e-01, -1.1886e+00,\n",
       "        -4.9812e-02, -1.3203e-01, -1.1899e+00, -5.1905e-02, -1.2571e-01,\n",
       "        -2.8537e-01, -8.2162e-01, -1.2649e-01, -2.4084e-01, -3.0747e-01,\n",
       "        -1.1974e+00, -2.1214e-01, -2.3712e-01, -3.1550e-01, -4.7337e-01,\n",
       "        -6.7046e-01, -7.5587e-01, -1.1262e+00, -1.9463e-01, -1.4198e-01,\n",
       "        -1.1640e-01, -1.1268e-01, -1.2719e-01, -1.9255e-01, -5.3407e-01,\n",
       "        -1.9918e-01, -5.5402e-01, -4.5173e-01, -3.9444e-01, -6.5568e-01,\n",
       "        -2.5656e-01, -9.7197e-01, -1.9351e+00, -2.8694e-02, -6.2691e-02,\n",
       "        -1.1077e-01, -1.1677e-01, -1.4758e-01, -2.2906e-01, -1.1254e+00,\n",
       "        -3.6639e-01, -6.1715e-01, -1.0435e+00, -1.8773e-01, -2.1980e+00,\n",
       "        -4.4798e-02, -2.5337e-02, -1.5600e-02, -1.0505e-02, -7.9090e-03,\n",
       "        -6.7740e-03, -7.5177e-03, -1.2443e-02, -3.3743e-02, -1.6736e-01,\n",
       "        -5.6542e-01, -1.4590e+00, -6.4371e-02, -3.5939e-01, -2.6246e-01,\n",
       "        -6.6263e-01, -1.2414e-01, -3.4239e-01, -8.1577e-01, -2.7113e-01,\n",
       "        -5.0659e-01, -4.9208e-01, -2.7079e-01, -8.8648e-01, -1.2559e-01,\n",
       "        -1.5424e+00, -4.9566e-02, -8.8369e-02, -1.0731e-01, -1.0314e-01,\n",
       "        -1.1851e-01, -1.6807e-01, -2.6422e-01, -4.6037e-01, -5.3662e-01,\n",
       "        -3.8118e-01, -2.9559e-01, -2.6523e-01, -2.7391e-01, -3.3273e-01,\n",
       "        -5.2008e-01, -3.8218e-01, -6.8696e-01, -4.9619e-01, -6.4122e-01,\n",
       "        -5.4590e-01, -8.1082e-01, -2.3641e-01, -4.0014e-01, -5.0023e-01,\n",
       "        -7.2187e-01, -1.0580e+00, -1.5982e+00, -1.0035e-01, -6.7480e-02,\n",
       "        -5.0985e-02, -4.5080e-02, -4.5430e-02, -5.7346e-02, -1.0568e-01,\n",
       "        -4.0907e-01, -1.4463e+00, -4.1664e-02, -2.2394e+00, -1.6392e-02,\n",
       "        -3.6399e-02, -8.5556e-02, -1.8530e+00, -3.4479e-02, -6.2792e-02,\n",
       "        -7.9120e-02, -7.4566e-02, -7.7959e-02, -2.3099e+00, -7.7055e-02,\n",
       "        -1.0431e-01, -1.6479e-01, -3.0870e-01, -6.2153e-01, -3.2760e-01,\n",
       "        -1.8669e-01, -1.1383e-01, -7.6774e-02, -5.9336e-02, -5.1950e-02,\n",
       "        -5.1582e-02, -6.7481e-02, -1.2437e-01, -4.1811e-01, -2.7127e-01,\n",
       "        -6.4589e-01, -1.6436e-01, -1.0382e+00, -8.0364e-02, -2.0673e-01,\n",
       "        -1.0484e+00, -2.3051e+00, -1.5701e-02, -3.3665e-02, -5.9862e-02,\n",
       "        -7.0981e-02, -6.5327e-02, -6.7869e-02, -8.3079e-02, -1.2291e-01,\n",
       "        -2.1558e-01, -4.4750e-01, -4.7352e-01, -2.8281e-01, -1.7896e-01,\n",
       "        -1.2671e-01, -1.0239e-01, -9.2891e-02, -9.5737e-02, -1.2779e-01,\n",
       "        -2.2896e-01, -6.7420e-01, -1.6065e-01, -4.4227e-01, -8.5681e-01,\n",
       "        -1.0958e+00, -3.1734e-01, -1.1163e+00, -3.0824e-01, -3.8536e-01,\n",
       "        -7.7707e-01, -9.6842e-01, -4.0143e-01, -9.6374e-01, -4.0611e-01,\n",
       "        -4.8356e-01, -7.6420e-01, -1.7182e+00, -3.1801e-02, -7.4674e-02,\n",
       "        -1.4170e-01, -1.5173e-01, -1.5025e-01, -1.8684e+00, -1.3086e-01,\n",
       "        -1.4688e-01, -1.8331e-01, -1.3846e+00, -2.8690e-01, -4.8182e-01,\n",
       "        -5.1422e-01, -1.0740e+00, -1.8284e-01, -1.2956e-01, -1.0361e-01,\n",
       "        -2.4220e+00, -4.4958e-02, -3.7242e-02, -3.4524e-02, -4.0602e-02,\n",
       "        -2.7715e+00, -3.3323e-02, -5.3686e-02, -1.2079e-01, -1.0171e+00,\n",
       "        -1.6178e-01, -5.9937e-01, -1.5442e-01, -4.3803e-01, -4.4501e-01,\n",
       "        -1.5349e+00, -3.4261e-02, -7.8700e-02, -1.6528e-01, -2.4222e-01,\n",
       "        -2.2088e-01, -2.1608e-01, -2.2334e-01, -2.8427e-01, -4.4542e-01,\n",
       "        -5.9551e-01, -4.8640e-01, -4.4637e-01, -1.0009e+00, -1.4709e+00,\n",
       "        -1.1906e-01, -9.4871e-02, -8.4817e-02, -8.6007e-02, -1.0066e-01,\n",
       "        -1.5883e-01, -3.2400e-01, -3.6136e-01, -5.1064e-01, -1.6344e+00,\n",
       "        -3.4606e+00, -4.3426e-03, -8.0429e-03, -1.4973e-02, -2.7510e-02,\n",
       "        -4.1342e-02, -3.6655e-02, -3.0346e-02, -2.8430e-02, -3.0725e-02,\n",
       "        -3.5674e-02, -5.0450e-02, -9.3230e-02, -2.0298e-01, -5.4838e-01,\n",
       "        -1.3833e+00, -7.5214e-02, -2.4743e-02, -9.3907e-03, -3.7773e-03],\n",
       "       grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(policy.saved_log_probs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 \tLoss: tensor(-0.8614, grad_fn=<NegBackward>) \tAverage episode Rewards: 239.0\n",
      "Episode: 100 \tLoss: tensor(-12.4414, grad_fn=<NegBackward>) \tAverage episode Rewards: 496.16\n",
      "Episode: 200 \tLoss: tensor(-2.7132, grad_fn=<NegBackward>) \tAverage episode Rewards: 499.36\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-cac7e5ad6ce5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mgenerate_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mep_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mrunning_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#     print(policy.saved_log_probs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-a27f05efa897>\u001b[0m in \u001b[0;36mgradient_update\u001b[0;34m(episode_series, gamma)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Update network weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/Documents/Envs/datasci/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/Documents/Envs/datasci/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate 10 episodes\n",
    "running_rewards = deque(maxlen=100)\n",
    "\n",
    "for i in range(1000):\n",
    "    generate_episode(env, policy)\n",
    "    ep_loss, ep_rewards = gradient_update(policy.rewards)\n",
    "    running_rewards.append(ep_rewards)\n",
    "#     print(policy.saved_log_probs)\n",
    "    policy.reset()\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "\n",
    "        print('\\rEpisode: %s \\tLoss: %s \\tAverage episode Rewards: %s' % (i, ep_loss, np.mean(running_rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci",
   "language": "python",
   "name": "datasci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
