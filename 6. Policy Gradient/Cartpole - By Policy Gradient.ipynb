{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole solving by Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import sys\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    The policy network\n",
    "    Args:\n",
    "        n_inputs (int)\n",
    "        n_outputs (int)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super().__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        \n",
    "        self.reward_history = []\n",
    "        self.loss_history = []\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.n_inputs, 64)\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(64, self.n_outputs)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        # save log probs history and rewards history\n",
    "        self.saved_log_probs_list = []\n",
    "        self.saved_log_probs = torch.Tensor([])\n",
    "        self.rewards = []\n",
    "        \n",
    "        # Logs\n",
    "        self.loss_history = []\n",
    "        self.reward_history = []\n",
    "        \n",
    "    def reset(self):\n",
    "        self.saved_log_probs_list = []\n",
    "        self.saved_log_probs = torch.Tensor([])\n",
    "        self.rewards = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        Args:\n",
    "            x (torch.Tensor)\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preview the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "next_obs, reward, done, info = env.step(env.action_space.sample())\n",
    "env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'next_obs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9ccaf8600efb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext_obs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'next_obs' is not defined"
     ]
    }
   ],
   "source": [
    "next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preview the policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms\n",
    "\n",
    "---\n",
    "```\n",
    "Input: a differentiable policy parameterization pi(a|s, theta)\n",
    "Algorithm parameter: step size alpha > 0\n",
    "Initialise policy parameter theta with dimension d'\n",
    "\n",
    "Loop forever for each episode:\n",
    "        Generate an episode S0, A0, R1, ..., ST-1, AT-1, RT, following pi(.|., theta) (def generate())\n",
    "        Loop for each step of the episode t = 0, 1, ..., T-1\n",
    "        G = sum(t+1:T)(gamma^(k-t-1))Rk (def calculate_reward)\n",
    "        theta = theta + alpha * gamma^t * grad of ln pi(At|St, theta) * Gt\n",
    "```\n",
    "---\n",
    "####  Policy Gradient as a supervised learning problem\n",
    "http://karpathy.github.io/2016/05/31/rl/   \n",
    "https://amoudgl.github.io/blog/policy-gradient/\n",
    "\n",
    "> Okay, but what do we do if we do not have the correct label in the Reinforcement Learning setting?  \n",
    "Here is the Policy Gradients solution (again refer to diagram below). Our policy network calculated probability of going UP as 30% (logprob -1.2) and DOWN as 70% (logprob -0.36). We will now sample an action from this distribution; E.g. suppose we sample DOWN, and we will execute it in the game. **At this point notice one interesting fact: We could immediately fill in a gradient of 1.0 for DOWN as we did in supervised learning, and find the gradient vector that would encourage the network to be slightly more likely to do the DOWN action in the future.**\n",
    "\n",
    "> So we can immediately evaluate this gradient and that’s great, but the problem is that at least for now we do not yet know if going DOWN is good.   \n",
    "But the critical point is that that’s okay, because we can simply wait a bit and see!  \n",
    "For example in Pong we could wait until the end of the game, then take the reward we get (either +1 if we won or -1 if we lost), and enter that scalar as the gradient for the action we have taken (DOWN in this case). In the example below, going DOWN ended up to us losing the game (-1 reward). So if we fill in -1 for log probability of DOWN and do backprop we will find a gradient that discourages the network to take the DOWN action for that input in the future (and rightly so, since taking that action led to us losing the game)\n",
    "\n",
    "> Policy gradients is exactly the same as supervised learning with two minor differences:  \n",
    "    1) We don’t have the correct labels yi so as a “fake label” we substitute the action we happened to sample from the policy when it saw xi, and  \n",
    "    2) We modulate the loss for each example multiplicatively based on the eventual outcome, since we want to increase the log probability for actions that worked and decrease it for those that didn’t.  \n",
    "    So in summary our loss now looks like $\\sum_{i}A_{i}\\log p(yi∣xi)$, where yi is the action we happened to sample and Ai is a number that we call an advantage. \n",
    " \n",
    "i.e., using the sampled outcome of the SARSA sequence as the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = PolicyNetwork(env.observation_space.shape[0], env.action_space.n)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(Variable(state))\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs_list.append(m.log_prob(action))\n",
    "\n",
    "    policy.saved_log_probs = torch.cat([\n",
    "        policy.saved_log_probs,\n",
    "        m.log_prob(action).reshape(1)\n",
    "    ])\n",
    "\n",
    "    return action.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env, policy):\n",
    "    \"\"\"\n",
    "    Generate an episode\n",
    "    Args:\n",
    "        env (gym.env)\n",
    "    \"\"\"\n",
    "    \n",
    "    obs = env.reset()\n",
    "    ep_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while True:\n",
    "        action = act(obs)\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        policy.rewards.append(reward)\n",
    "        ep_reward += reward\n",
    "        obs = next_obs\n",
    "        \n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_discount_return(rewards, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Calculate the discount return by given rewards series\n",
    "    Args:\n",
    "        rewards (np.array)\n",
    "    \"\"\"\n",
    "    G = 0\n",
    "    T = len(rewards)\n",
    "    returns = []\n",
    "    for r in rewards[::-1]:\n",
    "        G = gamma * G + r\n",
    "        returns.insert(0, G)\n",
    "    \n",
    "    # Flip the returns list\n",
    "#     reversed_returns = reversed(returns)\n",
    "    \n",
    "    return torch.FloatTensor(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_update(episode_series, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Calculate the discounted return by a given episode series\n",
    "    We use the start value of the episode as the performance measure function\n",
    "    Recall the return of a monte carlo policy is\n",
    "    G = sum(t+1:T)(gamma^(k-t-1))Rk\n",
    "    Args:\n",
    "        episode_series (list)\n",
    "        gamma (float)\n",
    "    Return:\n",
    "        Return (float)\n",
    "    \"\"\"\n",
    "\n",
    "    advantage_torch = calculate_discount_return(policy.rewards)\n",
    "    advantage_torch = (advantage_torch - advantage_torch.mean()) / \\\n",
    "        (advantage_torch.std() + np.finfo(np.float32).eps)\n",
    "    probs_torch = torch.cat(policy.saved_log_probs_list)\n",
    "    \n",
    "    # Calculate performance measure (~loss) function\n",
    "    # Use expected average reward as the advantage At\n",
    "\n",
    "    loss = (torch.sum(torch.mul(probs_torch, advantage_torch).mul(-1), -1))\n",
    "#     loss = torch.neg(torch.matmul(probs_torch, advantage_torch))\n",
    "\n",
    "    # Update network weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Log the records\n",
    "    policy.loss_history.append(loss.item())\n",
    "    policy.reward_history.append(np.sum(policy.rewards))\n",
    "    \n",
    "    return loss, np.sum(policy.rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PolicyNetwork(\n",
      "  (fc1): Linear(in_features=4, out_features=64, bias=True)\n",
      "  (dropout1): Dropout(p=0.5)\n",
      "  (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (softmax): Softmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_episode(env, policy)\n",
    "# ep_loss, ep_rewards = gradient_update(policy.rewards)\n",
    "# # running_rewards.append(ep_rewards)\n",
    "# print(policy.saved_log_probs)\n",
    "# policy.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5822, -0.7809, -0.8175, -0.8424, -0.5350, -0.8495, -0.8854, -0.8996,\n",
       "        -0.9243, -0.4889, -0.5024, -0.5173, -0.8920, -0.9038, -0.9264, -0.4933,\n",
       "        -0.5075], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.saved_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5822, -0.7809, -0.8175, -0.8424, -0.5350, -0.8495, -0.8854, -0.8996,\n",
       "        -0.9243, -0.4889, -0.5024, -0.5173, -0.8920, -0.9038, -0.9264, -0.4933,\n",
       "        -0.5075], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(policy.saved_log_probs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 \tLoss: tensor(0.9638, grad_fn=<SumBackward2>) \tAverage episode Rewards: 34.0\n",
      "Episode: 100 \tLoss: tensor(-2.5283, grad_fn=<SumBackward2>) \tAverage episode Rewards: 90.05\n",
      "Episode: 200 \tLoss: tensor(-2.6714, grad_fn=<SumBackward2>) \tAverage episode Rewards: 404.29\n",
      "Episode: 300 \tLoss: tensor(1.4144, grad_fn=<SumBackward2>) \tAverage episode Rewards: 382.67"
     ]
    }
   ],
   "source": [
    "# Generate 10 episodes\n",
    "running_rewards = deque(maxlen=100)\n",
    "\n",
    "for i in range(1000):\n",
    "    generate_episode(env, policy)\n",
    "    ep_loss, ep_rewards = gradient_update(policy.rewards)\n",
    "    running_rewards.append(ep_rewards)\n",
    "#     print(policy.saved_log_probs)\n",
    "    policy.reset()\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "\n",
    "        print('\\rEpisode: %s \\tLoss: %s \\tAverage episode Rewards: %s' % (i, ep_loss, np.mean(running_rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci",
   "language": "python",
   "name": "datasci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
