{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi-v2 solving by Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import sys\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    The policy network\n",
    "    Args:\n",
    "        n_inputs (int)\n",
    "        n_outputs (int)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super().__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        \n",
    "        self.reward_history = []\n",
    "        self.loss_history = []\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.n_inputs, 128)\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(128, self.n_outputs)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        # save log probs history and rewards history\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        # Logs\n",
    "        self.loss_history = []\n",
    "        self.reward_history = []\n",
    "        \n",
    "    def reset(self):\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        Args:\n",
    "            x (torch.Tensor)\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preview the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : :\u001b[43m \u001b[0m|\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "next_obs, reward, done, info = env.step(env.action_space.sample())\n",
    "env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: 500\n",
      "Action space: 6\n"
     ]
    }
   ],
   "source": [
    "print(\"Observation space:\", env.observation_space.n)\n",
    "print(\"Action space:\", env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preview the policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms\n",
    "\n",
    "---\n",
    "```\n",
    "Input: a differentiable policy parameterization pi(a|s, theta)\n",
    "Algorithm parameter: step size alpha > 0\n",
    "Initialise policy parameter theta with dimension d'\n",
    "\n",
    "Loop forever for each episode:\n",
    "        Generate an episode S0, A0, R1, ..., ST-1, AT-1, RT, following pi(.|., theta) (def generate())\n",
    "        Loop for each step of the episode t = 0, 1, ..., T-1\n",
    "        G = sum(t+1:T)(gamma^(k-t-1))Rk (def calculate_reward)\n",
    "        theta = theta + alpha * gamma^t * grad of ln pi(At|St, theta) * Gt\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = PolicyNetwork(env.observation_space.n, env.action_space.n)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(state):\n",
    "    state = torch.from_numpy(np.array(state))\n",
    "    state = F.one_hot(state, num_classes=env.observation_space.n).float()\n",
    "    probs = policy(Variable(state))\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "\n",
    "    return action.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env, policy):\n",
    "    \"\"\"\n",
    "    Generate an episode\n",
    "    Args:\n",
    "        env (gym.env)\n",
    "    \"\"\"\n",
    "    \n",
    "    obs = env.reset()\n",
    "    ep_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while True:\n",
    "        action = act(obs)\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        policy.rewards.append(reward)\n",
    "        ep_reward += reward\n",
    "        obs = next_obs\n",
    "        \n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_discount_return(rewards, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Calculate the discount return by given rewards series\n",
    "    Args:\n",
    "        rewards (np.array)\n",
    "    \"\"\"\n",
    "    G = 0\n",
    "    T = len(rewards)\n",
    "    returns = []\n",
    "    for r in reversed(rewards):\n",
    "        G = gamma * G + r\n",
    "        returns.insert(0, G)\n",
    "    \n",
    "    # Flip the returns list\n",
    "#     reversed_returns = reversed(returns)\n",
    "    \n",
    "    return torch.FloatTensor(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_update(episode_series, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Calculate the discounted return by a given episode series\n",
    "    We use the start value of the episode as the performance measure function\n",
    "    Recall the return of a monte carlo policy is\n",
    "    G = sum(t+1:T)(gamma^(k-t-1))Rk\n",
    "    Args:\n",
    "        episode_series (list)\n",
    "        gamma (float)\n",
    "    Return:\n",
    "        Return (float)\n",
    "    \"\"\"\n",
    "\n",
    "    advantage_torch = calculate_discount_return(policy.rewards)\n",
    "    advantage_torch = (advantage_torch - advantage_torch.mean()) / \\\n",
    "        (advantage_torch.std() + np.finfo(np.float32).eps)\n",
    "    probs_torch = torch.stack(policy.saved_log_probs)\n",
    "    \n",
    "    # Calculate performance measure (~loss) function\n",
    "    # Use expected average reward as the advantage At\n",
    "\n",
    "    loss = torch.neg(torch.matmul(probs_torch, advantage_torch))\n",
    "\n",
    "    # Update network weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Log the records\n",
    "    policy.loss_history.append(loss.item())\n",
    "    policy.reward_history.append(np.sum(policy.rewards))\n",
    "    \n",
    "    return loss, np.sum(policy.rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running one step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PolicyNetwork(\n",
      "  (fc1): Linear(in_features=500, out_features=128, bias=True)\n",
      "  (dropout1): Dropout(p=0.5)\n",
      "  (fc2): Linear(in_features=128, out_features=6, bias=True)\n",
      "  (softmax): Softmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(-1.8043, grad_fn=<SqueezeBackward1>), tensor(-1.7619, grad_fn=<SqueezeBackward1>), tensor(-1.8048, grad_fn=<SqueezeBackward1>), tensor(-1.8061, grad_fn=<SqueezeBackward1>), tensor(-1.8489, grad_fn=<SqueezeBackward1>), tensor(-1.7756, grad_fn=<SqueezeBackward1>), tensor(-1.8064, grad_fn=<SqueezeBackward1>), tensor(-1.7959, grad_fn=<SqueezeBackward1>), tensor(-1.7964, grad_fn=<SqueezeBackward1>), tensor(-1.8477, grad_fn=<SqueezeBackward1>), tensor(-1.7697, grad_fn=<SqueezeBackward1>), tensor(-1.8035, grad_fn=<SqueezeBackward1>), tensor(-1.7629, grad_fn=<SqueezeBackward1>), tensor(-1.7908, grad_fn=<SqueezeBackward1>), tensor(-1.7968, grad_fn=<SqueezeBackward1>), tensor(-1.7823, grad_fn=<SqueezeBackward1>), tensor(-1.7841, grad_fn=<SqueezeBackward1>), tensor(-1.8489, grad_fn=<SqueezeBackward1>), tensor(-1.7554, grad_fn=<SqueezeBackward1>), tensor(-1.8016, grad_fn=<SqueezeBackward1>), tensor(-1.8395, grad_fn=<SqueezeBackward1>), tensor(-1.7568, grad_fn=<SqueezeBackward1>), tensor(-1.8440, grad_fn=<SqueezeBackward1>), tensor(-1.7955, grad_fn=<SqueezeBackward1>), tensor(-1.7719, grad_fn=<SqueezeBackward1>), tensor(-1.8540, grad_fn=<SqueezeBackward1>), tensor(-1.7572, grad_fn=<SqueezeBackward1>), tensor(-1.7951, grad_fn=<SqueezeBackward1>), tensor(-1.7704, grad_fn=<SqueezeBackward1>), tensor(-1.7933, grad_fn=<SqueezeBackward1>), tensor(-1.7823, grad_fn=<SqueezeBackward1>), tensor(-1.7874, grad_fn=<SqueezeBackward1>), tensor(-1.7801, grad_fn=<SqueezeBackward1>), tensor(-1.8053, grad_fn=<SqueezeBackward1>), tensor(-1.7658, grad_fn=<SqueezeBackward1>), tensor(-1.8517, grad_fn=<SqueezeBackward1>), tensor(-1.7959, grad_fn=<SqueezeBackward1>), tensor(-1.7575, grad_fn=<SqueezeBackward1>), tensor(-1.7646, grad_fn=<SqueezeBackward1>), tensor(-1.7874, grad_fn=<SqueezeBackward1>), tensor(-1.7833, grad_fn=<SqueezeBackward1>), tensor(-1.7695, grad_fn=<SqueezeBackward1>), tensor(-1.7858, grad_fn=<SqueezeBackward1>), tensor(-1.7569, grad_fn=<SqueezeBackward1>), tensor(-1.7705, grad_fn=<SqueezeBackward1>), tensor(-1.7941, grad_fn=<SqueezeBackward1>), tensor(-1.7714, grad_fn=<SqueezeBackward1>), tensor(-1.8095, grad_fn=<SqueezeBackward1>), tensor(-1.7606, grad_fn=<SqueezeBackward1>), tensor(-1.7543, grad_fn=<SqueezeBackward1>), tensor(-1.8430, grad_fn=<SqueezeBackward1>), tensor(-1.7936, grad_fn=<SqueezeBackward1>), tensor(-1.7477, grad_fn=<SqueezeBackward1>), tensor(-1.7786, grad_fn=<SqueezeBackward1>), tensor(-1.7742, grad_fn=<SqueezeBackward1>), tensor(-1.7976, grad_fn=<SqueezeBackward1>), tensor(-1.7826, grad_fn=<SqueezeBackward1>), tensor(-1.8522, grad_fn=<SqueezeBackward1>), tensor(-1.8030, grad_fn=<SqueezeBackward1>), tensor(-1.8346, grad_fn=<SqueezeBackward1>), tensor(-1.7974, grad_fn=<SqueezeBackward1>), tensor(-1.7795, grad_fn=<SqueezeBackward1>), tensor(-1.7865, grad_fn=<SqueezeBackward1>), tensor(-1.7878, grad_fn=<SqueezeBackward1>), tensor(-1.7949, grad_fn=<SqueezeBackward1>), tensor(-1.7735, grad_fn=<SqueezeBackward1>), tensor(-1.7817, grad_fn=<SqueezeBackward1>), tensor(-1.7899, grad_fn=<SqueezeBackward1>), tensor(-1.8528, grad_fn=<SqueezeBackward1>), tensor(-1.7865, grad_fn=<SqueezeBackward1>), tensor(-1.7640, grad_fn=<SqueezeBackward1>), tensor(-1.8502, grad_fn=<SqueezeBackward1>), tensor(-1.8034, grad_fn=<SqueezeBackward1>), tensor(-1.7728, grad_fn=<SqueezeBackward1>), tensor(-1.8381, grad_fn=<SqueezeBackward1>), tensor(-1.7860, grad_fn=<SqueezeBackward1>), tensor(-1.7878, grad_fn=<SqueezeBackward1>), tensor(-1.7891, grad_fn=<SqueezeBackward1>), tensor(-1.7817, grad_fn=<SqueezeBackward1>), tensor(-1.8457, grad_fn=<SqueezeBackward1>), tensor(-1.8541, grad_fn=<SqueezeBackward1>), tensor(-1.8481, grad_fn=<SqueezeBackward1>), tensor(-1.7754, grad_fn=<SqueezeBackward1>), tensor(-1.7781, grad_fn=<SqueezeBackward1>), tensor(-1.7799, grad_fn=<SqueezeBackward1>), tensor(-1.8380, grad_fn=<SqueezeBackward1>), tensor(-1.8483, grad_fn=<SqueezeBackward1>), tensor(-1.7582, grad_fn=<SqueezeBackward1>), tensor(-1.8548, grad_fn=<SqueezeBackward1>), tensor(-1.8032, grad_fn=<SqueezeBackward1>), tensor(-1.7716, grad_fn=<SqueezeBackward1>), tensor(-1.7876, grad_fn=<SqueezeBackward1>), tensor(-1.7768, grad_fn=<SqueezeBackward1>), tensor(-1.8503, grad_fn=<SqueezeBackward1>), tensor(-1.7896, grad_fn=<SqueezeBackward1>), tensor(-1.8440, grad_fn=<SqueezeBackward1>), tensor(-1.8058, grad_fn=<SqueezeBackward1>), tensor(-1.7778, grad_fn=<SqueezeBackward1>), tensor(-1.8543, grad_fn=<SqueezeBackward1>), tensor(-1.7794, grad_fn=<SqueezeBackward1>), tensor(-1.7606, grad_fn=<SqueezeBackward1>), tensor(-1.7827, grad_fn=<SqueezeBackward1>), tensor(-1.7871, grad_fn=<SqueezeBackward1>), tensor(-1.7844, grad_fn=<SqueezeBackward1>), tensor(-1.7913, grad_fn=<SqueezeBackward1>), tensor(-1.7615, grad_fn=<SqueezeBackward1>), tensor(-1.7813, grad_fn=<SqueezeBackward1>), tensor(-1.7944, grad_fn=<SqueezeBackward1>), tensor(-1.7718, grad_fn=<SqueezeBackward1>), tensor(-1.8354, grad_fn=<SqueezeBackward1>), tensor(-1.7952, grad_fn=<SqueezeBackward1>), tensor(-1.7910, grad_fn=<SqueezeBackward1>), tensor(-1.7563, grad_fn=<SqueezeBackward1>), tensor(-1.7580, grad_fn=<SqueezeBackward1>), tensor(-1.7487, grad_fn=<SqueezeBackward1>), tensor(-1.8379, grad_fn=<SqueezeBackward1>), tensor(-1.7873, grad_fn=<SqueezeBackward1>), tensor(-1.7958, grad_fn=<SqueezeBackward1>), tensor(-1.8046, grad_fn=<SqueezeBackward1>), tensor(-1.7791, grad_fn=<SqueezeBackward1>), tensor(-1.7854, grad_fn=<SqueezeBackward1>), tensor(-1.7847, grad_fn=<SqueezeBackward1>), tensor(-1.7972, grad_fn=<SqueezeBackward1>), tensor(-1.7821, grad_fn=<SqueezeBackward1>), tensor(-1.7804, grad_fn=<SqueezeBackward1>), tensor(-1.7940, grad_fn=<SqueezeBackward1>), tensor(-1.7905, grad_fn=<SqueezeBackward1>), tensor(-1.7878, grad_fn=<SqueezeBackward1>), tensor(-1.8009, grad_fn=<SqueezeBackward1>), tensor(-1.7863, grad_fn=<SqueezeBackward1>), tensor(-1.7616, grad_fn=<SqueezeBackward1>), tensor(-1.7816, grad_fn=<SqueezeBackward1>), tensor(-1.7939, grad_fn=<SqueezeBackward1>), tensor(-1.8325, grad_fn=<SqueezeBackward1>), tensor(-1.7784, grad_fn=<SqueezeBackward1>), tensor(-1.7781, grad_fn=<SqueezeBackward1>), tensor(-1.7873, grad_fn=<SqueezeBackward1>), tensor(-1.7938, grad_fn=<SqueezeBackward1>), tensor(-1.7957, grad_fn=<SqueezeBackward1>), tensor(-1.7818, grad_fn=<SqueezeBackward1>), tensor(-1.7880, grad_fn=<SqueezeBackward1>), tensor(-1.8017, grad_fn=<SqueezeBackward1>), tensor(-1.7886, grad_fn=<SqueezeBackward1>), tensor(-1.8390, grad_fn=<SqueezeBackward1>), tensor(-1.7946, grad_fn=<SqueezeBackward1>), tensor(-1.7859, grad_fn=<SqueezeBackward1>), tensor(-1.8359, grad_fn=<SqueezeBackward1>), tensor(-1.8050, grad_fn=<SqueezeBackward1>), tensor(-1.7945, grad_fn=<SqueezeBackward1>), tensor(-1.7605, grad_fn=<SqueezeBackward1>), tensor(-1.7859, grad_fn=<SqueezeBackward1>), tensor(-1.7867, grad_fn=<SqueezeBackward1>), tensor(-1.7387, grad_fn=<SqueezeBackward1>), tensor(-1.8067, grad_fn=<SqueezeBackward1>), tensor(-1.7865, grad_fn=<SqueezeBackward1>), tensor(-1.7483, grad_fn=<SqueezeBackward1>), tensor(-1.7552, grad_fn=<SqueezeBackward1>), tensor(-1.7828, grad_fn=<SqueezeBackward1>), tensor(-1.7996, grad_fn=<SqueezeBackward1>), tensor(-1.7777, grad_fn=<SqueezeBackward1>), tensor(-1.7951, grad_fn=<SqueezeBackward1>), tensor(-1.8030, grad_fn=<SqueezeBackward1>), tensor(-1.7679, grad_fn=<SqueezeBackward1>), tensor(-1.8114, grad_fn=<SqueezeBackward1>), tensor(-1.8338, grad_fn=<SqueezeBackward1>), tensor(-1.7515, grad_fn=<SqueezeBackward1>), tensor(-1.7904, grad_fn=<SqueezeBackward1>), tensor(-1.8349, grad_fn=<SqueezeBackward1>), tensor(-1.8052, grad_fn=<SqueezeBackward1>), tensor(-1.7538, grad_fn=<SqueezeBackward1>), tensor(-1.7892, grad_fn=<SqueezeBackward1>), tensor(-1.7615, grad_fn=<SqueezeBackward1>), tensor(-1.7512, grad_fn=<SqueezeBackward1>), tensor(-1.7747, grad_fn=<SqueezeBackward1>), tensor(-1.8465, grad_fn=<SqueezeBackward1>), tensor(-1.8140, grad_fn=<SqueezeBackward1>), tensor(-1.7975, grad_fn=<SqueezeBackward1>), tensor(-1.7915, grad_fn=<SqueezeBackward1>), tensor(-1.7854, grad_fn=<SqueezeBackward1>), tensor(-1.7890, grad_fn=<SqueezeBackward1>), tensor(-1.7601, grad_fn=<SqueezeBackward1>), tensor(-1.7726, grad_fn=<SqueezeBackward1>), tensor(-1.7861, grad_fn=<SqueezeBackward1>), tensor(-1.8373, grad_fn=<SqueezeBackward1>), tensor(-1.7641, grad_fn=<SqueezeBackward1>), tensor(-1.7821, grad_fn=<SqueezeBackward1>), tensor(-1.7978, grad_fn=<SqueezeBackward1>), tensor(-1.7973, grad_fn=<SqueezeBackward1>), tensor(-1.7834, grad_fn=<SqueezeBackward1>), tensor(-1.7753, grad_fn=<SqueezeBackward1>), tensor(-1.7915, grad_fn=<SqueezeBackward1>), tensor(-1.7657, grad_fn=<SqueezeBackward1>), tensor(-1.7738, grad_fn=<SqueezeBackward1>), tensor(-1.7924, grad_fn=<SqueezeBackward1>), tensor(-1.7456, grad_fn=<SqueezeBackward1>), tensor(-1.7896, grad_fn=<SqueezeBackward1>), tensor(-1.7929, grad_fn=<SqueezeBackward1>), tensor(-1.8007, grad_fn=<SqueezeBackward1>), tensor(-1.7943, grad_fn=<SqueezeBackward1>), tensor(-1.7840, grad_fn=<SqueezeBackward1>)]\n"
     ]
    }
   ],
   "source": [
    "generate_episode(env, policy)\n",
    "ep_loss, ep_rewards = gradient_update(policy.rewards)\n",
    "print(policy.saved_log_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 4000 \tLoss: tensor(0.5291, grad_fn=<NegBackward>) \tAverage episode Rewards: -220.887"
     ]
    }
   ],
   "source": [
    "# Generate 10 episodes\n",
    "running_rewards = deque(maxlen=100)\n",
    "\n",
    "for i in range(5000):\n",
    "    generate_episode(env, policy)\n",
    "    ep_loss, ep_rewards = gradient_update(policy.rewards)\n",
    "    running_rewards.append(ep_rewards)\n",
    "    policy.reset()\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print('\\rEpisode: %s \\tLoss: %s \\tAverage episode Rewards: %s' % (i, ep_loss, np.mean(running_rewards)), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce variance by using with a critic\n",
    "The problem of REINFORCE is that it has a high variance, and we can use a \"baseline\" to reduce variance. \n",
    "\n",
    "Therefore, we use a \"critic\" to estimate the action-value function, just as what we have done in value approximation\n",
    "\\begin{equation}\n",
    "Q_{w}(s, a) \\approx Q^{\\pi_{\\theta}}(s, a)\n",
    "\\end{equation}\n",
    "\n",
    "Now, we have two sets of parameters:\n",
    "1. Critic - Updates action-value function parameters $w$\n",
    "2. Actor - Updates policy parameters $\\theta$, in direction suggested by critic\n",
    "\n",
    "Hence, the actor-critic algorithms follow an approximate policy gradient\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\nabla_{\\theta} J(\\theta) &= \\mathop{\\mathbb{E}_{\\pi_{\\theta}}}\\big[\\nabla_{\\theta} \\log pi_{\\theta}(s, a) Q_{w}(s,a) \\big]\\\\\n",
    "\\Delta \\theta &= \\alpha \\nabla_{\\theta} \\log \\pi_{\\theta}(s,a) Q_{w}(s,a)\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From REINFORCE to Actor-Critic\n",
    "\n",
    "The baseline can take various values. The set of equations below illustrates the classic variants of actor critic methods. \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\nabla_{\\theta}J(\\theta) &= \\mathop{\\mathbb{E}_{\\pi_{\\theta}}}\\big[\\nabla_{\\theta} \\log \\pi_{\\theta}(s,a)G_{t} \\big] \\hspace{3cm} \\text{REINFORCE}\\\\\n",
    "&= \\mathop{\\mathbb{E}_{\\pi_{\\theta}}}\\big[\\nabla_{\\theta} \\log \\pi_{\\theta}(s,a)Q^{w}(s,a) \\big] \\hspace{2cm} \\text{Q Actor-Critic}\\\\\n",
    "&= \\mathop{\\mathbb{E}_{\\pi_{\\theta}}}\\big[\\nabla_{\\theta} \\log \\pi_{\\theta}(s,a)A^{w}(s,a) \\big] \\hspace{2cm} \\text{Q Advantage Actor-Critic}\\\\\n",
    "&= \\mathop{\\mathbb{E}_{\\pi_{\\theta}}}\\big[\\nabla_{\\theta} \\log \\pi_{\\theta}(s,a)\\delta \\big] \\hspace{3.2cm} \\text{Q TD Actor-Critic}\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Implementation examples: \n",
    "- https://www.datahubbs.com/policy-gradients-and-advantage-actor-critic/\n",
    "- https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm\n",
    "---\n",
    "```\n",
    "Input: a differentiable policy parameterization pi(a|s, theta)\n",
    "Input: a differentiable state-value function parameterization Q_w(s, a, w)\n",
    "Parameters: step sizes alpha_theta > 0; alpha_w > 0\n",
    "\n",
    "Loop forever for each episode:\n",
    "\n",
    "        Initialise S, theta\n",
    "        Sample a in pi_theta\n",
    "        \n",
    "        Loop while S is not terminal for each time step:\n",
    "                A = pi(.|S, theta) [policy(state)]\n",
    "                Take action A, observe S', R\n",
    "                delta = R + gamma * Q_w(S', A', w) - Q_w(S, A, w)  [TD(0) error, or advantage]\n",
    "                theta = theta + alpha_theta * grad_pi log pi_theta(s,a) Q_w(S,A)     [policy gradient update]\n",
    "                w = w + alpha_w * delta * x(s, a)    [TD(0)]\n",
    "                A = A', S = S'\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new Value Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    \"\"\"Value network for value approximation\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size):\n",
    "        super().__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # MLP layers\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_critic(env, policy_network, value_network, num_episodes, gamma=0.9)\n",
    "    \"\"\"\n",
    "    Actor Critic Algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        \n",
    "        obs = env.reset()\n",
    "        action = act(obs)\n",
    "        \n",
    "        while True:\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
