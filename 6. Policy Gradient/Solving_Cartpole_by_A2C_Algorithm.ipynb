{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/RLWH/reinforcement-learning-notebook/blob/master/6.%20Policy%20Gradient/Solving_Cartpole_by_A2C_Algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vFkwgNu42Zpb"
   },
   "source": [
    "# Solving Cartpole by A2C Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QLMKwdvv2rBX",
    "outputId": "5abd8d0c-e94c-4c31-a315-2b71c227e67f"
   },
   "outputs": [],
   "source": [
    "#@title Wrapper function for openai gym rendering\n",
    "import gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "import numpy as np\n",
    "\n",
    "from gym import logger as gymlogger\n",
    "from gym.wrappers import Monitor\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "%matplotlib inline\n",
    "gymlogger.set_level(40) #error only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oloPheFf2wg0"
   },
   "source": [
    "## Setup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F06B_mEw2z91"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "wKRCguBK8c6B",
    "outputId": "d3d82019-c8c6-4eec-ab11-95446f6c6b4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: 4\n",
      "Action space: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Observation space: %s\" % env.observation_space.shape[0])\n",
    "print(\"Action space: %s\" % env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ftmTGsQI8ggL"
   },
   "outputs": [],
   "source": [
    "GLOBAL_STEP = env.env.spec.max_episode_steps\n",
    "SCORE_REQUIREMENT = -198\n",
    "NUM_EPISODES = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U4arnz_SBGo1"
   },
   "source": [
    "## Setup the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WJUSzUFzBLJl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import deque\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QWBXuuRgAY6w"
   },
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Policy Network -> Update the policy gradient\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, fc1_units=32, fc2_units=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        Essentially, the forward pass return the Q value\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "#         x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "#         x = self.dropout2(x)\n",
    "        x = F.softmax(self.fc3(x), dim=0)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3hAlDfYKBJQZ"
   },
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Policy Network\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size=1, fc1_units=32, fc2_units=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        Essentially, the forward pass return the Q value\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "#         x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "#         x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "27NGtHiFEvyF",
    "outputId": "a2011393-26fe-4f23-8166-984b65489a51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Check GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yUaEnmDNEdyN"
   },
   "source": [
    "### Algorithm\n",
    "---\n",
    "```\n",
    "Input: a differentiable policy parameterization pi(a|s, theta)                   [Policy Network]\n",
    "Input: a differentiable state-value function parameterization Q_w(s, a, w)       [Value Network]\n",
    "Parameters: step sizes alpha_theta > 0; alpha_w > 0\n",
    "​\n",
    "Loop forever for each episode:\n",
    "​\n",
    "        Initialise S, theta\n",
    "        Sample a from policy network\n",
    "        \n",
    "        Loop while S is not terminal for each time step:\n",
    "                A = pi(.|S, theta) [policy(state)]\n",
    "                Take action A, observe S', R\n",
    "                delta = R + gamma * A(S', A', w) - A(S, A, w)  [TD(0) error, or advantage]\n",
    "                theta = theta + alpha_theta * grad_pi log pi_theta(s,a) A(S,A)     [policy gradient update]\n",
    "                w = w + alpha_w * delta * x(s, a)    [TD(0)]\n",
    "                A = A', S = S'\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mp720K_hBqCW"
   },
   "outputs": [],
   "source": [
    "class A2CAgent:\n",
    "    \"\"\"Actor Critic Agent\"\"\"\n",
    "    \n",
    "    def __init__(self, n_state, n_action, policy_network):\n",
    "        \n",
    "        self.env = env\n",
    "        \n",
    "        self.n_state = n_state\n",
    "        self.n_action = n_action\n",
    "        \n",
    "        # Initialise the model\n",
    "        self.policy_network = policy_network\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = state.float()\n",
    "        probs = self.policy_network(state)\n",
    "#         value = self.value_network(Variable(state))\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        log_probs = m.log_prob(action)\n",
    "        entropy = m.entropy()\n",
    "#         policy.saved_log_probs.append(log_prob)\n",
    "\n",
    "        return log_probs, action.item(), entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "t4EmCVkZJFtr",
    "outputId": "c3f7033e-7c9e-47a0-a399-8b95abdbb8a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 \t Average Score: 15.0\n"
     ]
    }
   ],
   "source": [
    "observation_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "\n",
    "policy_network = PolicyNetwork(observation_space, action_space, fc1_units=128, fc2_units=64)\n",
    "value_network = ValueNetwork(observation_space, fc1_units=128, fc2_units=64)\n",
    "\n",
    "# critic_optimizer = torch.optim.Adam(value_network.parameters(), lr=3e-4)\n",
    "# actor_optimizer = torch.optim.Adam(policy_network.parameters(), lr=1e-4)\n",
    "\n",
    "optimizer = torch.optim.Adam(list(value_network.parameters()) + list(policy_network.parameters()), lr=3e-4)\n",
    "\n",
    "agent = A2CAgent(observation_space, action_space, policy_network)\n",
    "\n",
    "reward_list = deque(maxlen=100)\n",
    "\n",
    "for i in range(10000 + 1):\n",
    "    \n",
    "    # Log the results\n",
    "    action_log_probs = []\n",
    "    rewards = []\n",
    "    states = []\n",
    "    targets = []\n",
    "    errors = []\n",
    "    t = 0\n",
    "    R = 0\n",
    "    \n",
    "    state = torch.FloatTensor(env.reset())\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        # Select and take action\n",
    "        log_probs, action, entropy = agent.act(state)\n",
    "        \n",
    "        # Sample R_t+1 and S_t+1\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        next_state = torch.FloatTensor(next_state)\n",
    "        \n",
    "        # Find the value of the next step\n",
    "        next_state_value = value_network(next_state)\n",
    "        current_state_value = value_network(state)\n",
    "        \n",
    "        td_target = reward + 0.99 * next_state_value\n",
    "        \n",
    "        # TD loss, or advantage\n",
    "        advantage = td_target - current_state_value.item()\n",
    "        value_loss = (advantage) ** 2\n",
    "        actor_loss = -(log_probs * advantage) - 0.1 * entropy\n",
    "        \n",
    "        total_loss = 0.5 * value_loss + actor_loss\n",
    "        \n",
    "        # Optimize\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        R += reward\n",
    "        \n",
    "        if done:\n",
    "            reward_list.append(R)\n",
    "            break\n",
    "            \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "    if i % 1000 == 0:\n",
    "        print(\"\\rEpisode %s \\t Average Score: %s\" % (i, np.mean(reward_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fu9qrRcB3zsv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Solving Cartpole by A2C Algorithm",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
