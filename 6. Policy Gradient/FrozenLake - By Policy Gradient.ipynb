{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole solving by Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    The policy network\n",
    "    Args:\n",
    "        n_inputs (int)\n",
    "        n_outputs (int)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super().__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        \n",
    "        self.reward_history = []\n",
    "        self.loss_history = []\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.n_inputs, self.n_outputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        Args:\n",
    "            x (torch.Tensor)\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "    def act(self, state):\n",
    "        probs = self.forward(torch.from_numpy(np.array(state)).float().unsqueeze(0))\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action).reshape(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preview the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "obs = env.reset()\n",
    "next_obs, reward, done, info = env.step(env.action_space.sample())\n",
    "env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02465587,  0.23436827,  0.01544156, -0.27958903])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preview the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = PolicyNetwork(env.observation_space.shape[0], env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.5000]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.forward(torch.from_numpy(next_obs).float().unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc1.weight', tensor([[-0.3598, -0.4244, -0.0431, -0.3849],\n",
       "                      [-0.3363,  0.1368, -0.1586,  0.4164]])),\n",
       "             ('fc1.bias', tensor([-0.3971, -0.1613]))])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, tensor([-0.6931], grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.act(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms\n",
    "\n",
    "---\n",
    "```\n",
    "Input: a differentiable policy parameterization pi(a|s, theta)\n",
    "Algorithm parameter: step size alpha > 0\n",
    "Initialise policy parameter theta with dimension d'\n",
    "\n",
    "Loop forever for each episode:\n",
    "        Generate an episode S0, A0, R1, ..., ST-1, AT-1, RT, following pi(.|., theta) (def generate())\n",
    "        Loop for each step of the episode t = 0, 1, ..., T-1\n",
    "        G = sum(t+1:T)(gamma^(k-t-1))Rk (def calculate_reward)\n",
    "        theta = theta + alpha * gamma^t * grad of ln pi(At|St, theta) * Gt\n",
    "```\n",
    "---\n",
    "####  Policy Gradient as a supervised learning problem\n",
    "http://karpathy.github.io/2016/05/31/rl/ \n",
    "\n",
    "> Okay, but what do we do if we do not have the correct label in the Reinforcement Learning setting?  \n",
    "Here is the Policy Gradients solution (again refer to diagram below). Our policy network calculated probability of going UP as 30% (logprob -1.2) and DOWN as 70% (logprob -0.36). We will now sample an action from this distribution; E.g. suppose we sample DOWN, and we will execute it in the game. **At this point notice one interesting fact: We could immediately fill in a gradient of 1.0 for DOWN as we did in supervised learning, and find the gradient vector that would encourage the network to be slightly more likely to do the DOWN action in the future.**\n",
    "\n",
    "> So we can immediately evaluate this gradient and that’s great, but the problem is that at least for now we do not yet know if going DOWN is good.   \n",
    "But the critical point is that that’s okay, because we can simply wait a bit and see!  \n",
    "For example in Pong we could wait until the end of the game, then take the reward we get (either +1 if we won or -1 if we lost), and enter that scalar as the gradient for the action we have taken (DOWN in this case). In the example below, going DOWN ended up to us losing the game (-1 reward). So if we fill in -1 for log probability of DOWN and do backprop we will find a gradient that discourages the network to take the DOWN action for that input in the future (and rightly so, since taking that action led to us losing the game)\n",
    "\n",
    "> Policy gradients is exactly the same as supervised learning with two minor differences:  \n",
    "    1) We don’t have the correct labels yi so as a “fake label” we substitute the action we happened to sample from the policy when it saw xi, and  \n",
    "    2) We modulate the loss for each example multiplicatively based on the eventual outcome, since we want to increase the log probability for actions that worked and decrease it for those that didn’t.  \n",
    "    So in summary our loss now looks like $\\sum_{i}A_{i}\\log p(yi∣xi)$, where yi is the action we happened to sample and Ai is a number that we call an advantage. \n",
    " \n",
    "i.e., using the sampled outcome of the SARSA sequence as the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn = PolicyNetwork(env.observation_space.shape[0], env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env, policy):\n",
    "    \"\"\"\n",
    "    Generate an episode\n",
    "    Args:\n",
    "        env (gym.env)\n",
    "    \"\"\"\n",
    "    \n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    episode_series = []\n",
    "    \n",
    "    while not done:\n",
    "        action, log_prob = policy.act(obs)\n",
    "        next_obs, rewards, done, _ = env.step(action)\n",
    "        episode_series.append((obs, next_obs, rewards, log_prob, done))\n",
    "        obs = next_obs\n",
    "        \n",
    "    return episode_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = generate_episode(env, pn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, next_obs, rewards, log_probs, dones = zip(*sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reward(episode_series, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Calculate the discounted return by a given episode series\n",
    "    We use the start value of the episode as the performance measure function\n",
    "    Recall the return of a monte carlo policy is\n",
    "    G = sum(t+1:T)(gamma^(k-t-1))Rk\n",
    "    Args:\n",
    "        episode_series (list)\n",
    "        gamma (float)\n",
    "    Return:\n",
    "        Return (float)\n",
    "    \"\"\"\n",
    "    G = 0\n",
    "    G_list = []\n",
    "    \n",
    "    obs, next_obs, rewards, log_probs, dones = zip(*episode_series)\n",
    "    for r in rewards:\n",
    "        G = gamma * G + r\n",
    "        G_list.append(G)\n",
    "    G_list = torch.FloatTensor(G_list)\n",
    "    G_list = (G_list - G_list.mean()) / (G_list.std() + eps)\n",
    "    return G_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps\n",
    "1. Loss function: $\\sum \\log p(A_t | S_t) * G_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, next_obs, rewards, log_probs, dones = zip(*sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_list = calculate_reward(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.5068, -2.1433, -1.8162, -1.5218, -1.2569, -1.0184, -0.8038, -0.6107,\n",
       "        -0.4368, -0.2804, -0.1396, -0.0129,  0.1012,  0.2038,  0.2962,  0.3794,\n",
       "         0.4542,  0.5215,  0.5822,  0.6367,  0.6858,  0.7300,  0.7698,  0.8056,\n",
       "         0.8378,  0.8668,  0.8928,  0.9163,  0.9375,  0.9565,  0.9736])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('fc1.weight', tensor([[-0.3934,  0.2478,  0.4851,  0.4900],\n",
      "        [ 0.0833,  0.4846,  0.2927, -0.0955]])), ('fc1.bias', tensor([ 0.2978, -0.3185]))])\n",
      "Episode: 0 \tLoss: tensor(0.5315, grad_fn=<SumBackward2>) \tSum of Reward: 15.0\n",
      "Episode: 1000 \tLoss: tensor(-3.7940, grad_fn=<SumBackward2>) \tSum of Reward: 11.698301698301698\n",
      "Episode: 2000 \tLoss: tensor(-1.2205, grad_fn=<SumBackward2>) \tSum of Reward: 10.91704147926037\n",
      "Episode: 3000 \tLoss: tensor(-0.9151, grad_fn=<SumBackward2>) \tSum of Reward: 10.548483838720427\n",
      "Episode: 4000 \tLoss: tensor(-1.1759, grad_fn=<SumBackward2>) \tSum of Reward: 10.3056735816046\n",
      "OrderedDict([('fc1.weight', tensor([[-0.6245, -2.2341,  1.4952,  3.0133],\n",
      "        [-0.1660,  3.0492, -1.0103, -2.7476]])), ('fc1.bias', tensor([1.0885, 1.5203]))])\n"
     ]
    }
   ],
   "source": [
    "pn = PolicyNetwork(env.observation_space.shape[0], env.action_space.n)\n",
    "optimizer = optim.Adam(pn.parameters(), lr=1e-2)\n",
    "\n",
    "print(pn.state_dict())\n",
    "\n",
    "total_rewards = []\n",
    "\n",
    "# Generate 10 episodes\n",
    "for i in range(5000):\n",
    "    \n",
    "    sample = generate_episode(env, pn)\n",
    "    obs, next_obs, rewards, log_probs, dones = zip(*sample)\n",
    "    \n",
    "    reward_sum = sum(rewards)\n",
    "    g_list = calculate_reward(sample)\n",
    "    \n",
    "    log_probs_torch = torch.cat(log_probs)\n",
    "    \n",
    "    loss = (torch.sum(torch.mul(log_probs_torch, g_list).mul(-1), -1))\n",
    "    \n",
    "    # Backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    total_rewards.append(reward_sum)\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print('\\rEpisode: %s \\tLoss: %s \\tSum of Reward: %s' % (i, loss, np.mean(total_rewards)))\n",
    "    \n",
    "print(pn.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
