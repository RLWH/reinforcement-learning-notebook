{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/RLWH/reinforcement-learning-notebook/blob/master/6.%20Policy%20Gradient%20Methods/Policy_Gradient_Methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3XqtpSFdtv-A"
   },
   "source": [
    "# Policy Gradient Methods\n",
    "\n",
    "In previous chapters, we have learnt almost all the methods that have been used action-value methods. They learned the values of actions and then selected actions based on their estimated action values. \n",
    "\n",
    "Instead, we canlearn a *parameterized policy* that can select actions without consulting a value function. Why?\n",
    "\n",
    "- Some of the optimal policies are not a deterministic policy. It can be a stochastic policy.\n",
    "- Using a parameterized policy can solve large-scale problem, as it uses approximation method.\n",
    "\n",
    "#### What is a Parameterized policy?\n",
    "A parameterized policy is a policy that takes in a parameter vector, denote as $\\vec{\\theta} \\in \\mathop{\\mathbb{R}}^{d'}$ for approximation. \n",
    "\n",
    "##### The Parameterized policy formal representation\n",
    "- Denote $\\vec{\\theta} \\in \\mathop{\\mathbb{R}}^{d'} $ for the policy's parameter vector, and \n",
    "- Denote\n",
    "$\\pi(a|s, \\vec{\\theta}) = \\Pr\\{A_t=a | S_t=s, \\vec{\\theta}_t=\\vec{\\theta}\\}$\n",
    "for the probability that action $a$ is taken at time $t$, given that the environment is in state $s$ at time $t$ with parameter $\\vec{\\theta}$. \n",
    "- If a method uses a learned value function as well, then the value function's weight vector is denoted $\\vec{w} \\in \\mathop{\\mathbb{R}}^d$, as in $\\hat{v}(s, \\vec{w})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nX5ixgzk5-6F"
   },
   "source": [
    "## Why policy approximation?\n",
    "In practice, to ensure exploration, we generally require that the policy never becomes deterministic. \n",
    "\n",
    "If a policy can be parameterized in any way, as long as the policy $\\pi(a|s, \\vec{\\theta})$ is differentiable w.r.t its parameters, then some optimality can be found. \n",
    "\n",
    "#### Advantages\n",
    "- Better convergence properties\n",
    "- Effective in high-dimensional or continuous action spaces\n",
    "- Can learn stochastic policies\n",
    "\n",
    "#### Disadvantages\n",
    "- Typically converge to a local rather than global optimum\n",
    "- Evaluating a policy is typically inefficient and high variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dDdyAsHYawxf"
   },
   "source": [
    "# 1. The objective function - Performance Measure $J(\\vec{\\theta})$\n",
    "How we learn and what are we learning for the policy?\n",
    "\n",
    "The key element of learning the policy parameter is based on the gradient of performance measure $J(\\vec{\\theta})$\n",
    "\n",
    "The policy learning parameter is based on the gradient of some **scalar performance measure**, $J(\\vec{\\theta})$ w.r.t the policy parameters $\\vec{\\theta}$. \n",
    "\n",
    "#### Optimisation base on Gradient Ascent\n",
    "These methods seek to maximise performance, so their updates approximate gradient *ascent* in $J$:\n",
    "\\begin{equation}\n",
    "\\vec{\\theta}_{t+1} = \\vec{\\theta}_t + \\alpha \\widehat{\\nabla J(\\vec{\\theta}_t)}\n",
    "\\end{equation},\n",
    "where $\\widehat{\\nabla J(\\vec{\\theta}_t)} \\in \\mathop{\\mathbb{R}}^d'$ is a stochastic estimate whose expectation approximates the gradient of the performance measure w.r.t its argument $\\vec{\\theta}_t$\n",
    "\n",
    "For all the methods that follow this general schema, we call them *policy gradient methods*. \n",
    "\n",
    "#### Actor-Critic?\n",
    "For the methods that learn approximations to both policy and value functions are often called *actor-critic methods*, where *'actor'* is a reference to the learned policy, and *'critic'* refers to the learned value function, usually a state-value function. This will be covered later\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FjaTyOy7OdJ2"
   },
   "source": [
    "# 2. The Policy Objective Functions\n",
    "\n",
    "The policy performance measurement function $J(\\vec{\\theta})$ is different in episodic environments and continuing environments. \n",
    "\n",
    "## Problem definition\n",
    "- Goal: Given policy $\\pi_{\\theta}(s, a)$ and parameter $\\vec{\\theta}$, find the best $\\vec{\\theta}$ -> Performance measure function. i.e. Miximising the performance measure $J(\\vec{\\theta})$\n",
    "- The quality of the policy $\\pi_{\\theta}$ is measured by the performance measure function $J(\\vec{\\theta})$\n",
    "- The setup of function $J(\\vec{\\theta})$ is different for episodic environment and continuing environment\n",
    "\n",
    "## The formulation of $J(\\vec{\\theta})$\n",
    "### Episodic environments\n",
    "For Episodic environments, we define the *performance measure* $J(\\vec{\\theta})$ as the value of the start state of the episode\n",
    "\\begin{equation}\n",
    "J_1(\\theta) = V_{\\pi_{\\theta}}(s_1) = \\mathop{\\mathbb{E}}[v_1]\n",
    "\\end{equation}\n",
    "where $v_{\\pi_{\\theta}}(s_1)$ is the true value function for $\\pi_{\\theta}$, the policy determined by $\\vec{\\theta}$. \n",
    "\n",
    "This essentially means that we want the value from the beginning state to be as high as possible, so from this start state I can have a good policy\n",
    "\n",
    "### Continuing environments\n",
    "For Continuing environments, we define *performance measure* $J(\\vec{\\theta})$ as the **average value**\n",
    "\\begin{equation}\n",
    "J_{\\text{avV}}(\\theta) = \\sum_{s}\\mu^{\\pi_{\\theta}}(s) V^{\\pi_{\\theta}}(s)\n",
    "\\end{equation}\n",
    "where $\\mu_{\\pi}(s) = p(S_t=s | \\pi)$ is the probability of being in state s in the long run\n",
    "\n",
    "OR the **average reward per time-step**\n",
    "\\begin{equation}\n",
    "J_{avR}(\\theta) = \\sum_s{\\mu_{\\pi_{\\theta}}(s)} \\sum_{a}\\pi_{\\theta}(s, a) \\sum_{r}p(r|s, a)r\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TOQpQ5R0lgxk"
   },
   "source": [
    "# 3. Policy Optimisation\n",
    "\n",
    "- Let $J(\\vec{\\theta})$ be any policy objective function\n",
    "- Policy gradient algorithms search for a local maximum in $J(\\vec{\\theta})$ by gradient ascent of the policy w.r.t to parameters $\\vec{\\theta}$\n",
    "\n",
    "\\begin{equation}\n",
    "\\Delta \\vec{\\theta} = \\alpha \\nabla_{\\theta} J(\\vec{\\theta})\n",
    "\\end{equation}\n",
    "- where $\\nabla_{\\theta}J(\\theta)$ is the policy gradient\n",
    "- and $\\alpha$ is a step-size parameter\n",
    "\n",
    "#### Computing an estimate of the policy gradient\n",
    "- Assume policy $\\pi_{\\theta}$ is differentiable almost everywhere\n",
    "- The goal is to compute $\\nabla_{\\theta}J(\\vec{\\theta}) = \\nabla_{\\theta} \\mathop{\\mathbb{E_\\mu}}[v_{\\pi_{\\theta}}(S)]$\n",
    "- We will use Monte Carlo samples to compute this gradient\n",
    "- Gradient ascent can be optimised by optimizers in TF in practice\n",
    "\n",
    "The policy gradient can be calculated computationally or analytically.\n",
    "\n",
    "#### Computing Gradients By Finite Differences\n",
    "- If there is no access to the policy gradient, we can use computational method $J(\\vec{\\theta} + \\mu) - J(\\vec{\\theta})$ to approximate the gradient\n",
    "\n",
    "#### Evaluating the differentiable policy gradients analytically\n",
    "\n",
    "As we defined that $J(\\vec{\\theta})$ depends on the state-value function $v_{\\pi_{\\theta}}$, which means it depends on both the action selections and the distribution of states in which those selections are made, and that both of these are affected by the policy parameter. \n",
    "\n",
    "But how can we differentiate the state-value function? The effect of the policy on the state distribution is a function of the environment, and it is typically unknown. So, how can we estimate the performance gradient $\\nabla J(\\vec{\\theta})$ w.r.t to the policy parameter when the gradient depends on the unknown effect of policy changes on the state distribution?\n",
    "\n",
    "Fortunately, as long as the policy itself is differentiable, we can use the *policy gradient theorem* to obtain an analytic expression for the gradient of performance measure. \n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IGN1-Rk4hM7j"
   },
   "source": [
    "## One -step MDP (Contextual Bandit problem) Gradient of performance measure\n",
    "Consider a one-step MDP (Contextual Bandit problem),  the performance measure is the expected reward that we can get $J(\\vec{\\theta}) = \\mathop{\\mathbb{E}}[R(S,A)]$\n",
    "- We want to calculate the gradient of $J(\\theta)$, where $\\nabla_{\\theta}J(\\vec{\\theta}) = \\nabla_{\\theta} \\mathop{\\mathbb{E_\\mu}}[v_{\\pi_{\\theta}}(S)]$. \n",
    "- Now, we use the identity $\\nabla_{\\theta} \\mathop{\\mathbb{E}}[R(S, A)] = \\mathop{\\mathbb{E}}[\\nabla_{\\theta} \\log \\pi_{\\theta} (A|S) R(S,A)]$\n",
    "- Then, the right-hand side gives an expected gradient that can be sampled\n",
    "- Then, under stochastic policy-gradient update, we then have\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_{t+1} = \\theta_t + \\alpha R_{t+1} \\nabla_{\\theta} \\log \\pi_{\\theta_t}(A_t | S_t)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "## Proof of the identity - The score function trick\n",
    "- Assume the policy $\\pi(\\theta)$ is differentiable whenever it is non-zero\n",
    "- Assume the gradient $\\nabla_{\\theta}\\pi_{\\theta}$ is known\n",
    "- The likelihood ratios exploit the following identity\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\nabla_{\\theta} \\mathop{\\mathbb{E}}[R(S, A)] &= \\nabla_{\\theta} \\sum_s{\\mu (s)}\\sum_a \\pi_{\\theta}(a|s)R(s,a) \\\\\n",
    "& = \\sum_s{\\mu(s)}\\sum_a \\nabla_{\\theta}\\pi_{\\theta}(a|s)R(s,a) \\\\\n",
    "& = \\sum_s \\mu(s) \\sum_a \\pi_{\\theta}(a|s) \\frac{\\nabla_{\\theta} \\pi_{\\theta} (a|s)}{\\pi_{\\theta}(a|s)}R(s,a) \\\\\n",
    "& = \\sum_s \\mu(s) \\sum_a \\pi_{\\theta}(a|s) \\nabla_{\\theta} \\log \\pi_{\\theta}(a|s) R(s,a) \\\\\n",
    "& = \\mathop{\\mathbb{E}}[\\nabla_{\\theta} \\log \\pi_{\\theta} (A|S) R(S,A)]\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "- And the score function is $\\nabla_{\\theta} \\log \\pi_{\\theta}(s, a)$, which is the log likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ASVIF4r3Txyl"
   },
   "source": [
    "## Theorem\n",
    "- To extend the porlicy gradient approach in one-step MDP, we can replace the instant reward from on-step MDP to long term action-value $Q_{\\pi_{\\theta}}$, we have\n",
    "> For any differentiable policy $\\pi_{\\theta}(s, a)$ \\\\\n",
    "> For any of the policy objective functions $J = J_1, J_{avR}, \\frac{1}{1-\\gamma}J_{avV}$\n",
    "> The policy gradient is\n",
    "> \\begin{equation}\n",
    "\\nabla_{\\theta}J(\\vec{\\theta}) = \\mathop{\\mathbb{E_{\\pi_{\\theta}}}}[\\nabla_{\\theta} \\log \\pi_{\\theta} (A|S) Q_{\\pi_{\\theta}}(S, A)]\n",
    "\\end{equation}\n",
    "- The policy gradient theorem applies to start state objective, average reward and average value objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q9I0hih80srv"
   },
   "source": [
    "## Score function under the softmax policy\n",
    "One of the differentiable policies is the softmax policy\n",
    "\n",
    "The actions with the highest preferences in each state are given the highest probabilities of being selected, for example, according to a softmax distribution:\n",
    "\\begin{equation}\n",
    "\\pi(a|s, \\vec{\\theta}) = \\frac{e^{h(s, a, \\vec{\\theta})}}{\\sum_{b}{e^{h(s, b, \\vec{\\theta})}}}\n",
    "\\end{equation}\n",
    ", where the function $h(s, a, \\vec{\\theta})$ is the parameterized state-action preference.\n",
    "\n",
    "We call that this kind of policy parameterization as softmax in action preferences\n",
    "\n",
    "The action preferences themselves can be parameterized arbitrarily, for example using a deep artificial neural network (ANN), or it could simply be linear in features. For example:\n",
    "\n",
    "\\begin{equation}\n",
    "h(s, a, \\vec{\\theta}) = \\vec{\\theta}^T \\vec{x}(s, a)\n",
    "\\end{equation},\n",
    "where $\\vec{x}(s, a) \\in \\mathop{\\mathbb{R}}^d'$ constructed by any methods described in Section 9.5. \n",
    "\n",
    "Since softmax policy is differentiable, we can find out its score function is\n",
    "\\begin{equation}\n",
    "\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "#### Summary\n",
    "- Policy function: $\\pi(a|s, \\vec{\\theta}) = \\frac{e^{h(s, a, \\vec{\\theta})}}{\\sum_{b}{e^{h(s, b, \\vec{\\theta})}}}$\n",
    "- Score function: $\\nabla_{\\theta} \\log \\pi_{\\theta}(s, a) = \\vec{x}(s, a) - \\mathop{\\mathbb{E}}_{\\pi_{\\theta}}[\\vec{x}(s, \\cdot)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Policy Gradient as a supervised learning problem\n",
    "http://karpathy.github.io/2016/05/31/rl/   \n",
    "https://amoudgl.github.io/blog/policy-gradient/\n",
    "\n",
    "An alternative perspective to see of policy gradient is from the angle of supervised learning. \n",
    "\n",
    "#### A deeper thought on cross entropy\n",
    "Recall that in supervised learning, say for a classification problem with C classes, we train the classifier with cross-entropy loss. Cross entropy computes the difference between the distribution of modelled data and the true label.\n",
    "\n",
    "\\begin{equation}\n",
    "H(p,q) = \\mathop{\\mathbb{E}_{x \\sim P}}\\big[-log Q(x)\\big]\n",
    "\\end{equation}\n",
    "\n",
    "$H(P, Q)$ means that we calculate the expectation using P and the encoding size using $Q$.  \n",
    "\n",
    "As such, $H(P, Q)$ and $H(Q, P)$ is not necessarily the same except when $Q=P$, in which case $H(P, Q) = H(P, P) = H(P)$ and it becomes the entropy itself.\n",
    "\n",
    "#### Cross Entropy as a loss function\n",
    "In a classification problem, suppose we have $C$ classes, $c_1, c_2, ..., c_C$, our job is to calculate the likelihood for each of the class base on the feature input. The label is always with 100% certainty. For a 5-class classification problem, we may have a table like this:\n",
    "\n",
    "|Prediction ($\\hat{y}$)|Label (y)|\n",
    "|----------|-----|\n",
    "|[0.4 0.3 0.05 0.05 0.2]|[1 0 0 0 0]\n",
    "\n",
    "The question is: How well was the model's prediction? We can calculate the cross-entropy as follows\n",
    "\\begin{equation}\n",
    "H(\\hat{y_{i}}, y_{i}) = -\\sum_{i}^{C} \\hat{y_i} \\log(y_i)\n",
    "\\end{equation}\n",
    "where $p_i$ and $l_i$ are the ground truth and the score for each class $i$ in C. As usually an activation function (softmax) will be used to calculate the score for each class\n",
    "\n",
    "If we have N training examples, then the cross-entropy loss over the samples will be $\\sum_{m=1}^{N} H(\\hat{y_i}, y_i)$\n",
    "\n",
    "The objective of a classification problem maximise the likelihood of the correct class. In other words, the objective is to build a model with $\\hat{\\theta}$ that maximizes the probability of the observed data. i.e. **Maximum Likelihood Estimation MLE**\n",
    "\\begin{equation}\n",
    "\\hat{\\theta} = {\\arg \\max}_{\\theta} \\prod_{i=1}^{N}p(x_i|\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "However, multiplication is unstable and it can go overflow or underflow easily. By taking $\\log$ of both sides, we can rewrite the formulation as a sum. \n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\theta} = {\\arg \\max}_{\\theta} \\sum_{i=1}^{N}\\log p(x_i|\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "[In practice](https://stats.stackexchange.com/questions/174481/why-to-optimize-max-log-probability-instead-of-probability/), instead of maximisin the log likelihood, we tend to minimise the negative log likelihood, and thus minimising the KL divergence, which is equivalent to minimising the cross entropy\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\theta} = {\\arg \\min}_{\\theta} -\\sum_{i=1}^{N}\\log p(x_i|\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "Here's a good article to revisit cross-entropy in deeper level  \n",
    "https://towardsdatascience.com/demystifying-cross-entropy-e80e3ad54a8\n",
    "https://jhui.github.io/2017/01/05/Deep-learning-Information-theory/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The samples from Monte Carlo as labels\n",
    "The problem of Reinforcement Learning is that we don't have true labels. However, as mentioned in previous lectures, we can generate labels by our experience. \n",
    "\n",
    "In policy gradient, we first run our agent for an episode and observe the reward. Since the actions follow a stochastic policy, it actions will be selected according to the \"scores\" of each action. \n",
    "\n",
    "Our objective is to find an optimal policy. So, we want\n",
    "- If the sequence of actions lead to a win, the reward is positive and we encourage all actions of that episode by miniizing the negative log likelihood between the actions we took and the network probabilities\n",
    "- If the sequence of actions leaf to a loss, the reward is negative and we discourage all actions that episode by maximizing the negative log likelihood between the actions we took and the network probabilities\n",
    "\n",
    "Thus, we tweak the log likelihood formulation of Supervised learning by adding an extra term called \"Advantage\" $A_t$, that represent whether the actions should be encouraged or discouraged. A good advantage would be the discounted reward over time.\n",
    "\n",
    "Thus, the formation becomes\n",
    "\\begin{equation}\n",
    "L = -\\sum_{i=1}^{N}A_i \\log p(x_i|\\theta)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nU8zeye7mGnt"
   },
   "source": [
    "# 4. Monte-Carlo Policy Gradient (REINFORCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j9OwE6pefDum"
   },
   "source": [
    "## Algorithms\n",
    "\n",
    "#### Steps:\n",
    "- Update parameters by stochastic gradient ascent\n",
    "- Using policy gradient theorem\n",
    "- Using return $G_t$ as an unbiased sample of $Q_{\\pi_{\\theta}}(s_t, a_t)$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\Delta \\theta_t &= \\alpha \\nabla_{\\theta} J(\\vec{\\theta}) \\\\\n",
    "&= \\alpha \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t, a_t)G_t\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "- Loss function $L = -\\sum G_t \\log \\pi(s_t, a_t|\\theta)$\n",
    "\n",
    "#### Algorithm\n",
    "---\n",
    "```\n",
    "Input: a differentiable policy parameterization pi(a|s, theta)\n",
    "Algorithm parameter: step size alpha > 0\n",
    "Initialise policy parameter theta with dimension d'\n",
    "\n",
    "Loop forever for each episode:\n",
    "        Generate an episode S0, A0, R1, ..., ST-1, AT-1, RT, following pi(.|., theta)\n",
    "        Loop for each step of the episode t = 0, 1, ..., T-1\n",
    "        G = sum(t+1:T)(gamma^(k-t-1))Rk\n",
    "        theta = theta + alpha * gamma^t * grad of ln pi(At|St, theta) * Gt\n",
    "```\n",
    "---\n",
    "\n",
    "The problem of Monte-Carlo policy is that the gradient still has high variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "usmPlLJMk7o7"
   },
   "source": [
    "# 5. From REINFORCE to Actor-Critic methods\n",
    "\n",
    "- Simple actor-critic algorithm based on action-value critic\n",
    "- Using linear value function approximation $Q_w(s, a) = \\vec{x}(s, a)^T \\vec{w}$\n",
    "- The critic updates $w$ by linear TD(0), and the actor updates $\\theta$ by policy gradient\n",
    "\n",
    "From REINFORCE, recall that from **Policy Gradient Theorem**\n",
    "\\begin{equation}\n",
    "\\nabla_{\\theta}J(\\theta) = \\mathop{\\mathbb{E}_{\\pi_{\\theta}}}\\big[\\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t)Q^{\\pi_{\\theta}}(s,a) \\big]\n",
    "\\end{equation}\n",
    "\n",
    "As we know, the Q value can be learnt by parameterizing the Q function with a neural network (parameters denoted by $w$), and this leads us to **Actor Critic Methods**, where\n",
    "- *'actor'* is a reference to the learned policy, and\n",
    "- *'critic'* refers to the learned value function, usually a state-value function.\n",
    "\n",
    "Both the Actor and Critic functions are parameterized with Neural Networks. We will particularly cover the Q Actor Critic, but the rest should be the same.\n",
    "\n",
    "#### Baselines and Advantage values\n",
    "Intuitively, the advantage function meansures how better it is to take a specific action compared to the average, general action at the given state.\n",
    "\n",
    "\\begin{equation}\n",
    "A(s_t, a_t) = Q_{w}(s_t, a_t) - V_{v}(s_t)\n",
    "\\end{equation}\n",
    "where $w$ parameterised the action value function $Q$ and $v$ parametised the state value function $V$. Does it mean that we need two neural networks? No.\n",
    "\n",
    "From the Bellman optimality equation, we know that \n",
    "\\begin{equation}\n",
    "Q(s_t, a_t) = \\mathop{\\mathbb{E}}\\big[r_{t+1} + \\gamma V(s_{t+1}) \\big]\n",
    "\\end{equation}\n",
    "\n",
    "Thus, the advantage function can be rewritten as\n",
    "\\begin{equation}\n",
    "A(s_t, a_t) = r_{t+1} + \\gamma V_{v}(s_{t+1}) - V_{v}(s_t)\n",
    "\\end{equation}\n",
    "and this approach only requires one set of critic parameters $v$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm\n",
    "---\n",
    "```\n",
    "Input: a differentiable policy parameterization pi(a|s, theta)\n",
    "Input: a differentiable state-value function parameterization Q_w(s, a, w)\n",
    "Parameters: step sizes alpha_theta > 0; alpha_w > 0\n",
    "\n",
    "Loop forever for each episode:\n",
    "\n",
    "        Initialise S, theta\n",
    "        Sample a in pi_theta\n",
    "        \n",
    "        Loop while S is not terminal for each time step:\n",
    "                A = pi(.|S, theta)\n",
    "                Take action A, observe S', R\n",
    "                delta = R + gamma * Q_w(S', A', w) - Q_w(S, A, w)  [TD(0) error, or advantage]\n",
    "                theta = theta + alpha_theta * grad_pi log pi_theta(s,a) Q_w(S,A)     [policy gradient update]\n",
    "                w = w + alpha_w * delta * x(s, a)    [TD(0)]\n",
    "                A = A', S = S'\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oVSrvfucwiJP"
   },
   "source": [
    "## Full advantage actor critic agent\n",
    "\n",
    "Advantage actor critic includes:\n",
    "- A representation (e.g. LSTM): $(S_{t-1}, O_t) \\mapsto S_t$\n",
    "- A network $v_{w}: S \\mapsto v$\n",
    "- A network $\\pi_{\\theta} \\mapsto \\pi$\n",
    "- Copies/varients $\\pi^m$ of $\\pi_{\\theta}$ to use as policies: $S_{t}^{m} \\mapsto A_{t}^{m}$\n",
    "- A n-step TD loss on $v_{w}$\n",
    "\n",
    "\\begin{equation}\n",
    "L(w) = \\frac{1}{2}(G_{t}^{n} - v_{w}(S_t))^2\n",
    "\\end{equation}\n",
    "where $G_t^(n) = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...$\n",
    "\n",
    "- A n-step REINFORECE loss on $\\pi_{theta}$\n",
    "\\begin{equation}\n",
    "L(\\theta) = \\big[ G_{t}^{(n)} - v_{w}(S_t) \\big] \\log \\pi_{theta}(A_t | S_t)\n",
    "\\end{equation}\n",
    "\n",
    "- And use optimizers to minimize the losses"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Policy Gradient Methods.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
