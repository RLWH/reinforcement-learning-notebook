{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From REINFORCE to Actor-Critic\n",
    "\n",
    "The baseline can take various values. The set of equations below illustrates the classic variants of actor critic methods. \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\nabla_{\\theta}J(\\theta) &= \\mathop{\\mathbb{E}_{\\pi_{\\theta}}}\\big[\\nabla_{\\theta} \\log \\pi_{\\theta}(s,a)G_{t} \\big] \\hspace{3cm} \\text{REINFORCE}\\\\\n",
    "&= \\mathop{\\mathbb{E}_{\\pi_{\\theta}}}\\big[\\nabla_{\\theta} \\log \\pi_{\\theta}(s,a)Q^{w}(s,a) \\big] \\hspace{2cm} \\text{Q Actor-Critic}\\\\\n",
    "&= \\mathop{\\mathbb{E}_{\\pi_{\\theta}}}\\big[\\nabla_{\\theta} \\log \\pi_{\\theta}(s,a)A^{w}(s,a) \\big] \\hspace{2cm} \\text{Q Advantage Actor-Critic}\\\\\n",
    "&= \\mathop{\\mathbb{E}_{\\pi_{\\theta}}}\\big[\\nabla_{\\theta} \\log \\pi_{\\theta}(s,a)\\delta \\big] \\hspace{3.2cm} \\text{Q TD Actor-Critic}\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Implementation examples: \n",
    "- https://www.datahubbs.com/policy-gradients-and-advantage-actor-critic/\n",
    "- https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import sys\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[43mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "next_obs, reward, done, info = env.step(env.action_space.sample())\n",
    "env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: 500\n",
      "Action space: 6\n"
     ]
    }
   ],
   "source": [
    "print(\"Observation space:\", env.observation_space.n)\n",
    "print(\"Action space:\", env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm\n",
    "---\n",
    "```\n",
    "Input: a differentiable policy parameterization pi(a|s, theta)                   [Policy Network]\n",
    "Input: a differentiable state-value function parameterization Q_w(s, a, w)       [Value Network]\n",
    "Parameters: step sizes alpha_theta > 0; alpha_w > 0\n",
    "\n",
    "Loop forever for each episode:\n",
    "\n",
    "        Initialise S, theta\n",
    "        Sample a in pi_theta\n",
    "        \n",
    "        Loop while S is not terminal for each time step:\n",
    "                A = pi(.|S, theta) [policy(state)]\n",
    "                Take action A, observe S', R\n",
    "                delta = R + gamma * Q_w(S', A', w) - Q_w(S, A, w)  [TD(0) error, or advantage]\n",
    "                theta = theta + alpha_theta * grad_pi log pi_theta(s,a) Q_w(S,A)     [policy gradient update]\n",
    "                w = w + alpha_w * delta * x(s, a)    [TD(0)]\n",
    "                A = A', S = S'\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logs:\n",
    "    \"\"\"\n",
    "    Global logs\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # History\n",
    "        self.loss_history = []\n",
    "        self.reward_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    The policy network\n",
    "    Args:\n",
    "        n_inputs (int)\n",
    "        n_outputs (int)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super().__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        \n",
    "        self.reward_history = []\n",
    "        self.loss_history = []\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.n_inputs, 128)\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(128, self.n_outputs)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        # save log probs history and rewards history\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        \n",
    "    def reset(self):\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        Args:\n",
    "            x (torch.Tensor)\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    \"\"\"Value network for value approximation\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size):\n",
    "        super().__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # MLP layers\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CAgent:\n",
    "    \"\"\"Actor Critic Agent\"\"\"\n",
    "    \n",
    "    def __init__(self, env, policy_network, value_network):\n",
    "        \n",
    "        self.env = env\n",
    "        self.policy_network = policy_network\n",
    "        self.value_network = value_network\n",
    "        \n",
    "        self.n_state = env.observation_space.n\n",
    "        self.n_action = env.action_space.n\n",
    "        \n",
    "        # Experience Replay\n",
    "        self.experience_memory = deque(maxlen=500)\n",
    "        \n",
    "    def create_model(self, model):\n",
    "        return model(self.n_state, self.n_action)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(np.array(state))\n",
    "        state = F.one_hot(state, num_classes=env.observation_space.n).float()\n",
    "        probs = self.policy_network(Variable(state))\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        policy.saved_log_probs.append(m.log_prob(action))\n",
    "\n",
    "        return action.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci",
   "language": "python",
   "name": "datasci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
