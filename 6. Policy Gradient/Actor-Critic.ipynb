{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import sys\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "next_obs, reward, done, info = env.step(env.action_space.sample())\n",
    "env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: 500\n",
      "Action space: 6\n"
     ]
    }
   ],
   "source": [
    "print(\"Observation space:\", env.observation_space.n)\n",
    "print(\"Action space:\", env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm\n",
    "---\n",
    "```\n",
    "Input: a differentiable policy parameterization pi(a|s, theta)                   [Policy Network]\n",
    "Input: a differentiable state-value function parameterization Q_w(s, a, w)       [Value Network]\n",
    "Parameters: step sizes alpha_theta > 0; alpha_w > 0\n",
    "\n",
    "Loop forever for each episode:\n",
    "\n",
    "        Initialise S, theta\n",
    "        Sample a from policy network\n",
    "        \n",
    "        Loop while S is not terminal for each time step:\n",
    "                A = pi(.|S, theta) [policy(state)]\n",
    "                Take action A, observe S', R\n",
    "                delta = R + gamma * A(S', A', w) - A(S, A, w)  [TD(0) error, or advantage]\n",
    "                theta = theta + alpha_theta * grad_pi log pi_theta(s,a) A(S,A)     [policy gradient update]\n",
    "                w = w + alpha_w * delta * x(s, a)    [TD(0)]\n",
    "                A = A', S = S'\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    The policy network\n",
    "    Args:\n",
    "        n_inputs (int)\n",
    "        n_outputs (int)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super().__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.n_inputs, 128)\n",
    "        self.fc2 = nn.Linear(128, self.n_outputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        Args:\n",
    "            x (torch.Tensor)\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    \"\"\"Value network for value approximation\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size):\n",
    "        super().__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # MLP layers\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CAgent:\n",
    "    \"\"\"Actor Critic Agent\"\"\"\n",
    "    \n",
    "    def __init__(self, n_state, n_action, policy_network, value_network, gamma=0.9):\n",
    "        \n",
    "        self.env = env\n",
    "        \n",
    "        self.n_state = n_state\n",
    "        self.n_action = n_action\n",
    "        \n",
    "        # Initialise the model\n",
    "        self.policy_network = policy_network(self.n_state, self.n_action)\n",
    "        self.value_network = value_network(self.n_state, 1)\n",
    "        \n",
    "        # Hypterparameters\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = state.float()\n",
    "        probs = self.policy_network(Variable(state))\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        log_probs = m.log_prob(action)\n",
    "#         policy.saved_log_probs.append(log_prob)\n",
    "\n",
    "        return log_probs, action.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 \t Average Score: -776.0\n",
      "Episode 10 \t Average Score: -765.4545454545455\n",
      "Episode 20 \t Average Score: -753.9047619047619\n",
      "Episode 30 \t Average Score: -761.3225806451613\n",
      "Episode 40 \t Average Score: -744.219512195122\n",
      "Episode 50 \t Average Score: -742.156862745098\n",
      "Episode 60 \t Average Score: -719.4754098360655\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-fa1de8c3d25a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mactor_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/Documents/Envs/datasci/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = A2CAgent(env.observation_space.n, env.action_space.n, PolicyNetwork, ValueNetwork)\n",
    "reward_list = deque(maxlen=100)\n",
    "\n",
    "critic_optimizer = torch.optim.Adam(agent.value_network.parameters(), lr=1e-3)\n",
    "actor_optimizer = torch.optim.Adam(agent.policy_network.parameters(), lr=1e-5)\n",
    "\n",
    "for i in range(501):\n",
    "    \n",
    "    # Log the results\n",
    "    action_log_probs = []\n",
    "    rewards = []\n",
    "    states = []\n",
    "    targets = []\n",
    "    errors = []\n",
    "    t = 0\n",
    "    R = 0\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        # Select and take action\n",
    "        state_tensor = torch.from_numpy(np.array(state))\n",
    "        state_one_hot = F.one_hot(state_tensor, num_classes=agent.n_state).float()\n",
    "        action_log_prob, action = agent.act(state_one_hot)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        next_state_tensor = torch.from_numpy(np.array(next_state))\n",
    "        next_state_one_hot = F.one_hot(next_state_tensor, num_classes=agent.n_state).float()\n",
    "        \n",
    "        # Calculate predictions and error\n",
    "        next_value =  agent.value_network(next_state_one_hot)\n",
    "        value = agent.value_network(state_one_hot)\n",
    "        td_target = reward + agent.gamma * next_value\n",
    "        td_error = td_target - value.detach()\n",
    "        \n",
    "        # Gradient update for policy optimizer\n",
    "        actor_optimizer.zero_grad()\n",
    "        critic_optimizer.zero_grad()\n",
    "        \n",
    "        critic_loss = torch.nn.MSELoss(reduction=\"sum\")(td_target, value)\n",
    "        actor_loss = -action_log_prob * td_error\n",
    "        \n",
    "        actor_loss.backward(retain_graph=True)\n",
    "        critic_loss.backward()\n",
    "#         total_loss.backward()\n",
    "\n",
    "        actor_optimizer.step()\n",
    "        critic_optimizer.step()\n",
    "        \n",
    "\n",
    "        R += reward\n",
    "        \n",
    "        if done:\n",
    "            reward_list.append(R)\n",
    "            break\n",
    "            \n",
    "        state = next_state\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(\"\\rEpisode %s \\t Average Score: %s\" % (i, np.mean(reward_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci",
   "language": "python",
   "name": "datasci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
