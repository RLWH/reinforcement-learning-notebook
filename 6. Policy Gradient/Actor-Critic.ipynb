{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import sys\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "next_obs, reward, done, info = env.step(env.action_space.sample())\n",
    "env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: 500\n",
      "Action space: 6\n"
     ]
    }
   ],
   "source": [
    "print(\"Observation space:\", env.observation_space.n)\n",
    "print(\"Action space:\", env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm\n",
    "---\n",
    "```\n",
    "Input: a differentiable policy parameterization pi(a|s, theta)                   [Policy Network]\n",
    "Input: a differentiable state-value function parameterization Q_w(s, a, w)       [Value Network]\n",
    "Parameters: step sizes alpha_theta > 0; alpha_w > 0\n",
    "\n",
    "Loop forever for each episode:\n",
    "\n",
    "        Initialise S, theta\n",
    "        Sample a from policy network\n",
    "        \n",
    "        Loop while S is not terminal for each time step:\n",
    "                A = pi(.|S, theta) [policy(state)]\n",
    "                Take action A, observe S', R\n",
    "                delta = R + gamma * A(S', A', w) - A(S, A, w)  [TD(0) error, or advantage]\n",
    "                theta = theta + alpha_theta * grad_pi log pi_theta(s,a) A(S,A)     [policy gradient update]\n",
    "                w = w + alpha_w * delta * x(s, a)    [TD(0)]\n",
    "                A = A', S = S'\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 158,
=======
   "execution_count": 5,
>>>>>>> d6617d5c690c23542333a3c655e7bb5c692ad759
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    The policy network\n",
    "    Args:\n",
    "        n_inputs (int)\n",
    "        n_outputs (int)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super().__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.n_inputs, 128)\n",
    "        self.fc2 = nn.Linear(128, self.n_outputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        Args:\n",
    "            x (torch.Tensor)\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 159,
=======
   "execution_count": 6,
>>>>>>> d6617d5c690c23542333a3c655e7bb5c692ad759
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    \"\"\"Value network for value approximation\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size):\n",
    "        super().__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # MLP layers\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 160,
=======
   "execution_count": 7,
>>>>>>> d6617d5c690c23542333a3c655e7bb5c692ad759
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CAgent:\n",
    "    \"\"\"Actor Critic Agent\"\"\"\n",
    "    \n",
    "    def __init__(self, n_state, n_action, policy_network, value_network, gamma=0.9):\n",
    "        \n",
    "        self.env = env\n",
    "        \n",
    "        self.n_state = n_state\n",
    "        self.n_action = n_action\n",
    "        \n",
    "        # Initialise the model\n",
    "        self.policy_network = policy_network(self.n_state, self.n_action)\n",
    "        self.value_network = value_network(self.n_state, 1)\n",
    "        \n",
    "        # Hypterparameters\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = state.float()\n",
    "        probs = self.policy_network(Variable(state))\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        log_probs = m.log_prob(action)\n",
    "#         policy.saved_log_probs.append(log_prob)\n",
    "\n",
    "        return log_probs, action.item()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 161,
=======
   "execution_count": 8,
>>>>>>> d6617d5c690c23542333a3c655e7bb5c692ad759
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES = 1000"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-a2732cb4ffc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mpolicy_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mactor_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0maction_log_probs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mactor_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mpolicy_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/envs/env/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/envs/env/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
=======
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 \t Average Score: -776.0\n",
      "Episode 10 \t Average Score: -765.4545454545455\n",
      "Episode 20 \t Average Score: -753.9047619047619\n",
      "Episode 30 \t Average Score: -761.3225806451613\n",
      "Episode 40 \t Average Score: -744.219512195122\n",
      "Episode 50 \t Average Score: -742.156862745098\n",
      "Episode 60 \t Average Score: -719.4754098360655\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-fa1de8c3d25a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mactor_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/Documents/Envs/datasci/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
>>>>>>> d6617d5c690c23542333a3c655e7bb5c692ad759
     ]
    }
   ],
   "source": [
    "agent = A2CAgent(env.observation_space.n, env.action_space.n, PolicyNetwork, ValueNetwork)\n",
    "reward_list = deque(maxlen=100)\n",
    "\n",
    "critic_optimizer = torch.optim.Adam(agent.value_network.parameters(), lr=1e-3)\n",
    "actor_optimizer = torch.optim.Adam(agent.policy_network.parameters(), lr=1e-5)\n",
    "\n",
    "for i in range(501):\n",
    "    \n",
    "    # Log the results\n",
    "    action_log_probs = []\n",
    "    rewards = []\n",
    "    states = []\n",
    "    targets = []\n",
    "    errors = []\n",
    "    t = 0\n",
    "    R = 0\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        # Select and take action\n",
    "        state_tensor = torch.from_numpy(np.array(state))\n",
    "        state_one_hot = F.one_hot(state_tensor, num_classes=agent.n_state).float()\n",
    "        action_log_prob, action = agent.act(state_one_hot)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        next_state_tensor = torch.from_numpy(np.array(next_state))\n",
    "        next_state_one_hot = F.one_hot(next_state_tensor, num_classes=agent.n_state).float()\n",
    "        \n",
    "        # Calculate predictions and error\n",
    "        next_value =  agent.value_network(next_state_one_hot)\n",
    "        value = agent.value_network(state_one_hot)\n",
    "        td_target = reward + agent.gamma * next_value\n",
    "        td_error = td_target - value.detach()\n",
    "        \n",
    "        # Gradient update for policy optimizer\n",
<<<<<<< HEAD
    "        value_optimizer = torch.optim.Adam(agent.value_network.parameters(), lr=1e-2)\n",
    "        value_optimizer.zero_grad()\n",
    "        critic_loss = torch.nn.MSELoss(reduction=\"sum\")(predicted_current_state_value, target)\n",
    "        critic_loss.backward()\n",
    "        value_optimizer.step()\n",
    "\n",
    "        \n",
    "        # Gradient update for value optimizer\n",
    "#         predicted_next_state_value =  agent.value_network(next_state_tensor)\n",
    "#         predicted_current_state_value = agent.value_network(state_tensor)\n",
    "#         target = reward + agent.gamma * predicted_next_state_value\n",
    "#         error = target - predicted_current_state_value\n",
=======
    "        actor_optimizer.zero_grad()\n",
    "        critic_optimizer.zero_grad()\n",
>>>>>>> d6617d5c690c23542333a3c655e7bb5c692ad759
    "        \n",
    "        critic_loss = torch.nn.MSELoss(reduction=\"sum\")(td_target, value)\n",
    "        actor_loss = -action_log_prob * td_error\n",
    "        \n",
    "        actor_loss.backward(retain_graph=True)\n",
    "        critic_loss.backward()\n",
    "#         total_loss.backward()\n",
    "\n",
    "        actor_optimizer.step()\n",
    "        critic_optimizer.step()\n",
    "        \n",
    "\n",
    "        R += reward\n",
    "        \n",
    "        if done:\n",
    "            reward_list.append(R)\n",
    "            break\n",
    "            \n",
    "        state = next_state\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(\"\\rEpisode %s \\t Average Score: %s\" % (i, np.mean(reward_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci",
   "language": "python",
   "name": "datasci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
