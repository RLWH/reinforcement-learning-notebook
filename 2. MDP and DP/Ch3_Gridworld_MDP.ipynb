{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch3 - Gridworld MDP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "8-zvxtZSkwEW",
        "bYVnIp_fcSux",
        "gumfrThpTpsy"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RLWH/reinforcement-learning-notebook/blob/master/Ch3%20MPD/Ch3_Gridworld_MDP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6_y_XpfB_fN",
        "colab_type": "text"
      },
      "source": [
        "# The Gridworld problem and solving gridworld by Dynamic Programming\n",
        "1. Policy Evaluation\n",
        "2. Policy Iteration\n",
        "3. Value Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-zvxtZSkwEW",
        "colab_type": "text"
      },
      "source": [
        "## Setup OpenAI Rendering in Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZsyYV41f3iv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6u7fFBJqBn03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOh5oCdjgYEJ",
        "colab_type": "code",
        "outputId": "f4a2a3e4-9604-4a86-e676-66984b8dbe4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '400x300x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '400x300x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoR0qk1fkE6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xm5goJOPkHlA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlCrQxFfjcrV",
        "colab_type": "code",
        "outputId": "b2aa4616-114a-430c-d6d0-51176b092919",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "env = wrap_env(gym.make('CartPole-v0'))\n",
        "# env.reset()\n",
        "for i_episode in range(20):\n",
        "    observation = env.reset()\n",
        "    for t in range(100):\n",
        "        env.render()\n",
        "#         print(observation)\n",
        "        action = env.action_space.sample()\n",
        "        observation, reward, done, info = env.step(action)\n",
        "        if done:\n",
        "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
        "            break\n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode finished after 12 timesteps\n",
            "Episode finished after 13 timesteps\n",
            "Episode finished after 14 timesteps\n",
            "Episode finished after 21 timesteps\n",
            "Episode finished after 23 timesteps\n",
            "Episode finished after 12 timesteps\n",
            "Episode finished after 16 timesteps\n",
            "Episode finished after 9 timesteps\n",
            "Episode finished after 12 timesteps\n",
            "Episode finished after 15 timesteps\n",
            "Episode finished after 24 timesteps\n",
            "Episode finished after 17 timesteps\n",
            "Episode finished after 11 timesteps\n",
            "Episode finished after 18 timesteps\n",
            "Episode finished after 28 timesteps\n",
            "Episode finished after 17 timesteps\n",
            "Episode finished after 31 timesteps\n",
            "Episode finished after 33 timesteps\n",
            "Episode finished after 12 timesteps\n",
            "Episode finished after 14 timesteps\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUc-QXiIkcTg",
        "colab_type": "code",
        "outputId": "28b47ced-b814-4cf9-c016-9e3004134cd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        }
      },
      "source": [
        "show_video()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYVnIp_fcSux",
        "colab_type": "text"
      },
      "source": [
        "## Setup the Gridworld Environment\n",
        "https://github.com/dennybritz/reinforcement-learning/blob/master/lib/envs/gridworld.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hr2NvZGjlK69",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "from collections import defaultdict\n",
        "from gym.envs.toy_text import discrete"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPUJfJdoB-Zn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "UP = 0\n",
        "RIGHT = 1\n",
        "DOWN = 2\n",
        "LEFT = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnEep269cZ3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GridworldEnv(discrete.DiscreteEnv):\n",
        "  \"\"\"\n",
        "  The Gridworld environment that is a discrete space\n",
        "  \n",
        "  Grid World environment from Sutton's Reinforcement Learning book chapter 4.\n",
        "  You are an agent on an MxN grid and your goal is to reach the terminal\n",
        "  state at the top left or the bottom right corner.\n",
        "\n",
        "  For example, a 4x4 grid looks as follows:\n",
        "\n",
        "  T  o  o  o\n",
        "  o  x  o  o\n",
        "  o  o  o  o\n",
        "  o  o  o  T\n",
        "\n",
        "  x is your position and T are the two terminal states.\n",
        "\n",
        "  You can take actions in each direction (UP=0, RIGHT=1, DOWN=2, LEFT=3).\n",
        "  Actions going off the edge leave you in your current state.\n",
        "  You receive a reward of -1 at each step until you reach a terminal state.\n",
        "  \n",
        "  A toy text discrete environment has the followings:\n",
        "  1. nS: Number of states\n",
        "  2. nA: Number of actions\n",
        "  3. P: transitions (*)\n",
        "  4. isd: initial state distribution\n",
        "  \n",
        "  (*) dictionary dict of dicts of lists, where\n",
        "    P[s][a] == [(probability, nextstate, reward, done), ...]\n",
        "  (**) list or array of length nS\n",
        "\n",
        "  \"\"\"\n",
        "  \n",
        "  metadata = {'render.modes': ['human', 'ansi']}\n",
        "  \n",
        "  def __init__(self, shape=[4, 4]):\n",
        "    if not isinstance(shape, (list, tuple)) or not len(shape) == 2:\n",
        "      raise ValueError('shape argument must be a list/tuple of length 2')\n",
        "      \n",
        "    self.shape = shape\n",
        "    \n",
        "    # The grid world has n x n states\n",
        "    nS = np.prod(shape)\n",
        "    \n",
        "    # There are only 4 possible actions\n",
        "    actions = [UP, RIGHT, DOWN, LEFT]\n",
        "    nA = len(actions)\n",
        "    \n",
        "    # Define the maximum board shape\n",
        "    MAX_Y = shape[0]\n",
        "    MAX_X = shape[1]\n",
        "    \n",
        "    # Define terminate state\n",
        "    is_done = lambda s: s == 0 or s == (nS - 1)\n",
        "    \n",
        "    \n",
        "    # Initialise P\n",
        "    P = {}\n",
        "    \n",
        "    grid = np.arange(nS).reshape(shape)\n",
        "    it = np.nditer(grid, flags=['multi_index'])\n",
        "    \n",
        "    while not it.finished:\n",
        "      s = it.iterindex\n",
        "      y, x = it.multi_index\n",
        "      \n",
        "      # Define Reward is -1 for each step\n",
        "      reward = 0.0 if is_done(s) else -1.0\n",
        "      \n",
        "      # Initialize P\n",
        "      P[s] = {a: [] for a in actions}\n",
        "      \n",
        "      if is_done(s):\n",
        "        for a in actions:\n",
        "          P[s][a] = [(1.0, s, reward, True)]\n",
        "      else:\n",
        "        \n",
        "        # Define the next state of each state\n",
        "        ns_up = s if y == 0 else s - MAX_X\n",
        "        ns_right = s if x == (MAX_X - 1) else s + 1\n",
        "        ns_down = s if y == (MAX_Y - 1) else s + MAX_X\n",
        "        ns_left = s if x == 0 else s - 1\n",
        "        \n",
        "        P[s][UP] = [(1.0, ns_up, reward, is_done(ns_up))]\n",
        "        P[s][RIGHT] = [(1.0, ns_right, reward, is_done(ns_right))]\n",
        "        P[s][DOWN] = [(1.0, ns_down, reward, is_done(ns_down))]\n",
        "        P[s][LEFT] = [(1.0, ns_left, reward, is_done(ns_left))]\n",
        "        \n",
        "        \n",
        "      it.iternext()\n",
        "      \n",
        "      # Define Initial state distribution is uniform\n",
        "      isd = np.ones(nS) / nS\n",
        "      \n",
        "      self.P = P\n",
        "      \n",
        "      \n",
        "      super().__init__(nS, nA, P, isd)\n",
        "    \n",
        "    \n",
        "  def render(self, mode='human', close=False):\n",
        "    \"\"\"\n",
        "    Render the environment\n",
        "    \"\"\"\n",
        "    if close:\n",
        "      return\n",
        "    \n",
        "    outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
        "    \n",
        "    grid = np.arange(self.nS).reshape(self.shape)\n",
        "    it = np.nditer(grid, flags=['multi_index'])\n",
        "    \n",
        "    while not it.finished:\n",
        "      s = it.iterindex\n",
        "      y, x = it.multi_index\n",
        "      \n",
        "      if self.s == s:\n",
        "        output = \"x \"\n",
        "      elif s == 0 or s == self.nS - 1:\n",
        "        output = \"T \"\n",
        "      else:\n",
        "        output = \"o \"\n",
        "        \n",
        "      if x == 0:\n",
        "        output = output.lstrip()\n",
        "        \n",
        "      if x == self.shape[1] - 1:\n",
        "        output = output.rstrip()\n",
        "\n",
        "      outfile.write(output)\n",
        "      \n",
        "      if x == self.shape[1] - 1:\n",
        "        outfile.write(\"\\n\")\n",
        "        \n",
        "      it.iternext()\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfK1HvfUgq0H",
        "colab_type": "code",
        "outputId": "62a0c672-7e95-4374-f8a0-775e2036fe7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "env = GridworldEnv()\n",
        "for t in range(100):\n",
        "    env.render()\n",
        "#         print(observation)\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
        "        break\n",
        "env.close()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "T o o o\n",
            "o x o o\n",
            "o o o o\n",
            "o o o T\n",
            "T o o o\n",
            "x o o o\n",
            "o o o o\n",
            "o o o T\n",
            "T o o o\n",
            "o x o o\n",
            "o o o o\n",
            "o o o T\n",
            "T x o o\n",
            "o o o o\n",
            "o o o o\n",
            "o o o T\n",
            "Episode finished after 4 timesteps\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAA5WbvcJJdp",
        "colab_type": "code",
        "outputId": "91203953-e58b-4416-88f0-34ecf25381a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "env.P[5]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [(1.0, 1, -1.0, False)],\n",
              " 1: [(1.0, 6, -1.0, False)],\n",
              " 2: [(1.0, 9, -1.0, False)],\n",
              " 3: [(1.0, 4, -1.0, False)]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SMo1yCulGft",
        "colab_type": "text"
      },
      "source": [
        "## Solving Gridworld by dynamic programming\n",
        "We will solve a Bellman equation using two algorithms:\n",
        "1. Value iteration\n",
        "2. Policy iteration\n",
        "\n",
        "Q(s,a) = Transition probability * ( Reward probability + gamma * value_of_next_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hho44WOAq869",
        "colab_type": "text"
      },
      "source": [
        "### Value Iteration\n",
        "In value iteration, we start off with a random value function, then look for a new improved value function in iterative fashion until we find the optimal value function.\n",
        "\n",
        "1. Initialise random value function\n",
        "2. For each state, calculate Q(s, a)\n",
        "3. Since V(s) = Max W(s, a), update the value function with max value of Q(s, a)\n",
        "4. If V(S) is optimal,  then stop. Repeat otherwise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKMVvAUiCgDC",
        "colab_type": "text"
      },
      "source": [
        "#### Environment Inspection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZ7hq1AZq8Gs",
        "colab_type": "code",
        "outputId": "5095d746-a5d8-4c18-83f7-f65d4621f20b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(env.observation_space.n)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Egcs01ElCpqJ",
        "colab_type": "code",
        "outputId": "0ac5d826-f506-4786-c32f-d68a5adeda06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(env.action_space.n)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS0IutbRDu09",
        "colab_type": "text"
      },
      "source": [
        "#### Initialise the value table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF7lrWT-CslQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "value_table = np.zeros(env.observation_space.n)\n",
        "no_of_iterations = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk341PepKzy8",
        "colab_type": "code",
        "outputId": "fe71b677-bf56-465d-c857-78f16325aef3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "value_table.reshape(4, 4)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gumfrThpTpsy",
        "colab_type": "text"
      },
      "source": [
        "#### For each state, calculate Q"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClBe9xT-Er6z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upon starting each iteration, we copy the value_table to updated_value_table\n",
        "for i in range(no_of_iterations):\n",
        "  updated_value_table = np.copy(value_table)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuzOqkJxImwI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for state in range(env.observation_space.n):\n",
        "  Q_value = []\n",
        "  \n",
        "  for action in range(env.action_space.n):\n",
        "    next_states_rewards = []\n",
        "    for next_sr in env.P[state][action]:\n",
        "      trans_prob, next_state, reward_prob, _ = next_sr\n",
        "      next_states_rewards.append((trans_prob * (reward_prob + 1 * updated_value_table[next_state])))\n",
        "      Q_value.append(np.sum(next_states_rewards))\n",
        "      \n",
        "      # Pick up the maximum Q value and update it as value of a state\n",
        "      value_table[state] = max(Q_value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0k600LTK1_d",
        "colab_type": "code",
        "outputId": "de120adb-d241-4d0f-fc0a-5a0c8096b99a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "value_table.reshape((4, 4))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0., -1., -1., -1.],\n",
              "       [-1., -1., -1., -1.],\n",
              "       [-1., -1., -1., -1.],\n",
              "       [-1., -1., -1.,  0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGvtjT-jNIML",
        "colab_type": "text"
      },
      "source": [
        "#### For each state, calculate Q - Combine them into a function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_S1w2MMNHNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def value_iteration(env, gamma=1.0, no_of_iterations=100000):\n",
        "  \"\"\"\n",
        "  Value iteration function\n",
        "  \"\"\"\n",
        "  \n",
        "  # Initialise value tables\n",
        "  value_table = np.zeros(env.observation_space.n)\n",
        "  threshold = 1e-20\n",
        "  \n",
        "  print(no_of_iterations)\n",
        "  \n",
        "  # Table update\n",
        "  for i in range(no_of_iterations):\n",
        "    \n",
        "    # Copy the current value table to updated value table\n",
        "    updated_value_table = np.copy(value_table)\n",
        "    \n",
        "    for state in range(env.observation_space.n):\n",
        "      print(\"state: %s\" % state, end=\"\")\n",
        "      # For each of the state, calculate the state-action value q(s,a)\n",
        "      Q_value = []\n",
        "      \n",
        "      for action in range(env.action_space.n):\n",
        "        # One step ahead search for each action\n",
        "        next_states_rewards = []\n",
        "        \n",
        "        for next_sr in env.P[state][action]:\n",
        "          # P[s][a] == [(probability, nextstate, reward, done), ...]\n",
        "#           print(next_sr)\n",
        "          trans_prob, next_state, reward, _ = next_sr\n",
        "          next_states_rewards.append((trans_prob * (reward + gamma * updated_value_table[next_state])))\n",
        "          \n",
        "        Q_value.append(np.sum(next_states_rewards))\n",
        "          \n",
        "      # Pick the maximum Q value and update it as value of a state\n",
        "      value_table[state] = max(Q_value)\n",
        "    \n",
        "    print(\"Diff: %s\" % np.sum(np.fabs(updated_value_table - value_table)))\n",
        "\n",
        "    if (np.sum(np.fabs(updated_value_table - value_table))) <= threshold:\n",
        "      print(\"Value-iteration converged at iteration %d\" % (i + 1))\n",
        "      break\n",
        "\n",
        "  return value_table"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlvzvAJgWExf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = GridworldEnv()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fuxMVv5Uxee",
        "colab_type": "code",
        "outputId": "e46fa084-ea04-40e1-ef09-9dba988dabbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "optimal_value_function = value_iteration(env=env, gamma=1.0)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100000\n",
            "state: 0state: 1state: 2state: 3state: 4state: 5state: 6state: 7state: 8state: 9state: 10state: 11state: 12state: 13state: 14state: 15Diff: 14.0\n",
            "state: 0state: 1state: 2state: 3state: 4state: 5state: 6state: 7state: 8state: 9state: 10state: 11state: 12state: 13state: 14state: 15Diff: 10.0\n",
            "state: 0state: 1state: 2state: 3state: 4state: 5state: 6state: 7state: 8state: 9state: 10state: 11state: 12state: 13state: 14state: 15Diff: 4.0\n",
            "state: 0state: 1state: 2state: 3state: 4state: 5state: 6state: 7state: 8state: 9state: 10state: 11state: 12state: 13state: 14state: 15Diff: 0.0\n",
            "Value-iteration converged at iteration 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNvERZAAUxch",
        "colab_type": "code",
        "outputId": "b9ad7e4b-21b8-4a92-b261-062bc2f007f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "optimal_value_function.reshape((4,4))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0., -1., -2., -3.],\n",
              "       [-1., -2., -3., -2.],\n",
              "       [-2., -3., -2., -1.],\n",
              "       [-3., -2., -1.,  0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lak2emOp2H2",
        "colab_type": "text"
      },
      "source": [
        "#### Extracting the optimal policy\n",
        "After finding optimal value function, how can we extract the optimal policy from optimal function?  \n",
        "We calculate the Q value using our optimal value action and pick up actions greedily for each state as the optimal policy.\n",
        "\n",
        "We do this via a function called `extract_policy()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oR-oNf0wy6-O",
        "colab_type": "text"
      },
      "source": [
        "#### Build a Q table for each state\n",
        "\n",
        "A Q table looks like the following:\n",
        "\n",
        "|State|Action|Value|\n",
        "|--------|-----------|--------|\n",
        "State 1|Action 1|Value 1|\n",
        "State 1|Action 2|Value 2|\n",
        "State 1|Action 3|Value 3|\n",
        "State 1|Action 4|Value 4|\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6i7ZBvaysCM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_policy(value_table, gamma=1.0):\n",
        "  \n",
        "  # First, Define a random policy pi\n",
        "  policy = np.zeros(env.observation_space.n)\n",
        "  \n",
        "  # Build a Q table - One step ahead\n",
        "  for state in range(env.observation_space.n):\n",
        "    # For each state, the Q table has num_actions \n",
        "    Q_table = np.zeros(env.action_space.n)\n",
        "\n",
        "    for action in range(env.action_space.n):\n",
        "\n",
        "      for next_sr in env.P[state][action]:\n",
        "        # One step look ahead\n",
        "        trans_prob, next_state, reward, _ = next_sr\n",
        "\n",
        "        new_value = trans_prob * (reward + gamma * value_table[next_state])\n",
        "\n",
        "        Q_table[action] = new_value\n",
        "        \n",
        "    policy[state] =  np.argmax(Q_table)\n",
        "    \n",
        "  return policy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaMXyqoLsB4s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimal_policy = extract_policy(optimal_value_function)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9Pqn_SxvMOy",
        "colab_type": "code",
        "outputId": "cae18534-1350-4fdf-af81-5367644792b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "optimal_policy.reshape(4,4)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 3., 3., 2.],\n",
              "       [0., 0., 0., 2.],\n",
              "       [0., 0., 1., 2.],\n",
              "       [0., 1., 1., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KXl5f5En8Vn",
        "colab_type": "text"
      },
      "source": [
        "### Policy Iteration\n",
        "1. Policy evaluation: Evaluating the value function of a randomly estimated policy\n",
        "2. Policy improvement: Upon evaluating the value function, if it is not optimal, we find a new improved policy $\\pi'$\n",
        "\n",
        "##### How can we evaluate the policies?  \n",
        "We will evaluate our randomly initialized policies by computing value functions for them. If they are not good, then we find a new policy.   \n",
        "We repeat this process until we find a good policy.\n",
        "\n",
        "#### Steps\n",
        "1. Initialize random policy $\\pi$\n",
        "2. Calculate value function V(S) for the policy\n",
        "3. If V(S) is optimal -> End, otherwise find improved policy\n",
        "4. Repeat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpLonAOfx1YM",
        "colab_type": "text"
      },
      "source": [
        "#### Initialize a random policy $\\pi$\n",
        "For each state, we get the action from policy, and compute the value function according to the `action` and `state` as folows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjCMxHJhF5ot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random_policy = np.ones([env.nS, env.nA]) / env.nA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT7nJjIKGDIc",
        "colab_type": "code",
        "outputId": "644df5c2-4082-4862-d19a-9e3103481d74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "random_policy[1]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.25, 0.25, 0.25, 0.25])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvuzjgsvURpM",
        "colab_type": "text"
      },
      "source": [
        "#### Calculate the v(s) of the policy (Prediction problem) - In place version\n",
        "\n",
        "Algorithm:\n",
        "Input $\\pi$, the policy to be evaluated  \n",
        "Algorithm parameter: a small threshold $\\theta > 0$ determining accuracy of estimation  \n",
        "Initialize $V(s)$, for all $s \\in S^+$, arbitrarily except that $V(terminal) = 0$\n",
        "\n",
        "Loop:  \n",
        "$\\Delta \\leftarrow 0$  \n",
        "Loop for each $s \\in S^+$:\n",
        "* $v \\leftarrow V(s)$\n",
        "* $V(s) \\leftarrow \\sum_a \\pi(a|s) \\sum_{s', r}p(s', r|s, a)[r + \\gamma V(s')]$\n",
        "* $\\Delta \\leftarrow \\text{max}(\\Delta, |v - V(s|)$\n",
        "\n",
        "Until $\\Delta < \\theta$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrJvWLK7ZWt0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cd287d92-4b5b-4aca-fc85-0e1249302211"
      },
      "source": [
        "env.P[0][1]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1.0, 0, 0.0, True)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXrFxMRswvjp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_policy(policy, gamma=1.0, theta=1e-10):\n",
        "  \n",
        "  value_table = np.zeros(env.nS)\n",
        "  \n",
        "  \n",
        "  while True:\n",
        "    # for each state in the environment, select the action according to the policy and compute the value table\n",
        "    delta = 0\n",
        "\n",
        "    for s in range(env.nS):\n",
        "\n",
        "      v = value_table[s]\n",
        "      new_v = 0\n",
        "\n",
        "      for a, action_prob in enumerate(policy[s]):\n",
        "        # For each action, look at the possible next states\n",
        "        p, ns, r, _ = env.P[s][a][0]\n",
        "        new_v += (action_prob * p * (r + gamma * value_table[ns]))\n",
        "\n",
        "      # Update value table\n",
        "      value_table[s] = new_v\n",
        "          \n",
        "      # How much of our value function has changed?\n",
        "      delta = max(delta, np.abs(v - value_table[s]))\n",
        "      \n",
        "#     print(delta)\n",
        "\n",
        "    if delta < theta:\n",
        "      print(\"Policy value evaluated.\")\n",
        "      break\n",
        "  \n",
        "  return value_table"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF1X3r2hZI04",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "006bd8d7-11ac-4bb4-9dd5-d6bdad3c044f"
      },
      "source": [
        "evaluate_policy(random_policy)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Policy value evaluated.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0., -14., -20., -22., -14., -18., -20., -20., -20., -20., -18.,\n",
              "       -14., -22., -20., -14.,   0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kk4EOOOaKD9",
        "colab_type": "text"
      },
      "source": [
        "#### Policy Improvement\n",
        "\n",
        "Suppose we have determined the value function $v_{\\pi}$ for an arbitrary deterministic policy $\\pi$.  \n",
        "If at a step we would like to know whether or not we should change the policy to deterministically choose an action $a \\neq \\pi(s)$, and to know whether it will better off or worse off to change to the new policy, say $\\pi'(s)$. How can we do this?\n",
        "\n",
        "One way to compare whether a policy is better or worse off is to consider selecting action $a$ in state $s$, but following the policy $\\pi$ thereafer. Hence, we can evaluate the action value of $a$\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "q_\\pi(s,a) & := \\mathop{\\mathbb{E}}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t=s, A_t=a] \\\\\n",
        "& = \\sum_{s', r}p(s',r|s,a)[r + \\gamma v_{\\pi}(s')]\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "If we found that $q_{\\pi}(s,a)$ is greater than $v_{\\pi}(s)$, then it means it is better off to select $a$ once in $s$ and thereafter follow $\\pi$ all the time, and that new policy would in fact be a better one overall. The algorithm runs so on and so forth.\n",
        "\n",
        "##### The policy improvement theorem\n",
        "In general, Let $\\pi$ and $\\pi'$ be any pair of deterministic policies, such that, for all $s \\in S$,\n",
        "\n",
        "\\begin{align}\n",
        "q_{\\pi}(s, \\pi'(s)) \\geq v_{\\pi}(s)\n",
        "\\end{align}\n",
        "\n",
        "Then, the policy $\\pi'$ must be as good as, or better than, $\\pi$, and thus it must obtain greater or equal expected return from all states $s \\in S$:\n",
        "\n",
        "\\begin{align}\n",
        "v_{\\pi'}(s) \\geq v_{\\pi}(s)\n",
        "\\end{align}\n",
        "\n",
        "In general, this can be extend to consider changes at all states and to all ossible actions.  \n",
        "Suppose we have a new greedy policy $\\pi'$ that select the action according to the max $q_{\\pi}(s,a)$. \n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "\\pi'(s) & := \\mathop{\\arg\\max}_{a} q_{\\pi}(s,a) \\\\\n",
        "& =  \\mathop{\\arg\\max}_{a} \\mathop{\\mathbb{E}}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t=s, A_t=a] \\\\\n",
        "& = \\mathop{\\arg\\max}_{a} \\sum_{s',r}p(s',r|s,a)[r + \\gamma v_{\\pi}(s')]\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "i.e. The greedy policy takes the action that looks best in the short term, according to the value $v_{\\pi}$ of one step of lookahead\n",
        "\n",
        "##### When to stop iteration?\n",
        "Iteration stops when we found a policy, say $\\pi'$, that is as good as, but not better than, the old policy $\\pi$. That means, we have $v_\\pi = v_{\\pi'}$, then we can say $v_{\\pi'}$ must be $v_*$, as it follows the Bellman optimality equation, and both $\\pi$ and $\\pi'$ must be optimal policies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLfRJZzOpbtY",
        "colab_type": "text"
      },
      "source": [
        "#### Algorithm\n",
        "\n",
        "1. Initialization  \n",
        "$V(s) \\in \\mathop{\\mathbb{R}}$ and $\\pi(s) \\in A(s)$ arbitrarily for all $s \\in S$\n",
        "\n",
        "2. Policy Evaluation  \n",
        "* Loop:  \n",
        "$\\Delta \\leftarrow 0$\n",
        "  * Loop for each $s \\in S$:\n",
        "     * $v \\leftarrow V(s)$\n",
        "     * $V(s) \\leftarrow \\sum_{s',r}p(s',r|s,\\pi(s))[r+\\gamma V(s')]$\n",
        "     * $\\Delta \\leftarrow \\mathop{\\max}(\\Delta, |v - V(s)|)$\n",
        "* until $\\Delta < \\theta$\n",
        "\n",
        "3. Policy Improvement  \n",
        "* policy-stable $\\leftarrow true$  \n",
        "* For each $s \\in S$:  \n",
        "  * old-action $\\leftarrow \\pi(s)$\n",
        "  * $\\pi(s) \\leftarrow \\mathop{\\arg\\max}_{a} \\sum_{s',r}p(s',r|s,a)[r + \\gamma v_{\\pi}(s')]$\n",
        "  * If old-action $\\neq \\pi(s)$, then policy-stable $\\leftarrow false$\n",
        "* If policy-stable, then stop and return $V \\approx v_*$ and $\\pi \\approx \\pi_*$; else go to 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgQxbKRKZp-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def policy_improvement(policy, gamma=1.0, theta=1e-10):\n",
        "  \n",
        "  while True:\n",
        "    \n",
        "    policy_stable = True\n",
        "    V = evaluate_policy(policy, gamma=gamma, theta=theta)\n",
        "\n",
        "    for s in range(env.nS):\n",
        "\n",
        "      q_list = []\n",
        "      \n",
        "      old_action = np.argmax(policy[s])\n",
        "\n",
        "      # Base on V find the optimal action\n",
        "      for a in range(env.nA):\n",
        "        p, ns, r, _ = env.P[s][a][0]\n",
        "        q_list.append(p * (r + gamma * V[ns]))\n",
        "\n",
        "      new_action = np.argmax(q_list)\n",
        "      \n",
        "      # Evaluate if the policy is stable\n",
        "      print(\"State: %s - old_action -> %s | new_action -> %s\" % (s, old_action, new_action))\n",
        "      if old_action != new_action:\n",
        "        policy_stable = False\n",
        "        \n",
        "      policy[s] = np.eye(env.nA)[new_action]\n",
        "      \n",
        "    if policy_stable:\n",
        "      return policy, V"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jdr0CFqDvj45",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1190
        },
        "outputId": "3a732878-14a1-4b4e-b235-267ffa76d898"
      },
      "source": [
        "# Iterate the whole process\n",
        "\n",
        "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
        "\n",
        "policy_improvement(random_policy)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Policy value evaluated.\n",
            "State: 0 - old_action -> 0 | new_action -> 0\n",
            "State: 1 - old_action -> 0 | new_action -> 3\n",
            "State: 2 - old_action -> 0 | new_action -> 3\n",
            "State: 3 - old_action -> 0 | new_action -> 3\n",
            "State: 4 - old_action -> 0 | new_action -> 0\n",
            "State: 5 - old_action -> 0 | new_action -> 0\n",
            "State: 6 - old_action -> 0 | new_action -> 3\n",
            "State: 7 - old_action -> 0 | new_action -> 2\n",
            "State: 8 - old_action -> 0 | new_action -> 0\n",
            "State: 9 - old_action -> 0 | new_action -> 0\n",
            "State: 10 - old_action -> 0 | new_action -> 2\n",
            "State: 11 - old_action -> 0 | new_action -> 2\n",
            "State: 12 - old_action -> 0 | new_action -> 0\n",
            "State: 13 - old_action -> 0 | new_action -> 1\n",
            "State: 14 - old_action -> 0 | new_action -> 1\n",
            "State: 15 - old_action -> 0 | new_action -> 0\n",
            "Policy value evaluated.\n",
            "State: 0 - old_action -> 0 | new_action -> 0\n",
            "State: 1 - old_action -> 3 | new_action -> 3\n",
            "State: 2 - old_action -> 3 | new_action -> 3\n",
            "State: 3 - old_action -> 3 | new_action -> 2\n",
            "State: 4 - old_action -> 0 | new_action -> 0\n",
            "State: 5 - old_action -> 0 | new_action -> 0\n",
            "State: 6 - old_action -> 3 | new_action -> 0\n",
            "State: 7 - old_action -> 2 | new_action -> 2\n",
            "State: 8 - old_action -> 0 | new_action -> 0\n",
            "State: 9 - old_action -> 0 | new_action -> 0\n",
            "State: 10 - old_action -> 2 | new_action -> 1\n",
            "State: 11 - old_action -> 2 | new_action -> 2\n",
            "State: 12 - old_action -> 0 | new_action -> 0\n",
            "State: 13 - old_action -> 1 | new_action -> 1\n",
            "State: 14 - old_action -> 1 | new_action -> 1\n",
            "State: 15 - old_action -> 0 | new_action -> 0\n",
            "Policy value evaluated.\n",
            "State: 0 - old_action -> 0 | new_action -> 0\n",
            "State: 1 - old_action -> 3 | new_action -> 3\n",
            "State: 2 - old_action -> 3 | new_action -> 3\n",
            "State: 3 - old_action -> 2 | new_action -> 2\n",
            "State: 4 - old_action -> 0 | new_action -> 0\n",
            "State: 5 - old_action -> 0 | new_action -> 0\n",
            "State: 6 - old_action -> 0 | new_action -> 0\n",
            "State: 7 - old_action -> 2 | new_action -> 2\n",
            "State: 8 - old_action -> 0 | new_action -> 0\n",
            "State: 9 - old_action -> 0 | new_action -> 0\n",
            "State: 10 - old_action -> 1 | new_action -> 1\n",
            "State: 11 - old_action -> 2 | new_action -> 2\n",
            "State: 12 - old_action -> 0 | new_action -> 0\n",
            "State: 13 - old_action -> 1 | new_action -> 1\n",
            "State: 14 - old_action -> 1 | new_action -> 1\n",
            "State: 15 - old_action -> 0 | new_action -> 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[1., 0., 0., 0.],\n",
              "        [0., 0., 0., 1.],\n",
              "        [0., 0., 0., 1.],\n",
              "        [0., 0., 1., 0.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [0., 0., 1., 0.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [0., 0., 1., 0.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [1., 0., 0., 0.]]),\n",
              " array([ 0., -1., -2., -3., -1., -2., -3., -2., -2., -3., -2., -1., -3.,\n",
              "        -2., -1.,  0.]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaBkVpYQt33s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "61e318cb-3db6-4caf-d4d7-8f8dfec9d252"
      },
      "source": [
        "policy_improvement(random_policy).reshape(4,4)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Policy value evaluated.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 3., 3., 3.],\n",
              "       [0., 0., 3., 2.],\n",
              "       [0., 0., 2., 2.],\n",
              "       [0., 1., 1., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    }
  ]
}