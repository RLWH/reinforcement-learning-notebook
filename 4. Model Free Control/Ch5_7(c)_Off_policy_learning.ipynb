{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch5-7(c) - Off-policy learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RLWH/reinforcement-learning-notebook/blob/master/4.%20Model%20Free%20Control/Ch5_7(c)_Off_policy_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_uUtGw1N-9C",
        "colab_type": "text"
      },
      "source": [
        "# On-policy learning vs Off-policy learning\n",
        "\n",
        "On-policy learning is about learning the most optimal behaviour through the policy that the agent is following. However, the agent needs to behave non-optimally in order to explore all actions to find the optimal actions. So, how can they learn about th eoptimal policy while behaving according to an exploratory policy?\n",
        "\n",
        "An alternative way to learn the most optimal policy is to use two policies\n",
        "- One policy is going to learn the most optimal behaviour - called the *target policy $\\pi$*\n",
        "- Another policy is more exploratory and is used to generate behaviour - called the *behaviour/exploratory policy $\\mu$*\n",
        "\n",
        "Since in this case we say that learning is from data \"off\" the target policy, thus the overall process is termed *off-policy* learning\n",
        "\n",
        "## Pros and cons of On-policy learning and Off-policy learning\n",
        "\n",
        "##### On-policy\n",
        "- Concept is simpler\n",
        "- Easier to converge\n",
        "\n",
        "##### Off-policy\n",
        "- Concept is harder\n",
        "- slower to converge\n",
        "- Greater variance\n",
        "- More exploration\n",
        "- Learn from others' experience\n",
        "- Key to learning multi-step predictive models of the world's dynamics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0twfVQ2bbxQ",
        "colab_type": "text"
      },
      "source": [
        "## The prediction problem of off-policy methods\n",
        "\n",
        "Suppose we wish to estimate $v_\\pi$ or $q_\\pi$, but all we have are episodes folowing another policy $\\mu$, where $\\mu \\neq \\pi$. Under this setting, we call that the policy $\\pi$ is the target policy, and this is the policy that we want to learn and optimise. The policy $\\mu$ is the behaviour policy, where we will sample the actions from. Both policies are considered fixed and given.\n",
        "\n",
        "##### Some requirements - The assumption of converge\n",
        "If we want to use episodes from $\\mu$ to estimate values for $\\pi$, we need to ensure that every action taken from $\\pi$, is at least occasionally taken under $\\mu$. Formally, we require $\\pi(a|s) > 0 \\implies \\mu(a|s) > 0$. \n",
        "\n",
        "Under this setting, the policy $\\mu$ must be stochastic in states where it is not identical to $\\mu$, while the policy $\\mu$ can be deterministic. For simplicity, we can assume policy $\\mu$ is a greedy policy. i.e. $\\text{argmax}_a \\pi(a|s) = 1$\n",
        "\n",
        "### The cornerstone of off-policy methods - Importance Sampling\n",
        "Importance sampling is a general technique for estimating expected values under one distribution given samples from another. It weights the returns of each timestep according to the relative probability of their trajectories occuring under the target and behaviour policies. \n",
        "\n",
        "Given a starting state $S_t$, the probability of the subsequent state-action trajectory **under policy $\\pi$**, {$A_t, S_{t+1}, A_{t+1}, ..., S_T | S_t, A_{t:T-1}\\sim \\pi$} is \n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "Pr\\{A_t, &S_{t+1}, A_{t+1}, ..., S_T | S_t, A_{t:T-1} \\sim \\pi\\} \\\\\n",
        "& = \\pi(A_t|S_t)p(S_{t+1}|S_t,A_t) * \\pi(A_{t+1}|S_{t+1})...p(S_T|S_{T-1}, A_{T-1}) \\\\\n",
        "& = \\prod_{k=t}^{T-1}\\pi(A_k|S_k)p(S_{k+1}|S_k,A_k)\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "where $p$ is the state-transition probability function.\n",
        "\n",
        "Thus, the relative probability of the trajectory under the target and behaviour policies, or the importance-sampling ratio, is\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "\\rho_{t:T-1} &= \\frac{\\prod_{k=t}^{T-1}\\pi(A_k|S_k)p(S_{k+1}|S_k,A_k)}{\\prod_{k=t}^{T-1}\\mu(A_k|S_k)p(S_{k+1}|S_k,A_k)} \\\\\n",
        "&=\\prod_{k=t}^{T-1}\\frac{\\pi(A_k|S_k)}{\\mu(A_k|S_k)}\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "which is regardless of the transition probabilities.\n",
        "\n",
        "Under off-policy learning, we wish to estimate the expected returns under the target policy. However, we sample the actions base on the behaviour policy, thus, we cannot calculate the expectation straight from the sampled values. The ratio $\\rho_{t:T-1}$ thus transforms the returns to have the right expected value:\n",
        "\\begin{equation}\n",
        "\\mathop{\\mathbb{E}}[\\rho_{t:T-1}G_t | S_t = s] = v_{\\pi}(s)\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "### Applying to Monte Carlo value estimation\n",
        "Recall from Monte Carlo value estimation:\n",
        "\n",
        "If we wish to estimate a value of state $s$ under policy $\\pi$, i.e. $v_{\\pi}(s)$, we can generate a set of episode that pass through $s$ and average the returns of the visits to $s$. \n",
        "\n",
        "There are two ways of averaging. We can either:\n",
        "1. Average only the first visits to state $s$, which is called the first-visit MC method, or\n",
        "2. Average all visits to state $s$, which is called every-visit MC method.\n",
        "\n",
        "The same methodology also applies to off-policy method with a slight tweak.\n",
        "If we are using the first-visit method, under off-policy estimation, we first denote $J(s)$ as the as the set of all time steps in which state $s$ is visited, or first-visited, and T(t) denote the first time of termination following time t, G_t denite the return after t up to T. Then, to estimate $v_{\\pi}(s)$, we simply scale the returns by ratios and averages the result:\n",
        "\\begin{equation}\n",
        "V(s) = \\frac{\\sum_{t \\in J(s)} \\rho_{t:T(t)-1}G_t}{|J(s)|}\n",
        "\\end{equation}\n",
        "\n",
        "### Incremental update\n",
        "Suppose we have a sequence of returns $G_1, G_2, ..., G_{n-1}$, all starting in the same state, and each with a corresponding random weight $W_i = \\rho_{t_i:T(t_i) - 1}$, then we can form the estimate\n",
        "\\begin{equation}\n",
        "V_n = \\frac{\\sum_{k=1}^{n-1}W_kG_k}{\\sum_{k=1}^{n-1}W_k}, \\hspace{1cm} n \\geq 2\n",
        "\\end{equation}\n",
        "\n",
        "and the update rule becomes\n",
        "\\begin{equation}\n",
        "V_{n+1} = V_{n} + \\frac{W_n}{C_n}[G_n - V_n], \\hspace{1cm} n \\geq 1\n",
        "\\end{equation}\n",
        "and\n",
        "\n",
        "\\begin{equation}\n",
        "C_{n+1} = C_n + W_{n+1}\n",
        "\\end{equation}\n",
        "where $C_0 = 0$ and $V_1$ is arbitrary.\n",
        "\n",
        "The update rule is same for Q, just need to replace $V(s)$ by $Q(s,a)$\n",
        "\n",
        "##### Pseudo code of off-policy MC prediction for Q\n",
        "---\n",
        "```\n",
        "Input: an arbitrary target policy pi\n",
        "Initialise, for all s in S, a in A(s):\n",
        "    Q(s,a) arbitarily\n",
        "    C(s,a) = 0\n",
        "    \n",
        "Loop forever for each episode:\n",
        "    mu = any policy with converge of pi\n",
        "    Generate an episode following mu: S0, A0, R1, ..., ST-1, AT-1, RT\n",
        "    \n",
        "    G = 0\n",
        "    W = 1\n",
        "    \n",
        "    Loop for each step of episode, t = T-1, T-2, ..., 0, while W =/= 0:\n",
        "        G = gamma * G + Rt+1\n",
        "        C(s,a) = C(s,a) + W\n",
        "        Q(s,a) = Q(s,a) + W/C(s,a) * abs(G - Q(s,a))\n",
        "        W = W * (pi(a|s) / mu(a|s))\n",
        "```\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59pJHk1ZZ4Kt",
        "colab_type": "text"
      },
      "source": [
        "## Off-policy TD Control\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKpbnKZWp6cD",
        "colab_type": "text"
      },
      "source": [
        "## Q-learning\n",
        "Q-learning is similar to SARSA(0) but for off-policy learning\n",
        "Considering off-policy learning of action values Q(s,a). It does not require importance sampling. \n",
        "Here's the idea:\n",
        "1. The next action is chosen by using behaviour policy $A_{t+1} \\sim \\mu(.|S_t)$\n",
        "2. But we consider alternative successor action $A' \\sim \\pi(.|S_t)$\n",
        "3. Then, update $Q(S_t, A_t)$ towards value of alternative action\n",
        "\n",
        "\\begin{equation}\n",
        "Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}, A') - Q(S_t, A_t)]\n",
        "\\end{equation}\n",
        "\n",
        "If we now allow both behaviour and target policies to improve.\n",
        "Say the target policy $\\pi$ is greedy w.r.t $Q(s,a)$\n",
        "\n",
        "\\begin{equation}\n",
        "\\pi(S_{t+1}) = \\arg\\max_{a'} Q(S_{t+1}, a')\n",
        "\\end{equation}\n",
        "\n",
        "and the behaviour policy $\\mu$ is $\\epsilon$-greedy w.r.t $Q(s,a)$\n",
        "\n",
        "Then, the Q-learning target then simpilifes:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "& R_{t+1} + \\gamma Q(S_{t+1}, A') \\\\\n",
        "& = R_{t+1} + \\gamma Q(S_{t+1}, \\arg\\max_{a'} Q(S_{t+1}, a')) \\\\\n",
        "&= R_{t+1} + \\max_{a'} \\gamma Q(S_{t+1}, a')\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "#### SARSAMAX update\n",
        "\\begin{equation}\n",
        "Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma \\max_{a'}  Q(S_{t+1}, a') - Q(S_t, A_t)]\n",
        "\\end{equation}\n",
        "\n",
        "##### Pseudo code\n",
        "---\n",
        "```\n",
        "Algorithm parameters: step size alpha, smal epsilon > 0\n",
        "Initialise Q(s,a) for all s in S+, a in A(s)\n",
        "Arbitrarily except that Q(terminal,.) = 0\n",
        "\n",
        "Loop for each episode:\n",
        "    Initialise S\n",
        "    Loop for each step of episode:\n",
        "        Choose A from S using policy derived from Q (e.g. epsilon-greedy)\n",
        "        Take action A, observe R, S'\n",
        "        Q(S,A) = Q(S,A) + alpha * (R + gamma * max_aQ(S',a) - Q(S,A))\n",
        "        S = S'\n",
        "    until S is terminal\n",
        "    \n",
        "```\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk_AVg-kaAGl",
        "colab_type": "text"
      },
      "source": [
        "## Expected SARSA\n",
        "\n",
        "Expected SARSA is an alternative algorithm that is just like Q-learning, except that instead of using a greedy policy over next state-action pairs, it uses the expected value of how likely each action is under the current policy.\n",
        "\n",
        "#### Formulation\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "Q(S_t, A_t) & \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1}+ \\gamma \\mathop{\\mathbb{E_\\pi}}[Q(S_{t+1}, A_{t+1} | S_{t+1})] - Q(S_t, A_t)] \\\\\n",
        "& \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1}+ \\gamma \\sum_a \\pi(a|S_{t+1})Q(S_{t+1}, a) - Q(S_t, A_t)]\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "Given the next state $S_{t+1}$, this algorithm moves deterministically in the same diection as Sarsa moves in expectation, and thus it is called expected SARSA\n",
        "\n",
        "Expected SARSA is more complex computationally than SARSA, but, in return, it eliminates the variance due to the random selection of $A_{t+1}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZGDYy4OZ1Qi",
        "colab_type": "text"
      },
      "source": [
        "# Summary\n",
        "\n",
        "![Relationship Between DP and TD_1](https://raw.githubusercontent.com/RLWH/reinforcement-learning-notebook/master/images/Relationship_DP_TD_1.png)\n",
        "\n",
        "![Relationship Between DP and TD_2](https://raw.githubusercontent.com/RLWH/reinforcement-learning-notebook/master/images/Relationship_DP_TD_2.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNav6--tLUWU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}