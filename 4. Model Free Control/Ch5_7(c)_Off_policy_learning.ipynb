{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch5-7(c) - Off-policy learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RLWH/reinforcement-learning-notebook/blob/master/4.%20Model%20Free%20Control/Ch5_7(c)_Off_policy_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_uUtGw1N-9C",
        "colab_type": "text"
      },
      "source": [
        "# On-policy learning vs Off-policy learning\n",
        "\n",
        "On-policy learning is about learning the most optimal behaviour through the policy that the agent is following. However, the agent needs to behave non-optimally in order to explore all actions to find the optimal actions. So, how can they learn about th eoptimal policy while behaving according to an exploratory policy?\n",
        "\n",
        "An alternative way to learn the most optimal policy is to use two policies\n",
        "- One policy is going to learn the most optimal behaviour - called the *target policy $\\pi$*\n",
        "- Another policy is more exploratory and is used to generate behaviour - called the *behaviour/exploratory policy $\\mu$*\n",
        "\n",
        "Since in this case we say that learning is from data \"off\" the target policy, thus the overall process is termed *off-policy* learning\n",
        "\n",
        "## Pros and cons of On-policy learning and Off-policy learning\n",
        "\n",
        "##### On-policy\n",
        "- Concept is simpler\n",
        "- Easier to converge\n",
        "\n",
        "##### Off-policy\n",
        "- Concept is harder\n",
        "- slower to converge\n",
        "- Greater variance\n",
        "- More exploration\n",
        "- Learn from others' experience\n",
        "- Key to learning multi-step predictive models of the world's dynamics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0twfVQ2bbxQ",
        "colab_type": "text"
      },
      "source": [
        "## The prediction problem of off-policy methods\n",
        "\n",
        "Suppose we wish to estimate $v_\\pi$ or $q_\\pi$, but all we have are episodes folowing another policy $\\mu$, where $\\mu \\neq \\pi$. Under this setting, we call that the policy $\\pi$ is the target policy, and this is the policy that we want to learn and optimise. The policy $\\mu$ is the behaviour policy, where we will sample the actions from. Both policies are considered fixed and given.\n",
        "\n",
        "##### Some requirements - The assumption of converge\n",
        "If we want to use episodes from $\\mu$ to estimate values for $\\pi$, we need to ensure that every action taken from $\\pi$, is at least occasionally taken under $\\mu$. Formally, we require $\\pi(a|s) > 0 \\implies \\mu(a|s) > 0$. \n",
        "\n",
        "Under this setting, the policy $\\mu$ must be stochastic in states where it is not identical to $\\mu$, while the policy $\\mu$ can be deterministic. For simplicity, we can assume policy $\\mu$ is a greedy policy. i.e. $\\text{argmax}_a \\pi(a|s) = 1$\n",
        "\n",
        "### The cornerstone of off-policy methods - Importance Sampling\n",
        "Importance sampling is a general technique for estimating expected values under one distribution given samples from another. It weights the returns of each timestep according to the relative probability of their trajectories occuring under the target and behaviour policies. \n",
        "\n",
        "Given a starting state $S_t$, "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNav6--tLUWU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}