{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch5(b) - Monte Carlo Control.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RLWH/reinforcement-learning-notebook/blob/master/4.%20Model%20Free%20Control/Ch5(b)_Monte_Carlo_Control.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odhAofwGxM2i",
        "colab_type": "text"
      },
      "source": [
        "# Model-Free Control\n",
        "The best introduction of this chapter is probably given by David Silver in his video\n",
        "> For everything in the course up to this point is leading to this lecture. We gotta finally find out how can you drop the robot or agent into some unknown environment, and you don't tell it anything about how the environment works, how can it figure out the right thing to do.\n",
        "\n",
        "In previous chapters, we have discovered how to estimate the value of each state by different methods - including Monte Carlo, one-step TD, TD($\\lambda$), etc. In this chapter, we will bring it forward to control and find optimal policies.\n",
        "\n",
        "We will talk about \n",
        "1. On-policy Monte-Carlo Control\n",
        "2. On-Policy TD Learning\n",
        "3. Off-policy Learning\n",
        "\n",
        "#### On and Off-policy learning\n",
        "- On-policy learning\n",
        "  - \"Learn on the job\"\n",
        "  - Learn about policy $\\pi$ from experience sampled from $\\pi$\n",
        "\n",
        "- Off-policy learning\n",
        "  - \"Look over someone's shoulder\"\n",
        "  - Learn about policy $\\pi$ from experience sampled from $\\mu$\n",
        "  - Learn from others"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YOBwHymy0WZ",
        "colab_type": "text"
      },
      "source": [
        "# 1. On-policy MC Control\n",
        "\n",
        "## Generalised Policy Iteration\n",
        "The main idea that we will use here is the policy iteration method. This is the same method that has been used in dynamic progamming\n",
        "\n",
        "![Generalised Policy Iteration](https://raw.githubusercontent.com/RLWH/reinforcement-learning-notebook/master/images/generalised_policy_iteration.png)\n",
        "\n",
        "Steps:\n",
        "1. Policy evaluation\n",
        "  - Estimate $v_{\\pi}$\n",
        "  - Use a random trajectory to estimate the value function\n",
        "2. Policy Improvement\n",
        "  - Generate $\\pi' \\geq \\pi$ by greedy selection\n",
        "  \n",
        "There's an issue: \n",
        "For step 2, when choosing the greedy action, we need to know what is the transition probabilities from the environment. In this case, we still require a model of MDP if we use value function for the iteration process. \n",
        "\n",
        "Greedy policy\n",
        "\\begin{equation}\n",
        "\\pi'(s) = \\text{argmax}_{a \\in A} R^{a}_s + P^{a}_{ss'}V(s')\n",
        "\\end{equation}\n",
        "\n",
        "What can we do?\n",
        "- We iterate on the Q(s,a) function instead\n",
        "- i.e. Making an evaluation at each state of how good to take each of the action \n",
        "\n",
        "\\begin{equation}\n",
        " \\pi'(s) = \\text{argmax}_{a \\in A} Q(s,a)\n",
        "\\end{equation}\n",
        "\n",
        "## Improvement: Generalised Policy Iteration with Action-Value function\n",
        "\n",
        "1. We start off by having a Q value function with some policy\n",
        "2. Take the mean of all the state action pair\n",
        "3. Greedily choose wrt on Q\n",
        "4. Iterate\n",
        "\n",
        "![Policy Iteration on Q](https://raw.githubusercontent.com/RLWH/reinforcement-learning-notebook/master/images/policy_iteration_on_q.png)\n",
        "\n",
        "Still have issues:\n",
        "- If we act greedily, we can still get stuck, because there are lack of explorations.\n",
        "\n",
        "### Exploration by $\\epsilon$-greedy exploration\n",
        "This is the simplest idea for ensuring continual exploration. \n",
        "\n",
        "- Suppose there are m actions\n",
        "- Define a small probability $\\epsilon$\n",
        "  - With probability $1-\\epsilon$ choose greedy action\n",
        "  - With probability $\\epsilon$ choose an action at random\n",
        "  \n",
        "#### Formulation\n",
        "Note that the $\\epsilon$-greedy policy is a determinstic policy\n",
        "\\begin{equation}\n",
        "\\pi(a|s) = \n",
        "\\begin{cases}\n",
        "    \\frac{\\epsilon}{m} + 1 - \\epsilon, & \\text{if} a* = \\text{argmax}_{a \\in A} Q(s,a) \\\\\n",
        "    \\frac{\\epsilon}{m}, & \\text{otherwise}\n",
        "\\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "#### Theorem\n",
        "For any $\\epsilon$-greedy policy $\\pi$, the $\\epsilon$-greedy policy $\\pi'$ with respect to $q_{\\pi}$ is an improvement, $v_{\\pi'}(s) \\geq v_{\\pi}(s)$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07yVEOhsYfDg",
        "colab_type": "text"
      },
      "source": [
        "## GLIE policy (Greedy in the Limit with Infinite Exploration)\n",
        "How can we really find the most optimal policy? To do that, we need to balance two different things - Continue exploring for infinity time but asymtotically it will converge to a policy\n",
        "\n",
        "#### Properties\n",
        "1. All state-action pairs that are explored many times, to make sure as many as possible, if not all, the state space can be tried\n",
        "\\begin{equation}\n",
        "lim_{k \\to \\infty} N_k(s,a) = \\infty\n",
        "\\end{equation}\n",
        "\n",
        "2. The policy eventaully becomes a greedy policy, i.e. a deterministic policy that maximise the q value\n",
        "\n",
        "\\begin{equation}\n",
        "lim_{k \\to \\infty} \\pi_{k}(a|s) = \\mathop{\\mathbb{1}}(a = \\text{argmax}_{a' \\in A}Q_k(s, a'))\n",
        "\\end{equation}\n",
        "\n",
        "For instance, $epsilon$-greedy is GLIE if $\\epsilon$ reduces ot zero at $\\epsilon_k = \\frac{1}{k}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REtTAiYVca8F",
        "colab_type": "text"
      },
      "source": [
        "## GLIE Monte-Carlo Control\n",
        "\n",
        "1. First, sample kth episode using $\\pi: {S_1, A_1, R_2, S_2, A_2, ..., S_T} \\sim \\pi$\n",
        "2. For each state $S_t$ and action $A_t$ in the episode,\n",
        "\n",
        " \\begin{equation}\n",
        " \\begin{split}\n",
        " N(S_t, A_t) & \\leftarrow N(S_t, A_t) + 1 \\\\\n",
        " Q(S_t, A_t) & \\leftarrow Q(S_t, A_t) + \\frac{1}{N(S_t, A_t)}(G_t - Q(S_t, A_t))\n",
        " \\end{split}\n",
        " \\end{equation}\n",
        " \n",
        "3. Improve policy based on new action-value function\n",
        "\n",
        " \\begin{equation}\n",
        " \\begin{split}\n",
        " \\epsilon &\\leftarrow \\frac{1}{k} \\\\\n",
        " \\pi &\\leftarrow \\epsilon\\text{-greedy}(Q)\n",
        " \\end{split}\n",
        " \\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loDw0-QFibao",
        "colab_type": "text"
      },
      "source": [
        "## Monte Carlo Exploring Starts Algorithm\n",
        "For Monte Carlo policy iteration, it alternates between evaluation and improvement on an episode-by-episode basis. After each episode, the observed returns are used for policy evaluation, and the the policy is improved at all the states visited in the episode. \n",
        "\n",
        "#### Pseudo Code\n",
        "---\n",
        "```\n",
        "Initialise:\n",
        "  pi(s) in A(s) for all s in S\n",
        "  Q(s,a) for all s in S, a in A(s)\n",
        "  Returns(s,a) - Empty list, for all s in S and a in A(s)\n",
        "\n",
        "Loop forever (for each episode)\n",
        "   Choose S0 in S, A0 in A(S0) randomly such that all pairs have porbability > 0\n",
        "   Generate an episode from S0, A0, following pi: S0, A0, R1, S1, A1, ..., RT\n",
        "   G = 0\n",
        "   Loop for each step of episode, t=T-1, T-2, ..., 0\n",
        "   G = gamma * G + R_t+1\n",
        "   Unless the pair St, At appears in S0, A0, S1, A1, ..., St-1, At-1:\n",
        "    Append G to Returns(St, At)\n",
        "    Q(St, At) = Average(Returns(St, At))\n",
        "    pi(St) = argmax_a(Q(St, a))\n",
        "    \n",
        "\n",
        "```\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSj5aZKGyzne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvQi9V5CZfYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}