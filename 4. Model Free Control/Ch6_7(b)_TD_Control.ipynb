{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch6-7(b) TD Control.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RLWH/reinforcement-learning-notebook/blob/master/4.%20Model%20Free%20Control/Ch6_7(b)_TD_Control.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkvVPU_wtTNu",
        "colab_type": "text"
      },
      "source": [
        "# From MC Control to TD Control\n",
        "\n",
        "TD learning has several advantages over MC methods\n",
        "1. Lower variance\n",
        "2. Online training (No need to wait till the whole episode is run)\n",
        "3. It can deal with incomplete dequences\n",
        "\n",
        "As with MC methods, we need to face the tradeoff between exploration and exploitation, and again approaches fall into two main classes:\n",
        "- On-policy\n",
        "- Off-policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1RaGemzupw6",
        "colab_type": "text"
      },
      "source": [
        "# Use TD Learning to estimate Q(S,A)\n",
        "\n",
        "We do the similar things as MC. \n",
        "1. Apply TD to Q(S,A)\n",
        "2. Use $\\epsilon$-greedy policy improvmeent\n",
        "3. Update every time-step\n",
        "\n",
        "The most commonly used on-policy algorithm is the SARSA algorithm\n",
        "\n",
        "The first step of an on-policy TD control algorithm is to learn an action-value function rather than a state-value function. For the same reason, using the action-value function can get rid of know the complete MDP dynamics. \n",
        "\n",
        "In particular, for an on-policy method, we must estimate $q_{\\pi}(s,a)$ for the current behaviour policy $\\pi$ and for all states $s$ and actions $a$. If we review the sequence of events of an episode, it consissts of an alternating sequence of states and state-action pairs:\n",
        "\n",
        "![Sequences of events](https://raw.githubusercontent.com/RLWH/reinforcement-learning-notebook/master/images/sequence_of_events_in_episode.PNG)\n",
        "\n",
        "### The TD(0) algorithm with Q-values, or, SARSA\n",
        "For a transition form a state-action pair to another state-action pair, we can have an update on the Q-value\n",
        "\\begin{equation}\n",
        "Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]\n",
        "\\end{equation}\n",
        "\n",
        "As the TD algorithm is updated after every transition, which consists of the quintuple of events $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$, this is also known as the SARSA algorithm.\n",
        "\n",
        "### Plugging in SARSA into the generalised policy iteration (GPI) framework\n",
        "1. Policy evaluation Sarsa, $Q \\approx q_{\\pi}$\n",
        "2. Policy improvement $\\epsilon$-greedy policy improvement\n",
        "\n",
        "### Pseudo code of SARSA\n",
        "---\n",
        "```\n",
        "Initialise Q(s,a) for all s in S, a in A(s), arbitrarily, and Q(T,.)=0\n",
        "Loop for each episode:\n",
        "    Initialise S\n",
        "    Choose A from S using policy derived from Q (e.g. esp-greedy)\n",
        "    Loop for each step of episode:\n",
        "        Take action A, observe R, S'\n",
        "        Choose A' from S' using policy derived from Q (e.g. esp-greedy)\n",
        "        Q(St,At) = Q(St,At) + alpha * (R + gamma * Q(St+1, At+1) - Q(St, At))\n",
        "        St = St+1; At = At+1\n",
        "        Until S is terminal\n",
        "```\n",
        "---\n",
        "\n",
        "### How SARSA converge?\n",
        "\n",
        "#### Theorem\n",
        "> SARSA converges to the optimal action-value function, $Q(s,a) \\to q_*(s,a)$, under the folowing conditions:\n",
        " 1. GLIE sequence of policies $\\pi_t(a|s)$\n",
        " 2. Robbins-Monro sequence of step-sizes $\\alpha_t$\n",
        "    - $\\sum_{t=1}^{\\infty} \\alpha_t = \\infty$\n",
        "   - $\\sum_{t=1}^{\\infty} \\alpha_t^{2} < \\infty$\n",
        "   \n",
        "\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jw22Vw2M9fj0",
        "colab_type": "text"
      },
      "source": [
        "## From SARSA to n-step SARSA and SARSA($\\lambda$)\n",
        "\n",
        "As we have learnt that there is a wide spectrum between Monte Carlo and one-step TD update, which we call it n-step TD methods, we can apply the same logic on SARSA update. This is called the n-step SARSA\n",
        "\n",
        "![n-step SARSA](https://raw.githubusercontent.com/RLWH/reinforcement-learning-notebook/master/images/n_step_sarsa.PNG)\n",
        "\n",
        "#### The n-step Q-return\n",
        "\n",
        "\\begin{equation}\n",
        "q_t^{(n)} = R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{n-1} R_{t+n} + \\gamma^{n} Q(S_{t+n}), \\hspace{1cm} n \\geq 1, 0 \\leq t \\lt T-n\n",
        "\\end{equation}\n",
        "\n",
        "#### n-step SARSA update\n",
        "\n",
        "\\begin{equation}\n",
        "Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha (q_t^{(n)} - Q(S_t, A_t)), \\hspace{1cm} 0 \\leq t \\lt T\n",
        "\\end{equation}\n",
        "while the values of all other states remain unchanged\n",
        "\n",
        "![n-step SARSA backup diagram](https://raw.githubusercontent.com/RLWH/reinforcement-learning-notebook/master/images/n_step_sarsa_backup.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gige3AuxDyxy",
        "colab_type": "text"
      },
      "source": [
        "## Forward-view SARSA($\\lambda$)\n",
        "![Forward SARSA](https://github.com/RLWH/reinforcement-learning-notebook/blob/master/images/forward_sarsa_lambda.PNG?raw=true)\n",
        "\n",
        "## Backward-view SARSA($\\lambda$)\n",
        "Just like TD($\\lambda$), we also have a backward view algorithm that uses*Eligibility traces* in the online algorithm\n",
        "\n",
        "- SARSA($\\lambda$) has one eligibility trace for each state-action pair\n",
        " \n",
        " \\begin{equation}\n",
        " \\begin{split}\n",
        " E_0(s,a) &= 0 \\\\\n",
        " E_t(s,a) &= \\gamma \\lambda E_{t-1}(s,a) + \\mathop{\\mathbb{1}}(S_t=s, A_t=a)\n",
        " \\end{split}\n",
        " \\end{equation}\n",
        " \n",
        "- Hence, Q(s,a) is updated for every state $s$ and action $a$ in proportion to TD-error $\\delta_t$ and eligibility trace *E_t(s,a)*\n",
        "\n",
        " \\begin{equation}\n",
        " \\begin{split}\n",
        " &\\delta_t = R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\\\\n",
        " &Q(s,a) \\leftarrow Q(s,a) + \\alpha \\delta_t E_t(s,a)\n",
        " \\end{split}\n",
        " \\end{equation}\n",
        "\n",
        "## Pseudo Code of SARSA($\\lambda$) algorithm\n",
        "---\n",
        " ```\n",
        " Initialise Q(s,a) arbitrarily, for all s in S, a in A(s)\n",
        " Loop for each episode:\n",
        "    E(s,a) = 0 for all s in S, a in A(s)\n",
        "    Initialise S, A\n",
        "    \n",
        "    Loop for each step of the episode:\n",
        "        Take action At, observe Rt+1, St+1\n",
        "        Choose At+1 from St+1 using policy derived from Q (e.g. epsilon-greedy)\n",
        "        delta = R + gamma * Q(St+1, At+1) - Q(S,A)\n",
        "        E(S,A) = E(S,A) + 1      # Update The Eligibility Trace\n",
        "        \n",
        "        For all s in S, a in A(s):\n",
        "            Q(s,a) = Q(s,a) + alpha * delta * E(s,a)\n",
        "            E(s,a) = gamma * lambda * E(s,a)\n",
        "            \n",
        "    Until S is terminal\n",
        " ```\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES_obUUk8js3",
        "colab_type": "text"
      },
      "source": [
        "## Windy Gridworld Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoEPNRUutNC_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}