{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Integrating Learning and Planning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RLWH/reinforcement-learning-notebook/blob/master/7.%20Integrating%20Learning%20and%20Planning/Integrating_Learning_and_Planning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1ve8pabeRVb",
        "colab_type": "text"
      },
      "source": [
        "# Integrating Learning and Planning\n",
        "\n",
        "In previous lectures, we have learnt how to learn a policy irectly from experience, as well as learn a value function directly from experience. Those methods are called model-free algorithms as they rely on real world experience to learn a function or policy. \n",
        "\n",
        "In contrast, model-based RL algorithm deals with a different problem - it learns a **model** directly from the experience, and use **planning** to construct a value function or policy. The learning and planning activities can be integrated into a single architecture.\n",
        "\n",
        "Advantages of Model-based RL\n",
        "- Can efficiently learn model by supervisedlearning methods\n",
        "- Can reason about model uncertainty\n",
        "\n",
        "Disadvantages\n",
        "- First learn a model, then construct a value function\n",
        "    - It may cause two sources of approximation error\n",
        "\n",
        "In particular, we will cover:\n",
        "- Dyna-Q Algorithm\n",
        "- Forward search\n",
        "- Monte Carlo Tree Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mpv5bVeCebkV",
        "colab_type": "text"
      },
      "source": [
        "## What is a model?\n",
        "\n",
        "A model represents the environment that an agent can use it to predict how the environment will respond to its actions.\n",
        "Formally, a model $M$ is a representation of an MDP $\\langle S, A, P, R \\rangle$ that is parameterized by $\\eta$\n",
        "\n",
        "Assume that the state space $S$ and action space $A$ are known, a model $M=\\langle P_{\\eta}, R_{\\eta} \\rangle$ represents the state transitions $P_{\\eta} \\approx P$ and rewards $R_{\\eta} \\approx R$ \n",
        "\n",
        "With a given $S_t, A_t$, a model produces\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "S_{t+1} &\\sim P_{\\eta}(S_{t+1} | S_t, A_t) \\\\\n",
        "R_{t+1} &\\sim R_{\\eta}(R_{t+1} | S_t, A_t)\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "It is typically assumed that between state transitions and rewards are conditional independence\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathop{\\mathbb{P}}[S_{t+1}, R_{t+1} | S_t, A_t] = \\mathop{\\mathbb{P}}[S_{t+1} | S_t, A_t]\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUqwp5yDoO5q",
        "colab_type": "text"
      },
      "source": [
        "## Planning with a Model\n",
        "For a given model $M_{\\eta} = \\langle P_{\\eta}, R_{\\eta} \\rangle$\n",
        "Solve the MDP $\\langle S, A, P_{\\eta}, R_{\\eta} \\rangle$ by using\n",
        "- Value iteration\n",
        "- Policy iteration\n",
        "- Tree seaerch ** ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kkabbn2onH1",
        "colab_type": "text"
      },
      "source": [
        "## A more concrete example - Sample-Based Planning\n",
        "\n",
        "Sample-based planning uses the model only to generate samples\n",
        "\n",
        "1. Sample experience from model\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "S_{t+1} &\\sim P_{\\eta}(S_{t+1} | S_t, A_t) \\\\\n",
        "R_{t+1} &= R_{\\eta}(R_{t+1} | S_t, A_t)\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "Then, we apply model-free RL to samples,\n",
        "- Monte-Carlo Control\n",
        "- SARSA\n",
        "- Q-learning\n",
        "![Model Based RL](https://github.com/RLWH/reinforcement-learning-notebook/blob/master/images/model_based_rl.PNG?raw=true)\n",
        " \n",
        "#### AB MDP Example\n",
        "  ![AB MDP](https://github.com/RLWH/reinforcement-learning-notebook/blob/master/images/model_based_rl_example_ab.PNG?raw=true)\n",
        "  \n",
        "#### Drawbacks - Planning with an Inaccurate Model\n",
        "If we have an imperfect model $\\langle P_{\\eta}, R_{\\eta} \\rangle \\neq \\langle P, R \\rangle$, the performance of model-based RL is limited to optimal policy for approximate MDP, hence, we will have a suboptimal policy\n",
        "\n",
        "What can we do?\n",
        "1. When the model is wrong, use model-free RL\n",
        "2. Reason explicitly about model uncertainty\n",
        "\n",
        "#### Algorithm - One step Q-planning\n",
        "---\n",
        "```\n",
        "Loop forever:\n",
        "        1. Select a state and an action at random\n",
        "        2. Send S, A to a sample model, and obtain a sample next reward R, and a sample next state S'\n",
        "        3. Apply one-step tabular Q-learning to S, A, R, S'\n",
        "              Q(S, A) += alpha * (R + gamma * max_a Q(S', A) - Q(S, A))\n",
        "```\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lSOhgzHuqy6",
        "colab_type": "text"
      },
      "source": [
        "## Integrating Learning and Planning\n",
        "We can use two sources of experience\n",
        "\n",
        "1. **Real experience** - Sampled from the environment (true MDP)\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "S' &\\sim P^{a}_{s, s'}\\\\\n",
        "R &= R^a_{s}\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "2. **Simulated experience** - Sampled from model (approximate MDP)\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "S' &\\sim P_{\\eta}(S' | S, A)\\\\\n",
        "R &= R_{\\eta}(R | S, A)\n",
        "\\end{split}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkMMkx8dvleC",
        "colab_type": "text"
      },
      "source": [
        "## DYNA (Integrated Planning, Acting and Learning)\n",
        "- Learn a model from experience\n",
        "- Learn and plan value function (and /or policy) from real and simulated experience\n",
        "\n",
        "#### DYNA cycle\n",
        "![DYNA cycle](https://raw.githubusercontent.com/RLWH/reinforcement-learning-notebook/master/images/dyna_cycle.PNG)\n",
        "\n",
        "#### Algorithm (DYNA-Q)\n",
        "---\n",
        "```\n",
        "Initialise Q(S,A) and Model(S,A) for all S and A\n",
        "Do forever:\n",
        "        1. S = current state\n",
        "        2. A = epsilon-greedy(S, Q)\n",
        "        3. Execute A, observe R, S'\n",
        "        4. Q(S,A) += alpha * [R + gamma * max_a Q(S',A) - Q(S,A)]   <--- [Using One step TD]\n",
        "        5. Update model\n",
        "              Model(S,A) <- R, S'\n",
        "        6. Repeat n times (imagination):\n",
        "               S <- random previousy observed state\n",
        "               A <- random action previously taken in S\n",
        "               R, S' <- Model(S,A)\n",
        "               Q(S,A) += alpha * [R + gamma * max_a Q(S',A) - Q(S,A)]\n",
        "```\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CHMfmOJWlsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}