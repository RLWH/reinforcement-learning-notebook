{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch3 - Gridworld MDP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "8-zvxtZSkwEW"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RLWH/reinforcement-learning-notebook/blob/master/Ch3%20MPD/Ch3_Gridworld_MDP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6_y_XpfB_fN",
        "colab_type": "text"
      },
      "source": [
        "# The Gridworld problem and solving gridworld by Dynamic Programming\n",
        "1. Policy Evaluation\n",
        "2. Policy Iteration\n",
        "3. Value Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-zvxtZSkwEW",
        "colab_type": "text"
      },
      "source": [
        "## Setup OpenAI Rendering in Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZsyYV41f3iv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6u7fFBJqBn03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOh5oCdjgYEJ",
        "colab_type": "code",
        "outputId": "d70c0439-29b7-4dbf-d95e-863341306537",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '400x300x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '400x300x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoR0qk1fkE6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xm5goJOPkHlA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlCrQxFfjcrV",
        "colab_type": "code",
        "outputId": "b2aa4616-114a-430c-d6d0-51176b092919",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "env = wrap_env(gym.make('CartPole-v0'))\n",
        "# env.reset()\n",
        "for i_episode in range(20):\n",
        "    observation = env.reset()\n",
        "    for t in range(100):\n",
        "        env.render()\n",
        "#         print(observation)\n",
        "        action = env.action_space.sample()\n",
        "        observation, reward, done, info = env.step(action)\n",
        "        if done:\n",
        "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
        "            break\n",
        "env.close()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode finished after 12 timesteps\n",
            "Episode finished after 13 timesteps\n",
            "Episode finished after 14 timesteps\n",
            "Episode finished after 21 timesteps\n",
            "Episode finished after 23 timesteps\n",
            "Episode finished after 12 timesteps\n",
            "Episode finished after 16 timesteps\n",
            "Episode finished after 9 timesteps\n",
            "Episode finished after 12 timesteps\n",
            "Episode finished after 15 timesteps\n",
            "Episode finished after 24 timesteps\n",
            "Episode finished after 17 timesteps\n",
            "Episode finished after 11 timesteps\n",
            "Episode finished after 18 timesteps\n",
            "Episode finished after 28 timesteps\n",
            "Episode finished after 17 timesteps\n",
            "Episode finished after 31 timesteps\n",
            "Episode finished after 33 timesteps\n",
            "Episode finished after 12 timesteps\n",
            "Episode finished after 14 timesteps\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUc-QXiIkcTg",
        "colab_type": "code",
        "outputId": "28b47ced-b814-4cf9-c016-9e3004134cd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        }
      },
      "source": [
        "show_video()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYVnIp_fcSux",
        "colab_type": "text"
      },
      "source": [
        "## Setup the Gridworld Environment\n",
        "https://github.com/dennybritz/reinforcement-learning/blob/master/lib/envs/gridworld.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hr2NvZGjlK69",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "from collections import defaultdict\n",
        "from gym.envs.toy_text import discrete"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPUJfJdoB-Zn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "UP = 0\n",
        "RIGHT = 1\n",
        "DOWN = 2\n",
        "LEFT = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnEep269cZ3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GridworldEnv(discrete.DiscreteEnv):\n",
        "  \"\"\"\n",
        "  The Gridworld environment that is a discrete space\n",
        "  \n",
        "  Grid World environment from Sutton's Reinforcement Learning book chapter 4.\n",
        "  You are an agent on an MxN grid and your goal is to reach the terminal\n",
        "  state at the top left or the bottom right corner.\n",
        "\n",
        "  For example, a 4x4 grid looks as follows:\n",
        "\n",
        "  T  o  o  o\n",
        "  o  x  o  o\n",
        "  o  o  o  o\n",
        "  o  o  o  T\n",
        "\n",
        "  x is your position and T are the two terminal states.\n",
        "\n",
        "  You can take actions in each direction (UP=0, RIGHT=1, DOWN=2, LEFT=3).\n",
        "  Actions going off the edge leave you in your current state.\n",
        "  You receive a reward of -1 at each step until you reach a terminal state.\n",
        "  \n",
        "  A toy text discrete environment has the followings:\n",
        "  1. nS: Number of states\n",
        "  2. nA: Number of actions\n",
        "  3. P: transitions (*)\n",
        "  4. isd: initial state distribution\n",
        "  \n",
        "  (*) dictionary dict of dicts of lists, where\n",
        "    P[s][a] == [(probability, nextstate, reward, done), ...]\n",
        "  (**) list or array of length nS\n",
        "\n",
        "  \"\"\"\n",
        "  \n",
        "  metadata = {'render.modes': ['human', 'ansi']}\n",
        "  \n",
        "  def __init__(self, shape=[4, 4]):\n",
        "    if not isinstance(shape, (list, tuple)) or not len(shape) == 2:\n",
        "      raise ValueError('shape argument must be a list/tuple of length 2')\n",
        "      \n",
        "    self.shape = shape\n",
        "    \n",
        "    # The grid world has n x n states\n",
        "    nS = np.prod(shape)\n",
        "    \n",
        "    # There are only 4 possible actions\n",
        "    actions = [UP, RIGHT, DOWN, LEFT]\n",
        "    nA = len(actions)\n",
        "    \n",
        "    # Define the maximum board shape\n",
        "    MAX_Y = shape[0]\n",
        "    MAX_X = shape[1]\n",
        "    \n",
        "    # Define terminate state\n",
        "    is_done = lambda s: s == 0 or s == (nS - 1)\n",
        "    \n",
        "    \n",
        "    # Initialise P\n",
        "    P = {}\n",
        "    \n",
        "    grid = np.arange(nS).reshape(shape)\n",
        "    it = np.nditer(grid, flags=['multi_index'])\n",
        "    \n",
        "    while not it.finished:\n",
        "      s = it.iterindex\n",
        "      y, x = it.multi_index\n",
        "      \n",
        "      # Define Reward is -1 for each step\n",
        "      reward = 0.0 if is_done(s) else -1.0\n",
        "      \n",
        "      # Initialize P\n",
        "      P[s] = {a: [] for a in actions}\n",
        "      \n",
        "      if is_done(s):\n",
        "        for a in actions:\n",
        "          P[s][a] = [(1.0, s, reward, True)]\n",
        "      else:\n",
        "        \n",
        "        # Define the next state of each state\n",
        "        ns_up = s if y == 0 else s - MAX_X\n",
        "        ns_right = s if x == (MAX_X - 1) else s + 1\n",
        "        ns_down = s if y == (MAX_Y - 1) else s + MAX_X\n",
        "        ns_left = s if x == 0 else s - 1\n",
        "        \n",
        "        P[s][UP] = [(1.0, ns_up, reward, is_done(ns_up))]\n",
        "        P[s][RIGHT] = [(1.0, ns_right, reward, is_done(ns_right))]\n",
        "        P[s][DOWN] = [(1.0, ns_down, reward, is_done(ns_down))]\n",
        "        P[s][LEFT] = [(1.0, ns_left, reward, is_done(ns_left))]\n",
        "        \n",
        "        \n",
        "      it.iternext()\n",
        "      \n",
        "      # Define Initial state distribution is uniform\n",
        "      isd = np.ones(nS) / nS\n",
        "      \n",
        "      self.P = P\n",
        "      \n",
        "      \n",
        "      super().__init__(nS, nA, P, isd)\n",
        "    \n",
        "    \n",
        "  def render(self, mode='human', close=False):\n",
        "    \"\"\"\n",
        "    Render the environment\n",
        "    \"\"\"\n",
        "    if close:\n",
        "      return\n",
        "    \n",
        "    outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
        "    \n",
        "    grid = np.arange(self.nS).reshape(self.shape)\n",
        "    it = np.nditer(grid, flags=['multi_index'])\n",
        "    \n",
        "    while not it.finished:\n",
        "      s = it.iterindex\n",
        "      y, x = it.multi_index\n",
        "      \n",
        "      if self.s == s:\n",
        "        output = \"x \"\n",
        "      elif s == 0 or s == self.nS - 1:\n",
        "        output = \"T \"\n",
        "      else:\n",
        "        output = \"o \"\n",
        "        \n",
        "      if x == 0:\n",
        "        output = output.lstrip()\n",
        "        \n",
        "      if x == self.shape[1] - 1:\n",
        "        output = output.rstrip()\n",
        "\n",
        "      outfile.write(output)\n",
        "      \n",
        "      if x == self.shape[1] - 1:\n",
        "        outfile.write(\"\\n\")\n",
        "        \n",
        "      it.iternext()\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfK1HvfUgq0H",
        "colab_type": "code",
        "outputId": "de1b8151-55a0-4748-e102-cd14aa3a4553",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "env = GridworldEnv()\n",
        "for t in range(100):\n",
        "    env.render()\n",
        "#         print(observation)\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
        "        break\n",
        "env.close()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "T o o o\n",
            "o o o o\n",
            "o o o o\n",
            "o o o x\n",
            "Episode finished after 1 timesteps\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAA5WbvcJJdp",
        "colab_type": "code",
        "outputId": "2148daa3-6f11-43dd-af86-52aea0ba0399",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "env.P[1]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [(1.0, 1, -1.0, False)],\n",
              " 1: [(1.0, 2, -1.0, False)],\n",
              " 2: [(1.0, 5, -1.0, False)],\n",
              " 3: [(1.0, 0, -1.0, True)]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SMo1yCulGft",
        "colab_type": "text"
      },
      "source": [
        "## Solving Gridworld by dynamic programming\n",
        "We will solve a Bellman equation using two algorithms:\n",
        "1. Value iteration\n",
        "2. Policy iteration\n",
        "\n",
        "Q(s,a) = Transition probability * ( Reward probability + gamma * value_of_next_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hho44WOAq869",
        "colab_type": "text"
      },
      "source": [
        "### Value Iteration\n",
        "In value iteration, we start off with a random value function, then look for a new improved value function in iterative fashion until we find the optimal value function.\n",
        "\n",
        "1. Initialise random value function\n",
        "2. For each state, calculate Q(s, a)\n",
        "3. Since V(s) = Max W(s, a), update the value function with max value of Q(s, a)\n",
        "4. If V(S) is optimal,  then stop. Repeat otherwise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKMVvAUiCgDC",
        "colab_type": "text"
      },
      "source": [
        "#### Environment Inspection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZ7hq1AZq8Gs",
        "colab_type": "code",
        "outputId": "f84f2a90-f7f6-4628-f72f-db4df36937fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(env.observation_space.n)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Egcs01ElCpqJ",
        "colab_type": "code",
        "outputId": "31267ffc-cb43-4bdc-b78f-0bc095e04da9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(env.action_space.n)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS0IutbRDu09",
        "colab_type": "text"
      },
      "source": [
        "#### Initialise the value table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF7lrWT-CslQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "value_table = np.zeros(env.observation_space.n)\n",
        "no_of_iterations = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk341PepKzy8",
        "colab_type": "code",
        "outputId": "ed002d41-d79e-440a-fc3d-ff2487dfa316",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "value_table"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClBe9xT-Er6z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upon starting each iteration, we copy the value_table to updated_value_table\n",
        "for i in range(no_of_iterations):\n",
        "  updated_value_table = np.copy(value_table)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuzOqkJxImwI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for state in range(env.observation_space.n):\n",
        "  Q_value = []\n",
        "  \n",
        "  for action in range(env.action_space.n):\n",
        "    next_states_rewards = []\n",
        "    for next_sr in env.P[state][action]:\n",
        "      trans_prob, next_state, reward_prob, _ = next_sr\n",
        "      next_states_rewards.append((trans_prob * (reward_prob + 1 * updated_value_table[next_state])))\n",
        "      Q_value.append(np.sum(next_states_rewards))\n",
        "      \n",
        "      # Pick up the maximum Q value and update it as value of a state\n",
        "      value_table[state] = max(Q_value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0k600LTK1_d",
        "colab_type": "code",
        "outputId": "de120adb-d241-4d0f-fc0a-5a0c8096b99a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "value_table.reshape((4, 4))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0., -1., -1., -1.],\n",
              "       [-1., -1., -1., -1.],\n",
              "       [-1., -1., -1., -1.],\n",
              "       [-1., -1., -1.,  0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGvtjT-jNIML",
        "colab_type": "text"
      },
      "source": [
        "#### Combine them into a function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_S1w2MMNHNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def value_iteration(env, gamma=1.0, no_of_iterations=100000):\n",
        "  \"\"\"\n",
        "  Value iteration function\n",
        "  \"\"\"\n",
        "  \n",
        "  # Initialise value tables\n",
        "  value_table = np.zeros(env.observation_space.n)\n",
        "  threshold = 1e-20\n",
        "  \n",
        "  print(no_of_iterations)\n",
        "  \n",
        "  # Table update\n",
        "  for i in range(no_of_iterations):\n",
        "    \n",
        "    # Copy the current value table to updated value table\n",
        "    updated_value_table = np.copy(value_table)\n",
        "    \n",
        "    for state in range(env.observation_space.n):\n",
        "      print(\"state: %s\" % state, end=\"\")\n",
        "      # For each of the state, calculate the state-action value q(s,a)\n",
        "      Q_value = []\n",
        "      \n",
        "      for action in range(env.action_space.n):\n",
        "        # One step ahead search for each action\n",
        "        next_states_rewards = []\n",
        "        \n",
        "        for next_sr in env.P[state][action]:\n",
        "          # P[s][a] == [(probability, nextstate, reward, done), ...]\n",
        "#           print(next_sr)\n",
        "          trans_prob, next_state, reward, _ = next_sr\n",
        "          next_states_rewards.append((trans_prob * (reward + gamma * updated_value_table[next_state])))\n",
        "          \n",
        "        Q_value.append(np.sum(next_states_rewards))\n",
        "          \n",
        "      # Pick the maximum Q value and update it as value of a state\n",
        "      value_table[state] = max(Q_value)\n",
        "    \n",
        "    print(\"Diff: %s\" % np.sum(np.fabs(updated_value_table - value_table)))\n",
        "\n",
        "    if (np.sum(np.fabs(updated_value_table - value_table))) <= threshold:\n",
        "      print(\"Value-iteration converged at iteration %d\" % (i + 1))\n",
        "      break\n",
        "\n",
        "  return value_table"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlvzvAJgWExf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = GridworldEnv()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fuxMVv5Uxee",
        "colab_type": "code",
        "outputId": "d9519aad-de36-46db-f0ac-100c7b0a181b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "optimal_value_function = value_iteration(env=env, gamma=1.0)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100000\n",
            "state: 0state: 1state: 2state: 3state: 4state: 5state: 6state: 7state: 8state: 9state: 10state: 11state: 12state: 13state: 14state: 15Diff: 14.0\n",
            "state: 0state: 1state: 2state: 3state: 4state: 5state: 6state: 7state: 8state: 9state: 10state: 11state: 12state: 13state: 14state: 15Diff: 10.0\n",
            "state: 0state: 1state: 2state: 3state: 4state: 5state: 6state: 7state: 8state: 9state: 10state: 11state: 12state: 13state: 14state: 15Diff: 4.0\n",
            "state: 0state: 1state: 2state: 3state: 4state: 5state: 6state: 7state: 8state: 9state: 10state: 11state: 12state: 13state: 14state: 15Diff: 0.0\n",
            "Value-iteration converged at iteration 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNvERZAAUxch",
        "colab_type": "code",
        "outputId": "f68aea69-ca76-4437-bd6d-9df9c0743705",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "optimal_value_function.reshape((4,4))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0., -1., -2., -3.],\n",
              "       [-1., -2., -3., -2.],\n",
              "       [-2., -3., -2., -1.],\n",
              "       [-3., -2., -1.,  0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lak2emOp2H2",
        "colab_type": "text"
      },
      "source": [
        "#### Extracting the optimal policy\n",
        "After finding optimal value function, how can we extract the optimal policy from optimal function?  \n",
        "We calculate the Q value using our optimal value action and pick up actions greedily for each state as the optimal policy.\n",
        "\n",
        "We do this via a function called `extract_policy()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oR-oNf0wy6-O",
        "colab_type": "text"
      },
      "source": [
        "#### Build a Q table for each state\n",
        "\n",
        "A Q table looks like the following:\n",
        "\n",
        "|State|Action|Value|\n",
        "|--------|-----------|--------|\n",
        "State 1|Action 1|Value 1|\n",
        "State 1|Action 2|Value 2|\n",
        "State 1|Action 3|Value 3|\n",
        "State 1|Action 4|Value 4|\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6i7ZBvaysCM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_policy(value_table, gamma=1.0):\n",
        "  \n",
        "  # First, Define a random policy pi\n",
        "  policy = np.zeros(env.observation_space.n)\n",
        "  \n",
        "  # Build a Q table - One step ahead\n",
        "  for state in range(env.observation_space.n):\n",
        "    # For each state, the Q table has num_actions \n",
        "    Q_table = np.zeros(env.action_space.n)\n",
        "\n",
        "    for action in range(env.action_space.n):\n",
        "\n",
        "      for next_sr in env.P[state][action]:\n",
        "        # One step look ahead\n",
        "        trans_prob, next_state, reward, _ = next_sr\n",
        "\n",
        "        new_value = trans_prob * (reward + gamma * value_table[next_state])\n",
        "\n",
        "        Q_table[action] += new_value\n",
        "        \n",
        "    policy[state] =  np.argmax(Q_table)\n",
        "    \n",
        "  return policy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaMXyqoLsB4s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimal_policy = extract_policy(optimal_value_function)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9Pqn_SxvMOy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "21fd065d-ba0b-46ae-d41c-cba4d8b64653"
      },
      "source": [
        "optimal_policy"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 3., 3., 2., 0., 0., 0., 2., 0., 0., 1., 2., 0., 1., 1., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KXl5f5En8Vn",
        "colab_type": "text"
      },
      "source": [
        "### Policy Iteration\n",
        "1. Policy evaluation: Evaluating the value function of a randomly estimated policy\n",
        "2. Policy improvement: Upon evaluating the value function, if it is not optimal, we find a new improved policy $\\pi'$\n",
        "\n",
        "##### How can we evaluate the policies?  \n",
        "We will evaluate our randomly initialized policies by computing value functions for them. If they are not good, then we find a new policy.   \n",
        "We repeat this process until we find a good policy.\n",
        "\n",
        "#### Steps\n",
        "1. Initialize random policy $\\pi$\n",
        "2. Calculate value function V(S) for the policy\n",
        "3. If V(S) is optimal -> End, otherwise find improved policy\n",
        "4. Repeat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgELiBo0YPmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gamma = 1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMUTsXluYrV0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a value table with the number of states\n",
        "value_table = np.zeros(env.nS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXrFxMRswvjp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For each state, we get the action from policy, and compute the value function \n",
        "# according to the `action` and `state` as folows\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpiXsHgLwvM6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YOY5g1bpiIs",
        "colab_type": "text"
      },
      "source": [
        "#### Combining them into `extract_policy` function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-TvwkUepgrn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}