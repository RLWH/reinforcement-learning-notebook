{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Value Function Approximation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "jio89h3c0vyy",
        "hfaNy7YusKgv",
        "bwiyyIiNth8D"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RLWH/reinforcement-learning-notebook/blob/master/5.%20Value%20Approximation/Value_Function_Approximation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEqAnQ71hvOR",
        "colab_type": "text"
      },
      "source": [
        "# 5. Value Approximation\n",
        "\n",
        "Basically, all the model-free methods require either value function or state-value function, that essentially they are a one-to-one mapping function for a given state S. These algorithms might be able to solve small to medium problems, but they may not be able to solve large problems like Backgammon ($10^{20}$ states), Computer Go ($10^{170}$ states), or other problems that have a continuous state space. \n",
        "\n",
        "So, how to scale up the model-free methods? The answer is by function approximation. \n",
        "\n",
        "#### The feature weights\n",
        "In this chapter, we will introduce a new weight vector $\\vec{w} \\in \\mathop{\\mathbb{R^d}}$.\n",
        "\n",
        "Hence, we will rewrite the value function into $\\hat{v}(s, \\vec{w}) \\approx v_{\\pi}(s)$, which means the value function uses the weight vector to approximate the true value function. On the other hand, we can also approximate state-value function $\\hat{q}(s, a, \\vec{w}) \\approx q_{\\pi}(s,a)$. The goal of having the feature weights is to generalise the learnings from seen states to unseen states. \n",
        "\n",
        "On the side note, extending reinforcement learning to function approximation also makes it applicable to partially observable problems, where the full state is not available to the agent. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1r5sJNRUuDjd",
        "colab_type": "text"
      },
      "source": [
        "# Value-function Approximation\n",
        "\n",
        "In order to use SGD to approximate a value function, we need to first define the objective function. \n",
        "\n",
        "The goal of SGD is to find a parameter vector $w$ that minimise the mean-squared error between the approximate value function $\\hat{v}(s, \\vec w)$ and the true value function $v_{\\pi}(s)$\n",
        "Of course, the states are not equally important, therefore, we are obligated to specify a state distribution $\\mu(s) \\geq 0, \\sum_{s} = 1$, to represent how much we care about the error in each state. \n",
        "\n",
        "Suppose the true value $v_{\\pi}(s)$ exists, the formulation of the objective function, also known as the *Mean Squared Value Error*, becomes\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "J(\\vec w) & = \\sum_s\\mu(s)[v_{\\pi}(s) - \\hat{v}(s, \\vec{w})]^2 \\\\\n",
        "& = \\mathop{\\mathbb{E_{\\pi}}}[(v_{\\pi}(s) - \\hat{v}(s, \\vec{w}))]\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "for on-policy distribution in episodic tasks.\n",
        "\n",
        "Ideally, the goal in terms of $J(\\vec{w})$ would be to find a global optimum, where a weight vector $\\vec{w^*}$ such that $J(\\vec{w^*}) \\leq J(\\vec{w})$ for all possible $\\vec{w}. $\n",
        "\n",
        "## Stochastic gradient descent methods\n",
        "\n",
        "Suppose the weight vector is a column vector with a fixed number of real valued components, $\\vec{w} = (w_1, w_2, w_3, ..., w_d)^{T}$, and the approximate value function $\\hat{v}(s, \\vec{w})$ is a differentiable function of $\\vec{w}$ for all $s \\in S$. \n",
        "\n",
        "The weight vector $\\vec{w}$ will be updated at each of the discrete time steps, for $t=0, 1, 2, 3, ...$, so we instead use $\\vec{w_t}$ to represent the weight vector.\n",
        "\n",
        "The SGD method adjusts the weight vector at each time step, or after each example, by a small amount in the direction that would most reduce the error on that example.\n",
        "\\begin{equation}\n",
        "\\Delta{\\vec{w}} = \\alpha [v_{\\pi}(S_t) - \\hat{v}(S,\\vec{w_t})] \\nabla{\\hat{v}}(S_t, \\vec{w_t})\n",
        "\\end{equation},\n",
        "where $\\alpha$ is a positive step-size parameter, and $\\nabla f(\\vec{w})$, for any scalar expression $f(\\vec{w})$ that is a function of a vector, denotes the column vector of partial derivatives of the expression w.r.t the components of the vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82Cpn3Qz1Cf2",
        "colab_type": "text"
      },
      "source": [
        "## Incremental Prediction Algorithms\n",
        "\n",
        "In the previous discussion we have assumed the true value $v_{\\pi}$ has been given. However, in reinforcement learning problem, there is no supervisor, only rewards. Thus, in practice, we need to substitute a target for $v_{\\pi}(s)$. We can denote $U_t \\in \\mathop{\\mathbb{R}}$, of the *t*th training example, such that $S_t \\mapsto U_t$. We have to be aware of it that $U_t$ is not the true value, $v_{\\pi}(S_t)$, but some, possibly random, approximation to it. \n",
        "\n",
        "Thus, in general the gradient of $\\vec{w}$ can be rewritten as\n",
        "\\begin{equation}\n",
        "\\Delta{\\vec{w}} = \\alpha [U_t - \\hat{v}(S,\\vec{w_t})] \\nabla_{\\vec{w}}{\\hat{v}}(S_t, \\vec{w_t})\n",
        "\\end{equation}\n",
        "\n",
        "$U_t$ can be approximated by various methods:\n",
        "\n",
        "### 1. Monte Carlo\n",
        "For Monte Carlo method, the target is the return $G_t$, or we can assign $U_t = G_t$.\n",
        "\n",
        "The gradient of the weight then become\n",
        "\\begin{equation}\n",
        "\\Delta{\\vec{w}} = \\alpha [G_t - \\hat{v}(S,\\vec{w_t})] \\nabla_{\\vec{w}}{\\hat{v}}(S_t, \\vec{w_t})\n",
        "\\end{equation}\n",
        "\n",
        "##### Pseudo Code\n",
        "---\n",
        "```\n",
        "Input: The policy pi to be evaluated\n",
        "Input: a differentiable function v_hat that takes in state and weights and map to a value: S x Rd -> R\n",
        "\n",
        "Algorithm parameter: stepsize alpha > 0\n",
        "Initialise value-function weights w (dimension d) arbitrarily\n",
        "\n",
        "Loop forever (for each episode):\n",
        "        Generate an episode S0, A0, R1, S1, A1, ..., RT, ST using pi\n",
        "        Loop for each step of episode, t=0, 1, ..., T-1:\n",
        "                w = w + alpha * (Gt - vhat(st, w)) * grad_vhat(st, w)\n",
        "```\n",
        "---\n",
        "\n",
        "### 2. TD(0)\n",
        "\n",
        "For one-step TD, the target $U_t = R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\vec{w})$\n",
        "\n",
        "The gradient of the weight then becomes\n",
        "\\begin{equation}\n",
        "\\Delta{\\vec{w}} = \\alpha [R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\vec{w}) - \\hat{v}(S,\\vec{w_t})] \\nabla_{\\vec{w}}{\\hat{v}}(S_t, \\vec{w_t})\n",
        "\\end{equation}\n",
        "\n",
        "There is no guarantee to converge by using a bootstrapping method like TD(0) or TD($\\lambda$), as it relies on a bootstraping estimate to update the gradient. Bootstraping methods, in fact, are not the instences of true gradient descent (Barnard, 1993), as they all depend on the current value of the weight vector $\\vec{w_t}$, which imples that they will be biased and that they will not produce a true gradient-descent method. These methods are called *semi-gradient* methods.\n",
        "\n",
        "Although semi-gradient methods do not converge as robustly as gradient methods, they do converge reliably in important cases such as the linear cases. Another advantage for semi-gradient methods is that they enable learning to be coninual and online, without waiting for the end of an episode.\n",
        "\n",
        "##### Pseudo Code\n",
        "---\n",
        "```\n",
        "Input: The policy pi to be evaluated\n",
        "Input: A differentiable function v_hat(state, weights) |-> R such that v_hat(terminal, .) = 0\n",
        "Algorithm parameter: step size alpha > 0\n",
        "Initialise value-function weights w as a d-dimensional zero vectors\n",
        "\n",
        "Loop for each episode:\n",
        "        Initialise S\n",
        "        Loop for each step of episode:\n",
        "                Choose A ~ pi(.|S)\n",
        "                Take action A, observe R, S'\n",
        "                w = w + alpha * (R + gamma * v_hat(S', w) - v_hat(S, w)) * grad_vhat(S, w)\n",
        "        until S is terminal\n",
        "```\n",
        "---\n",
        "\n",
        "### 3. TD($\\lambda$)\n",
        "\n",
        "For TD lambda, the target is the $\\lambda$-return $G_t^{\\lambda}$. i.e. $U_t = G_t^{\\lambda}$\n",
        "\n",
        "The gradient becomes\n",
        "\\begin{equation}\n",
        "\\Delta{\\vec{w}} = \\alpha [G_t^{\\lambda} - \\hat{v}(S,\\vec{w_t})] \\nabla_{\\vec{w}}{\\hat{v}}(S_t, \\vec{w_t})\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxVbzPwDqlOH",
        "colab_type": "text"
      },
      "source": [
        "## Linear Methods of Value Approximation\n",
        "\n",
        "One of the most important special cases of function approximation is that, where the approximate function, $\\hat{v}(\\cdot, \\vec{w})$, is a linear function of the weight vector, $\\vec{w}$\n",
        "\n",
        "Denote a real-valued feature vector\n",
        "\\begin{equation}\n",
        "\\vec{x}(s) = (x_1(s), x_2(s), ..., x_d(s))^{T}\n",
        "\\end{equation}\n",
        "with the same number of components as $\\vec{w}$\n",
        "\n",
        "Linear methods approximate the state-value function by the inner product between $\\vec{w}$ and $\\vec{x}$, such that \n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "\\hat{v}(s, \\vec{w}) & = \\vec{w} ^T \\vec{x}(s) \\\\\n",
        "& = \\sum_{i=1}^{d}w_i x_i(s)\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "Note: Feature vector can be anything that tells you about the state space. For example:\n",
        "- Distance of robot from landmarks\n",
        "- Trends in the stock market\n",
        "- Piece and pawn configurations in chess\n",
        "\n",
        "\n",
        "The update rule is particularly simple with Linear Value Function Approximation\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "\\nabla_{\\vec{w}}\\hat{v}(s, \\vec{w}) &= \\vec{x}(s) \\\\\n",
        "\\Delta \\vec{w} &= \\alpha[v_{\\pi}(s) - \\hat{v}(s, \\vec{w})] \\vec{x}(s)\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "In other words:\n",
        "**Update = step-size x prediction error x feature value**\n",
        "\n",
        "Since in the linear case there is only one optimum, and thus any method  that is guaranteed to converge to or near a local optimum is automatically guaranteed to converge to or near the global optimum. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4GcN3pZI1v9",
        "colab_type": "text"
      },
      "source": [
        "### Monte Carlo with Linear Methods\n",
        "\n",
        "- The return $G_t$ is an unbiased, noisy sample of true value $v_{\\pi}(s_t)$\n",
        "- The \"training data\"\n",
        "\n",
        "\\begin{equation}\n",
        "\\langle S_1, G_1\\rangle, \\langle S_2, G_2\\rangle, ..., \\langle S_T, G_T\\rangle\n",
        "\\end{equation}\n",
        "\n",
        "- The gradient used for MC policy evaluation\n",
        "\\begin{equation}\n",
        "\\Delta{\\vec{w}} = \\alpha [G_t^{\\lambda} - \\hat{v}(S,\\vec{w_t})] \\vec{x}(S_t)\n",
        "\\end{equation}\n",
        "\n",
        "The gradient MC algorithm converges to the global optimum of $J$ under linear function approximation if $\\alpha$ is reduced over time according to the usual conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AHwZ9mGK1o5",
        "colab_type": "text"
      },
      "source": [
        "### TD(0) with Linear Methods\n",
        "- The TD-target $R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\vec{w})$ is a biased sample of true value $v_{\\pi}(s_t)$\n",
        "- This can still apply supervised learning to the \"training data\"\n",
        "\\begin{equation}\n",
        "\\langle S_1, R_2 + \\gamma \\hat{v}(S_2, \\vec{w})\\rangle, \\langle S_2, R_3 + \\gamma \\hat{v}(S_3, \\vec{w})\\rangle, ..., \\langle S_{T-1}, R_T + \\gamma \\hat{v}(S_T, \\vec{w})\\rangle\n",
        "\\end{equation}\n",
        "- The gradient used for TD(0) policy evaluation\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "\\Delta{\\vec{w}} &= \\alpha [R + \\gamma \\hat{v}(s_{t+1}, \\vec{w}) - \\hat{v}(S,\\vec{w_t})] \\vec{x}(S_t) \\\\\n",
        "&=\\alpha \\delta \\vec{x}(s)\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "- Linear TD(0) converges close to the global optimum, which is called the *TD Fixed point* (Proof in p.206), where\n",
        "\\begin{equation}\n",
        "J(\\vec{w}_{TD}) \\leq \\frac{1}{1-\\gamma} \\min_\\vec{w} J(\\vec{w})\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKw1d3cuTJTH",
        "colab_type": "text"
      },
      "source": [
        "###  Forward-view linear TD($\\lambda$) and Backward-view linear TD($\\lambda$)\n",
        "\n",
        "- The $\\lambda$-return $G_t^{\\lambda}$ is also a biased sample of true value $v_{\\pi}(s)$\n",
        "- Again, it can apply supervised learning to \"training data\"\\begin{equation}\n",
        "\\langle S_1, G_1^{\\lambda}\\rangle, \\langle S_2, G_2^{\\lambda}\\rangle, ..., \\langle S_{T-1}, G_{T-1}^{\\lambda}\\rangle\n",
        "\\end{equation}\n",
        "\n",
        "#### Forward view linear TD($\\lambda$)\n",
        "\\begin{equation}\n",
        "\\Delta{\\vec{w}} = \\alpha [G_t^{\\lambda} - \\hat{v}(S,\\vec{w_t})] \\vec{x}(S_t)\n",
        "\\end{equation}\n",
        "\n",
        "#### Backward vieww linear TD($\\lambda$)\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "& \\delta_t = R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\vec{w}) - \\hat{v}(S_t, \\vec{w}) \\\\\n",
        "& E_t = \\gamma \\lambda E_{t-1} + \\vec{x}(S_t) \\\\\n",
        "& \\Delta{\\vec{w}} = \\alpha \\delta_t E_t\n",
        "\\end{split}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hC7SXc6TaHhR",
        "colab_type": "text"
      },
      "source": [
        "# Implementation - 1000 state random walk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcMimES-bdg2",
        "colab_type": "text"
      },
      "source": [
        "We revisit a random walk problem, but this time we add more states into the environment. Instead of having 5 states in between, we now have 1000 states, and the starting state will be the 500th state\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jGq7R--jupw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "\n",
        "from collections import defaultdict\n",
        "from gym.envs.toy_text import discrete"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxQbmvg7ELhi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LEFT = 0\n",
        "RIGHT = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5h_trYYfKPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RandomWalkEnv(discrete.DiscreteEnv):\n",
        "    \"\"\"\n",
        "    A Random Walk environment for Example 9.1\n",
        "    \"\"\"\n",
        "    \n",
        "    metadata = {'render.modes': ['human', 'ansi']}\n",
        "    \n",
        "    \n",
        "    def __init__(self, nS, start_state):\n",
        "        nS = nS\n",
        "        \n",
        "        actions = [LEFT, RIGHT]\n",
        "        nA = len(actions)\n",
        "        self.start_state = start_state\n",
        "        \n",
        "        # Calculate transition probabilities and rewards\n",
        "        P = {}\n",
        "        \n",
        "        for s in range(nS):\n",
        "            \n",
        "            P[s] = None\n",
        "            \n",
        "            # At each state, left = right = 0.5\n",
        "            # and then distribute to 100 states\n",
        "                \n",
        "            # Initialise all the vectors\n",
        "            all_states = np.arange(nS)\n",
        "            all_probs = np.zeros(nS)\n",
        "\n",
        "            all_rewards = np.zeros(nS)\n",
        "            all_rewards[0] = -1\n",
        "            all_rewards[nS - 1] = 1\n",
        "\n",
        "            all_done = np.zeros(nS)\n",
        "            all_done[0] = 1\n",
        "            all_done[nS - 1] = 1\n",
        "\n",
        "            if s == 0 or s == (nS - 1):\n",
        "                continue\n",
        "\n",
        "            # Update the probs\n",
        "            all_probs[max(1, s-100):s] = 0.5/100\n",
        "            if (s - 100) < 0:\n",
        "                all_probs[0] = 0.5 - abs(s - 1) * (0.5/100)\n",
        "\n",
        "            all_probs[(s + 1):min(nS, s + 1 + 100)] = 0.5/100\n",
        "            if (s + 100) > (nS - 1):\n",
        "                all_probs[nS - 1] = 0.5 - abs(nS - (s + 2)) * (0.5/100)\n",
        "\n",
        "            # Convert to zipped list\n",
        "            psa = list(zip(all_probs, all_states, all_rewards, all_done))\n",
        "\n",
        "            P[s] = psa\n",
        "        \n",
        "        isd = np.zeros(nS)\n",
        "        isd[start_state] = 1\n",
        "        \n",
        "        super(RandomWalkEnv, self).__init__(nS, nA, P, isd)\n",
        "        \n",
        "    def step(self):\n",
        "        transitions = self.P[self.s]\n",
        "#         print(transitions)\n",
        "        probs = np.array([t[0] for t in transitions])\n",
        "        i = np.random.choice(np.flatnonzero(probs == probs.max()))\n",
        "#         print(i)\n",
        "#         i = categorical_sample([t[0] for t in transitions], self.np_random)\n",
        "        p, s, r, d= transitions[i]\n",
        "        self.s = s\n",
        "#         self.lastaction = a\n",
        "        return (s, r, d, {\"prob\" : p})\n",
        "     \n",
        "    def render(self, mode='human'):\n",
        "        outfile = sys.stdout\n",
        "        \n",
        "        output = \"Current position: %s\" % self.s\n",
        "        \n",
        "        if self.s == 0 or self.s == (self.nS - 1):\n",
        "            output += \" - Terminate.\"\n",
        "            \n",
        "        outfile.write(output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bJK-tRbu22_",
        "colab_type": "text"
      },
      "source": [
        "#### Small test on the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPnxBTYgM9Ln",
        "colab_type": "code",
        "outputId": "fe9c308e-1f9e-4aa6-d81b-a9065cfb16d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "env = RandomWalkEnv(1002, 500)\n",
        "env.render()\n",
        "# env.P[500][RIGHT]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current position: 500"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxMTXXk63BRs",
        "colab_type": "code",
        "outputId": "ae9cdb78-2381-46ef-978c-5472bca77d92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "env.step()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(839, 0.0, 0.0, {'prob': 0.005})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 247
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_Fae0CGUaMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dynamics_to_probability(psa):\n",
        "    \"\"\"\n",
        "    Converting a list of tuple into sum of probability\n",
        "    \n",
        "    Args:\n",
        "        psa: [(probability, nextstate, reward, done), ...]\n",
        "    \"\"\"\n",
        "    \n",
        "    psa_np = np.array(psa)\n",
        "    \n",
        "    probs = psa_np[:,0]\n",
        "    nss = psa_np[:, 1]\n",
        "    rewards = psa_np[:, 2]\n",
        "    \n",
        "    return probs, nss, rewards"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jio89h3c0vyy",
        "colab_type": "text"
      },
      "source": [
        "#### Estimate the true value by dynamic programming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ki7CwNt0CdG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def value_estimation(env, gamma=1.0, threshold=1e-3):\n",
        "    \"\"\"\n",
        "    Iterative Policy Evaluation for estimating V = Vpi\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialise \n",
        "    nS = env.observation_space.n\n",
        "    V = np.zeros(nS)\n",
        "    \n",
        "    n_iter = 0\n",
        "    \n",
        "    while True:\n",
        "        \n",
        "        delta = 0\n",
        "        \n",
        "        for s in range(nS):\n",
        "            \n",
        "            if s == 0 or s == (nS-1):\n",
        "                continue\n",
        "            \n",
        "            old_v = V[s]\n",
        "            v = 0\n",
        "\n",
        "            # Actions are either left or right\n",
        "            probs, nss, rewards = dynamics_to_probability(env.P[s])\n",
        "\n",
        "            bellman = rewards + gamma * V\n",
        "            prob_bellman = np.dot(probs, bellman.T)\n",
        "\n",
        "            v += prob_bellman\n",
        "                        \n",
        "            V[s] = v\n",
        "            delta = max(delta, abs(old_v - V[s]))\n",
        "        \n",
        "        if n_iter % 10 == 0:\n",
        "            print(\"Iteration %s done; delta = %s\" % (n_iter, delta))\n",
        "    \n",
        "        \n",
        "        if delta < threshold:\n",
        "            break\n",
        "        \n",
        "        n_iter += 1\n",
        "\n",
        "    return V"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhcmCv1IadN9",
        "colab_type": "code",
        "outputId": "a994bac1-d41c-42e4-e23e-5fe767411368",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "V = value_estimation(env)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 0 done; delta = 0.6466659121736846\n",
            "Iteration 10 done; delta = 0.023880212740720386\n",
            "Iteration 20 done; delta = 0.007487071465396289\n",
            "Iteration 30 done; delta = 0.0029639424751838894\n",
            "Iteration 40 done; delta = 0.0014797939353352607\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsdhIVv4qhhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk_sR6awpavJ",
        "colab_type": "code",
        "outputId": "047bd840-3446-4a9a-cf8c-a61d16b21ed1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "lp = sns.lineplot(x=np.arange(env.observation_space.n), y=V)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfaNy7YusKgv",
        "colab_type": "text"
      },
      "source": [
        "#### Approximate by Gradient MC method and state aggregation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwiyyIiNth8D",
        "colab_type": "text"
      },
      "source": [
        "#### What is state aggregation?\n",
        "State aggregation is a simple form of generalising function approximation in which states are grouped together, with one estimated value (one component of the weight vector $\\vec{w}$) for each group. The value of a state is estimated as its group's component, and when the state is updated, the component alone is updated.\n",
        "\n",
        "i.e.\n",
        "1. State aggregation is a special case of SGD in which the gradient  is \n",
        "\n",
        "\\begin{equation}\n",
        "\\nabla \\hat{v}(S_t, \\vec{w}_t) =\n",
        "\\begin{cases}\n",
        "1, \\text{for } S_t \\text{'s component} \\\\\n",
        "0,  \\text{otherwise}\n",
        "\\end{cases}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxRmXbYZ6pxf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKygHniR9Atc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_episode_random(policy, env, verbose=False):\n",
        "    \"\"\"\n",
        "    Generate a sequence according to the policy\n",
        "    \"\"\"\n",
        "    \n",
        "    states, actions, rewards = [], [], []\n",
        "    \n",
        "    observation = env.reset()\n",
        "    while True:\n",
        "        if verbose:\n",
        "            print(observation)\n",
        "        states.append(observation)\n",
        "#         action = policy(observation, env)\n",
        "#         if verbose:\n",
        "#             print(\"Action taken: %s\" % action)\n",
        "#         actions.append(action)\n",
        "        observation, reward, done, info = env.step()\n",
        "        rewards.append(reward)\n",
        "        \n",
        "        if done:           \n",
        "            if verbose:\n",
        "                print(\"Game end. End state: %s; Reward: %s\" % (observation, reward))\n",
        "            break\n",
        "            \n",
        "    return states, rewards\n",
        "\n",
        "\n",
        "def random_policy(observations, env):\n",
        "    return env.action_space.sample()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHDx4D0sA8GQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def func_v(s, w):\n",
        "    \"\"\"\n",
        "    Construct a differentiable function v\n",
        "\n",
        "    Args:\n",
        "        s: State\n",
        "        w: d-dimension weight vector\n",
        "\n",
        "    Return:\n",
        "        v, grad_v\n",
        "    \"\"\"\n",
        "\n",
        "    assert s != 0\n",
        "    assert s != 1001\n",
        "    \n",
        "    if s % 100 == 0:\n",
        "        s_split = (s // 100) - 1\n",
        "    else:\n",
        "        s_split = s // 100\n",
        "\n",
        "    grad_v = np.zeros(10)\n",
        "    grad_v[s_split] = 1\n",
        "\n",
        "    return w[s_split], grad_v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ3aa9ou73v-",
        "colab_type": "code",
        "outputId": "741f6653-61d1-45b0-ee96-12fee7f9f91d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "func_v(58, np.zeros(10))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 266
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yc8llHzksJyV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def grad_mc_with_state_aggregation(env, policy, func_v,\n",
        "                                   alpha=2e-5, gamma=1.0, num_episodes=10000):\n",
        "    \"\"\"\n",
        "    Gradient MC method with state aggregation\n",
        "    \n",
        "    1. Separate the 100 states into 10 groups of 100 states each (excluding terminal states)\n",
        "    2. Apply the Gradient Monte Carlo Algorithm\n",
        "    \"\"\"\n",
        "    \n",
        "    nS = env.observation_space.n\n",
        "    w = np.zeros(DIM_W)\n",
        "    \n",
        "    for i in range(num_episodes):\n",
        "        \n",
        "        if i % 5000 == 0:\n",
        "            print(\"Iteration %s. Current w: %s\" % (i, w))\n",
        "        \n",
        "        states, rewards = generate_episode_random(policy, env)\n",
        "#         print(states)\n",
        "        \n",
        "        G = np.sum(rewards)\n",
        "#         print(G)\n",
        "        \n",
        "        for t in range(len(states)):\n",
        "            \n",
        "            if states[t] == 0 or states[t] == 1001:\n",
        "                break\n",
        "            \n",
        "            v_hat, grad_v = func_v(states[t], w)\n",
        "#             G = gamma * G + rewards[t]\n",
        "            \n",
        "            # Update w\n",
        "            w += alpha * (G - v_hat) * grad_v\n",
        "            \n",
        "    return w  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGY1kPnX6ua6",
        "colab_type": "code",
        "outputId": "90ff8f64-ad2f-4b1b-848b-1e89a31eb529",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "env = RandomWalkEnv(1002, 500)\n",
        "env.render()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current position: 500"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaTWk-efBiAd",
        "colab_type": "code",
        "outputId": "722debe2-7234-4c03-d4d3-44af122877de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1394
        }
      },
      "source": [
        "%%time\n",
        "w = grad_mc_with_state_aggregation(env, random_policy, func_v, num_episodes=200000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 0. Current w: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Iteration 5000. Current w: [-0.04863309 -0.16440897 -0.21729649 -0.17233404 -0.04503898  0.08806633\n",
            "  0.20180109  0.23988676  0.1751122   0.0506517 ]\n",
            "Iteration 10000. Current w: [-0.09628126 -0.30482712 -0.37042697 -0.26964865 -0.0870015   0.11563398\n",
            "  0.29094406  0.38019193  0.30627435  0.09764906]\n",
            "Iteration 15000. Current w: [-0.14112455 -0.40654129 -0.44085385 -0.29059225 -0.08209179  0.12379076\n",
            "  0.32394865  0.44882546  0.40510621  0.14254737]\n",
            "Iteration 20000. Current w: [-0.18392733 -0.48903045 -0.5031689  -0.33401853 -0.11318743  0.10267385\n",
            "  0.31675131  0.48020953  0.48239803  0.18527813]\n",
            "Iteration 25000. Current w: [-0.22520883 -0.55518839 -0.53062269 -0.34090285 -0.12196573  0.0866263\n",
            "  0.3055423   0.50211376  0.54503656  0.22536821]\n",
            "Iteration 30000. Current w: [-0.26387062 -0.60984951 -0.55669986 -0.34933284 -0.11442278  0.1033756\n",
            "  0.32318281  0.53722365  0.59861017  0.26376717]\n",
            "Iteration 35000. Current w: [-0.30014879 -0.6440596  -0.56004318 -0.34207877 -0.10449686  0.10710805\n",
            "  0.32813903  0.55128492  0.6432144   0.30075953]\n",
            "Iteration 40000. Current w: [-0.33505297 -0.67436709 -0.56988878 -0.35079545 -0.11628695  0.10592822\n",
            "  0.32925973  0.55131856  0.67411715  0.33552954]\n",
            "Iteration 45000. Current w: [-0.3689355  -0.70202662 -0.58226016 -0.36097926 -0.11977906  0.10595626\n",
            "  0.32180899  0.54984991  0.69602171  0.36793647]\n",
            "Iteration 50000. Current w: [-0.39985897 -0.71968229 -0.57172884 -0.350557   -0.09882461  0.11643764\n",
            "  0.3380035   0.54839567  0.7098703   0.39956513]\n",
            "Iteration 55000. Current w: [-0.42945561 -0.73300718 -0.56555337 -0.3406735  -0.10186947  0.11707826\n",
            "  0.3441636   0.55788498  0.72453741  0.42998498]\n",
            "Iteration 60000. Current w: [-0.457167   -0.73751539 -0.54546091 -0.30537573 -0.06929696  0.15429031\n",
            "  0.36846762  0.58044471  0.74810576  0.45874615]\n",
            "Iteration 65000. Current w: [-0.48384097 -0.74918598 -0.55424102 -0.32509193 -0.08379568  0.14276296\n",
            "  0.36734692  0.57832959  0.75632013  0.48603875]\n",
            "Iteration 70000. Current w: [-0.50974993 -0.75532297 -0.56039129 -0.33999845 -0.10576737  0.1120156\n",
            "  0.3467006   0.57560323  0.76544374  0.51135925]\n",
            "Iteration 75000. Current w: [-0.53384145 -0.76294628 -0.55792176 -0.3142421  -0.07198041  0.14449839\n",
            "  0.36633611  0.58301273  0.7724779   0.53598705]\n",
            "Iteration 80000. Current w: [-0.55660631 -0.76642896 -0.55857948 -0.3297774  -0.09265705  0.12104365\n",
            "  0.34464398  0.5672101   0.77194024  0.55932741]\n",
            "Iteration 85000. Current w: [-0.5785839  -0.77263965 -0.56528703 -0.34183206 -0.11773477  0.09664166\n",
            "  0.33901105  0.56446768  0.77528319  0.58113822]\n",
            "Iteration 90000. Current w: [-0.59956253 -0.77780399 -0.57032411 -0.3564048  -0.11309267  0.10950769\n",
            "  0.33475216  0.56572547  0.77679935  0.60187333]\n",
            "Iteration 95000. Current w: [-0.61944934 -0.77490719 -0.55197492 -0.32493299 -0.08262255  0.12889439\n",
            "  0.34940302  0.56996347  0.78197764  0.62167244]\n",
            "Iteration 100000. Current w: [-0.6382665  -0.77257871 -0.5499631  -0.32071099 -0.07690359  0.15061382\n",
            "  0.35617666  0.57505287  0.78279767  0.64055761]\n",
            "Iteration 105000. Current w: [-0.65644435 -0.77625959 -0.55695795 -0.34358753 -0.11448373  0.11013411\n",
            "  0.34283956  0.57541449  0.78354137  0.65794244]\n",
            "Iteration 110000. Current w: [-0.67360551 -0.78039094 -0.56763421 -0.35575539 -0.12275117  0.10738103\n",
            "  0.342977    0.56200409  0.78115726  0.67471021]\n",
            "Iteration 115000. Current w: [-0.69000036 -0.7863737  -0.56863782 -0.35922784 -0.13092252  0.0884933\n",
            "  0.33220729  0.56169326  0.77729553  0.69060557]\n",
            "Iteration 120000. Current w: [-0.70533831 -0.78498191 -0.56478266 -0.33958285 -0.10740692  0.10984495\n",
            "  0.35034094  0.56567054  0.77840331  0.70587796]\n",
            "Iteration 125000. Current w: [-0.72001903 -0.78805232 -0.56699336 -0.33638782 -0.10825944  0.10525\n",
            "  0.34539192  0.5664004   0.77949331  0.72017255]\n",
            "Iteration 130000. Current w: [-0.73409447 -0.78775588 -0.56750806 -0.34686354 -0.10853569  0.1147607\n",
            "  0.33956556  0.56334503  0.77930754  0.73391659]\n",
            "Iteration 135000. Current w: [-0.74728725 -0.78815187 -0.56379178 -0.3426054  -0.10908835  0.11709941\n",
            "  0.34708149  0.5686904   0.78294528  0.74707748]\n",
            "Iteration 140000. Current w: [-0.75995216 -0.78901457 -0.57750976 -0.35771123 -0.11927197  0.11979905\n",
            "  0.34918918  0.57391057  0.78863749  0.75941578]\n",
            "Iteration 145000. Current w: [-0.77194689 -0.78390215 -0.57293046 -0.35093622 -0.11322337  0.11243061\n",
            "  0.34147141  0.57126457  0.78898627  0.77133691]\n",
            "Iteration 150000. Current w: [-0.78307278 -0.78815681 -0.57298325 -0.34898359 -0.10587412  0.11087175\n",
            "  0.34557154  0.57886785  0.79043754  0.78287223]\n",
            "Iteration 155000. Current w: [-0.7940632  -0.78874225 -0.58029004 -0.3579473  -0.12457447  0.10331481\n",
            "  0.34164694  0.57295822  0.79256304  0.79326898]\n",
            "Iteration 160000. Current w: [-0.80438466 -0.78998771 -0.57924964 -0.36056908 -0.13045336  0.08740412\n",
            "  0.3211807   0.56764091  0.79365669  0.80317655]\n",
            "Iteration 165000. Current w: [-0.81404145 -0.79138052 -0.58067048 -0.3692049  -0.12961612  0.07365336\n",
            "  0.30985816  0.56373701  0.79409049  0.81281533]\n",
            "Iteration 170000. Current w: [-0.82314688 -0.78397631 -0.56787989 -0.34672625 -0.10826515  0.09880228\n",
            "  0.33179494  0.57292711  0.79173377  0.82211261]\n",
            "Iteration 175000. Current w: [-0.83202112 -0.78367898 -0.56659965 -0.34063237 -0.09852969  0.10358946\n",
            "  0.33526604  0.56214293  0.79180734  0.83069679]\n",
            "Iteration 180000. Current w: [-0.84041342 -0.7854224  -0.56712156 -0.34500064 -0.10143003  0.11539491\n",
            "  0.34482023  0.56212263  0.78715688  0.83896453]\n",
            "Iteration 185000. Current w: [-0.84821014 -0.78581375 -0.56380953 -0.33751926 -0.09829013  0.11026995\n",
            "  0.33352174  0.56686404  0.7883687   0.84693804]\n",
            "Iteration 190000. Current w: [-0.8556676  -0.78791528 -0.5613261  -0.33093275 -0.09596515  0.12323259\n",
            "  0.34248543  0.56878137  0.78850247  0.85437049]\n",
            "Iteration 195000. Current w: [-0.86267421 -0.78147997 -0.54784708 -0.31405184 -0.07711938  0.14556025\n",
            "  0.3546892   0.57096752  0.78912083  0.86154689]\n",
            "CPU times: user 35min 55s, sys: 303 ms, total: 35min 55s\n",
            "Wall time: 35min 55s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2q7hQifKBzhL",
        "colab_type": "code",
        "outputId": "556312d7-95a7-4d50-e6a6-89d1ea589ae5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "w"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.86945075, -0.78101032, -0.55508357, -0.3435995 , -0.10807082,\n",
              "        0.1164297 ,  0.34210257,  0.56684471,  0.78756985,  0.86821532])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 278
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUy1IKHwquzx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mc_approx = [func_v(x, w)[0] for x in np.arange(1, env.observation_space.n - 1)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb_p7Dr2r0-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNOBdbXMro2x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_preproc = pd.DataFrame({\"states\": np.arange(1, env.observation_space.n - 1),\n",
        "                             \"dp_value\": V[1:-1],\n",
        "                             \"mc_approx\": mc_approx})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_n72VHUqcbz",
        "colab_type": "code",
        "outputId": "63978144-931d-4222-a3fa-f072879bfdc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "lp = sns.lineplot(x='states', y='value', hue='variable', \n",
        "                  data=pd.melt(data_preproc, [\"states\"]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEKCAYAAAA1qaOTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofZDGOhgWXGZ",
        "colab_type": "text"
      },
      "source": [
        "# Action-Value $Q(s,a)$ Function Approximation\n",
        "\n",
        "For policy control, we need to approximate the action-value function such that $\\hat{q}(s, a, \\vec{w}) \\approx q_*(s,a)$, where $\\vec{w} \\in \\mathop{\\mathbb{R}}^d$ is a finite-dimensional weight vector. \n",
        "\n",
        "In this section, we will cver the on-policy methods only."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R4sGDtWZvNy",
        "colab_type": "text"
      },
      "source": [
        "### From approximating state value to action value\n",
        "\n",
        "The objective is to approximate the action value function\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{q}(S, A, \\vec{w}) \\approx q_{\\pi}(S, A)\n",
        "\\end{equation}\n",
        "\n",
        "We can use the same approach as approximating state value function, but change into action-value. Suppose there is a true action-value $q_{\\pi}(S,A)$\n",
        "\n",
        "The objective function becomes\n",
        "\\begin{equation}\n",
        "J(\\vec{w}) = \\mathop{\\mathbb{E_{\\pi}}}[(q_{\\pi}(S,A) - \\hat{q}(S, A, \\vec{w}))^2]\n",
        "\\end{equation}\n",
        "\n",
        "And, by using stochastic gradient descent to find a local minimum\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "-\\frac{1}{2} \\nabla_{\\vec{w}} J(\\vec{w}) &= (q_{\\pi}(S,A) - \\hat{q}(S, A, \\vec{w})) \\nabla_{\\vec{w}}\\hat{q}(S,A,\\vec{w})\\\\\n",
        "\\Delta \\vec{w} &= \\alpha [q_{\\pi}(S,A) - \\hat{q}(S,A,\\vec{w})] \\nabla_{\\vec{w}}\\hat{q}(S,A,\\vec{w})\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "### Approximation by using Linear methods\n",
        "Similarly, the action-value function can be represented by linear combinations of features\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{q}(S, A, \\vec{w}) = \\vec{x}(S,A)^T \\vec{w} = \\sum_{j=1}^{n} x_j(S,A) w_j\n",
        "\\end{equation}\n",
        "\n",
        "### The update process\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "\\nabla_{\\vec{w}} \\hat{q}(S, A, \\vec{w}) &= \\vec{x}(S,A) \\\\\n",
        "\\Delta \\vec{w} &= \\alpha [q_{\\pi}(S,A) - \\hat{q}(S, A, \\vec{w})]\\vec{x}(S,A)\n",
        "\\end{split}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "318WsaoqgpOL",
        "colab_type": "text"
      },
      "source": [
        "## Incremental Control Algorithms\n",
        "\n",
        "Like prediction, we also need to substitute a target for $q_{\\pi}(S,A)$\n",
        "\n",
        "### 1. Monte Carlo Methods\n",
        "For Monte Carlo method, the target is the return $G_t$, or we can assign $U_t = G_t$.\n",
        "\n",
        "The gradient of the weight then become\n",
        "\\begin{equation}\n",
        "\\Delta{\\vec{w}} = \\alpha [G_t - \\hat{q}(S_t,A_t,\\vec{w_t})] \\nabla_{\\vec{w}}{\\hat{q}}(S_t, A_t, \\vec{w_t})\n",
        "\\end{equation}\n",
        "\n",
        "### 2. SARSA(0) Methods\n",
        "\n",
        "This method is called *episodic semi-gradient one-step SARSA*. For a constant policy, this method converges in the same way that TD(0) does, with the same kind of error bound.\n",
        "\n",
        "\\begin{equation}\n",
        "\\Delta{\\vec{w}} = \\alpha [R_{t+1} + \\gamma \\hat{q}(S_{t+1}, A_{t+1}, \\vec{w}) - \\hat{q}(S_t,A_t, \\vec{w_t})] \\nabla_{\\vec{w}}{\\hat{q}}(S_t, A_t, \\vec{w_t})\n",
        "\\end{equation}\n",
        "\n",
        "In order to form control methods, we need to plug the predicted action-value into the GPI framework. That is, for each possible action $a$ available in the current state $S_t$, we can compute $\\hat{q}(S_t, A_t, \\vec{w})$ and then find the greedy action $A_t^* = \\text{argmax}_a \\hat{q}(S_t, a, \\vec{w}_{t-1})$. Considering the on-policy method, the policy improvement can be done by changing the estimation policy to a soft approximation of the greedy policy, such as $\\epsilon$-greedy policy. \n",
        "\n",
        "##### Pseudocode\n",
        "---\n",
        "```\n",
        "Input: a differentiable action-value function parameterisation q_hat(state, action, weights) |-> score\n",
        "Algorithm parameters: stepsize alpha > 0, small eps > 0\n",
        "Initialise value-function weights (D-dimensional zero vector)\n",
        "\n",
        "Loop for each episode:\n",
        "       Initialise State and action of episode (eps-greedy)\n",
        "       \n",
        "       Loop for each step of episode:\n",
        "            Take action A, observe R, S'\n",
        "            \n",
        "            If S' is terminal:\n",
        "                    w = w + alpha * (R - q_hat(S, A, w)) * grad_q(S, A, w)\n",
        "                    break\n",
        "                    \n",
        "            Choose A' as a function of q_hat(S', ., w) (e.g. eps-greedy):\n",
        "                    w = w + alpha * (R + gamma * q_hat(S', A', w) - q(S', A', w)) * grad_q(S, A, w)\n",
        "                    \n",
        "                    S = S'\n",
        "                    A = A'\n",
        "```\n",
        "---\n",
        "\n",
        "### 3. SARSA($\\lambda$)\n",
        "\n",
        "##### Forward-view\n",
        "\\begin{equation}\n",
        "\\Delta w = \\alpha (q_t^{\\lambda} - \\hat{q}(S_t, A_t, \\vec{w})) \\nabla_{\\vec{w}} \\hat{q}(S_t, A_t, \\vec{w})\n",
        "\\end{equation}\n",
        "\n",
        "##### Backward-view\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "\\delta_t &= R_{t+1} + \\gamma \\hat{q}(S_{t+1}, A_{t+1}, \\vec{w}) - \\hat{q}(S_t, A_t, \\vec{w}) \\\\\n",
        "E_t &= \\gamma \\lambda E_{t-1} + \\nabla_{\\vec{w}}\\hat{q}(S_t, A_t, \\vec{w}) \\\\\n",
        "\\Delta{\\vec{w}} &= \\alpha \\delta_t E_t\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "### 4. Gradient TD\n",
        "Not covered here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-vw5dgLBnZS",
        "colab_type": "text"
      },
      "source": [
        "## Summary\n",
        "![Prediction convergence summary](https://raw.githubusercontent.com/RLWH/reinforcement-learning-notebook/master/images/prediction_algos_convergence_summary.png)\n",
        "\n",
        "![Control convergence summary](https://raw.githubusercontent.com/RLWH/reinforcement-learning-notebook/master/images/control_algo_convergence_summary.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsJQtslnBiEr",
        "colab_type": "text"
      },
      "source": [
        "# Exploiting the samples of data by using batch update\n",
        "\n",
        "## Stochastic Gradient Descent with Experience Replay\n",
        "\n",
        "Given expereince consisting of $\\langle \\text{state}, \\text{value} \\rangle$ pairs\n",
        "\\begin{equation}\n",
        "D = \\{\\langle s_1, v_1^{\\pi} \\rangle\\, \\langle s_2, v_2^{\\pi} \\rangle\\, ..., \\langle s_T, v_T^{\\pi} \\rangle\\}\n",
        "\\end{equation}\n",
        "\n",
        "##### Algorithm\n",
        "---\n",
        "```\n",
        "Repeat:\n",
        "\n",
        "    1. Sample state, value from experience\n",
        "    2. Apply stochastic gradient descent update\n",
        "    w = w + alpha * (v_pi - v_hat(s, w)) * grad_w v(s, w)\n",
        "    \n",
        "Until it converges to least square solution\n",
        "```\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbnVDi0sIXt4",
        "colab_type": "text"
      },
      "source": [
        "## Deep Q Network\n",
        "\n",
        "Boostraping methods, as discussed previously, are not stable when it comes to non-linear approximation methods. DQN uses two tricks to stabalise with NN\n",
        "1. Experience Replay\n",
        "2. Fixed Q-targets\n",
        "\n",
        "Method\n",
        "1. Take action $a_t$ according to $\\epsilon$-greedy policy\n",
        "2. Store transition ($s_t, a_t, r_{t+1}, s_{t+1}$) in replay memory $D$\n",
        "3. Sample random mini-batch of transitions ($s, a, r, s'$) from $D$\n",
        "4. **Compute Q-learning targets w.r.t old, fixed parameters $w^-$**\n",
        "5. Optimise MSE between Q-network and Q-learning agents\n",
        "\\begin{equation}\n",
        "LS(\\vec{w_i}) = \\mathop{\\mathbb{E_{s, a, r, s'}}} \\Big[ (r + \\gamma \\max_{a'}Q(s', a'; w_i^{-}) - Q(s, a; w_i))^2 \\Big]\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cY0GrKgnQnig",
        "colab_type": "text"
      },
      "source": [
        "## One-step Linear Least Square Method for Linear approximators\n",
        "\n",
        "If the approximator is a linear approximator, we can use an analytical method to find out the least square method. \n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "\\mathop{\\mathbb{E}}_D[\\Delta \\vec{w}] &= 0\\\\\n",
        "\\alpha\\sum_{t=1}^{T} \\vec{x}(s_t)(v_t^{\\pi} - x(s_t)^T\\vec{w}) &= 0 \\\\\n",
        "\\sum_{t=1}^{T} \\vec{x}(s_t)v_t^{\\pi} &= \\sum_{t=1}^{T} \\vec{x}(s_t) \\vec{x}(s_t)^T\\vec{w} \\\\\n",
        "\\vec{w} &= \\Big( \\sum_{t=1}^T \\vec{x}(s_t) \\vec{x}(s_t)^T \\Big)^{-1} \\sum_{t=1}^T \\vec{x}(s_t)v_t^{\\pi}\n",
        "\\end{split}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-yk-2t1Wb5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}