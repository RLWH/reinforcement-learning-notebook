{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Cartpole v0 by DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "    env.render()\n",
    "    action = env.action_space.sample() # your agent here (this takes random actions)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        print(\"Episode finished\")\n",
    "        observation = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 - Solving by using DQN with $\\epsilon$-greedy policy\n",
    "\n",
    "Checklist:\n",
    "1. Objective function\n",
    "2. Preprocess data\n",
    "3. Samples generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space 4\n",
      "Action space 2\n"
     ]
    }
   ],
   "source": [
    "# Environment understanding\n",
    "print(\"State space\", env.observation_space.shape[0])\n",
    "print(\"Action space\", env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The policy network\n",
    "The network takes in the state of the game and decide what we should do. \n",
    "\n",
    "For simplicity, use a simple 2-layer NN that takes in the observations and then produce a single number indicating the probability of pushing LEFT or RIGHT. It is standard to use a stochastic policy, meaning that the NN will only produce a probability of each action. \n",
    "\n",
    "We are going to train our model with a single experience:\n",
    "1. Let the model estimate Q values of the old state\n",
    "2. Let the model estimate Q values of the new state\n",
    "3. Calculate the new target Q value for the action, using the known reward\n",
    "4. Train the model with input = (old state), output = (target Q values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3\n",
    "INITIAL_EPSILON = 0.95\n",
    "EPSILON_DECAY_RATE = 0.995\n",
    "MIN_EPSILON = 0.01\n",
    "GAMMA = 0.95\n",
    "C = 10  # Update the network parameters every C iteration\n",
    "MEMORY_CAPACITY = 100000  # Capacity of experience replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo code\n",
    "---\n",
    "```\n",
    "Initialise replay memory D to capacity N\n",
    "Initialise action-value function Q with random weights\n",
    "Initialise target action-value function Q_hat with weights_hat = weights\n",
    "\n",
    "For episode = 1, M:\n",
    "    Reset environment and get initial state\n",
    "    Preprocess initial state phi1 = phi(s1)\n",
    "    For t = 1, T:\n",
    "        Use epsilon-greedy policy to select an action\n",
    "        Execute action, observe states and rewards\n",
    "        Store transition S, A, R, S' (Inside function step())\n",
    "        Sample random minibatch of transitions from experience D (Inside function step())\n",
    "        Calculate TD target and TD error (Inside function step())\n",
    "        Perform a gradient descent step on TD error (Inside function step())\n",
    "        For every C steps reset Q_hat = Q\n",
    "    End For\n",
    "End For\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Policy Network\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, fc1_units=24, fc2_units=24):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        Essentially, the forward pass return the Q value\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "def loss_fn(output, labels):\n",
    "    \"\"\"\n",
    "    Compute the loss given outputs and labels\n",
    "    \n",
    "    Args:\n",
    "        outputs (Variable)\n",
    "        labels (Variable)\n",
    "    \"\"\"\n",
    "    return torch.nn.MSELoss(reduction=\"sum\")(output, labels)\n",
    "\n",
    "def some_measurement(outputs, labels):\n",
    "    \"\"\"\n",
    "    Compute the performance measurement, given the outputs and labels for all images\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\"Experience\", \"s a r s_ done\")\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    A Deep Q learning agent\n",
    "    https://github.com/udacity/deep-reinforcement-learning/blob/master/dqn/exercise/dqn_agent.py\n",
    "    https://towardsdatascience.com/reinforcement-learning-tutorial-part-3-basic-deep-q-learning-186164c3bf4\n",
    "    https://morvanzhou.github.io/tutorials/machine-learning/torch/4-05-DQN/\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env, loss_fcn, learning_rate=LEARNING_RATE, gamma=GAMMA):\n",
    "        \n",
    "        # Environment parameters\n",
    "        self.env = env\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.n_state = env.observation_space.shape[0]\n",
    "        \n",
    "        # NN - Q function with random parameters\n",
    "        # The main_net is used for training at every step - weights is theta\n",
    "        # The target_net is used for prediction at every step - weights is theta^neg\n",
    "        self.main_net, self.target_net = self.create_model(), self.create_model()\n",
    "        self.loss_fcn = loss_fcn\n",
    "        self.optimizer = torch.optim.Adam(self.main_net.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Set the weights of target_net equals to main_net\n",
    "        self.target_net.load_state_dict(self.main_net.state_dict())\n",
    "        \n",
    "        # Experience replay\n",
    "        self.experience_memory = deque(maxlen=MEMORY_CAPACITY)\n",
    "        \n",
    "        # Other parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Counters\n",
    "        self.target_update_counter = 0\n",
    "    \n",
    "    def create_model(self):\n",
    "        return QNetwork(self.n_state, self.n_actions)\n",
    "    \n",
    "    def act(self, states, epsilon):\n",
    "        \"\"\"\n",
    "        Epsilon Greedy Policy\n",
    "        if eps = 0 -> Greedy policy\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Variable): features\n",
    "        \"\"\"\n",
    "        states_torch = torch.FloatTensor(states)\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            action_value = self.main_net.forward(states_torch)\n",
    "            action = torch.argmax(action_value).item()\n",
    "            return action\n",
    "    \n",
    "    def step(self, s, a, r, s_pi, done):\n",
    "        \"\"\"\n",
    "        Something that we should do for each step\n",
    "        \"\"\"\n",
    "        # Save the experience in replay memory\n",
    "        self.experience_memory.append(Experience(s, a, r, s_pi, done))\n",
    "        \n",
    "        # Start learning when there are enough samples\n",
    "        if len(self.experience_memory) > BATCH_SIZE:\n",
    "            sample_experiences = sample(self.experience_memory, BATCH_SIZE)\n",
    "            self.learn(sample_experiences, GAMMA)\n",
    "    \n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"\n",
    "        Update value parameters using given batch of experience tuples\n",
    "        \n",
    "        Args:\n",
    "            experiences (list of Experience tuple)\n",
    "            gamma (float)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Sample the data from experience memory\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "        \n",
    "        states_torch = torch.FloatTensor(states)\n",
    "        actions_torch = torch.FloatTensor(actions)\n",
    "        rewards_torch = torch.FloatTensor(rewards)\n",
    "        next_states_torch = torch.FloatTensor(next_states)\n",
    "        dones_torch = torch.FloatTensor(dones)\n",
    "        \n",
    "        # Calculate the new Q value\n",
    "        q_main = self.main_net(states_torch).gather(1, actions_torch.long().view(-1, 1))\n",
    "        q_target_next = self.target_net(next_states_torch).detach().max(dim=1)[0].view(-1, 1)\n",
    "        \n",
    "        # Calculate TD target\n",
    "        td_target = rewards_torch.view(-1, 1) + gamma * q_target_next * (1 - dones_torch.view(-1, 1))\n",
    "#         print(td_target)\n",
    "        \n",
    "        loss = self.loss_fcn(td_target, q_main)\n",
    "        # Gradient descent\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def update_target_net_weights(self):\n",
    "        main_net_state_dict = self.main_net.state_dict()\n",
    "        self.target_net.load_state_dict(main_net_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 \t Average Score: 38.0 \t Epsilon: 0.9452499999999999\n",
      "Episode 100 \t Average Score: 32.7029702970297 \t Epsilon: 0.57260450509286038\n",
      "Episode 200 \t Average Score: 101.68656716417911 \t Epsilon: 0.34686688098665924\n",
      "Episode 300 \t Average Score: 119.98338870431894 \t Epsilon: 0.21012170189946602\n",
      "Episode 400 \t Average Score: 127.45386533665835 \t Epsilon: 0.12728551507581423\n",
      "Episode 500 \t Average Score: 140.09780439121755 \t Epsilon: 0.07710580202642309\n",
      "Episode 600 \t Average Score: 152.01497504159732 \t Epsilon: 0.046708415349514534\n",
      "Episode 700 \t Average Score: 159.98430813124108 \t Epsilon: 0.028294577154065315\n",
      "Episode 800 \t Average Score: 166.8676654182272 \t Epsilon: 0.0171400183529387232\n",
      "Episode 900 \t Average Score: 172.42397336293007 \t Epsilon: 0.010382916399118782\n",
      "Episode 1000 \t Average Score: 177.0 \t Epsilon: 0.011silon: 0.010024920157445967\n",
      "Episode 1100 \t Average Score: 180.16167120799273 \t Epsilon: 0.01\n",
      "Episode 1200 \t Average Score: 186.27810158201498 \t Epsilon: 0.01\n",
      "Episode 1300 \t Average Score: 196.52651806302845 \t Epsilon: 0.01\n",
      "Episode 1354 \t Average Score: 200.00516605166052 \t Epsilon: 0.01\n",
      "Environment solved in 1354 episodes! \tAverage Score: 200.00516605166052\n"
     ]
    }
   ],
   "source": [
    "# Initialise the agent\n",
    "agent = DQNAgent(env, loss_fcn=loss_fn)\n",
    "\n",
    "scores = []\n",
    "scores_window = deque(maxlen=100)\n",
    "\n",
    "epsilon = INITIAL_EPSILON\n",
    "\n",
    "for i in range(NUM_EPISODES):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    t = 0\n",
    "    while not done:\n",
    "        action = agent.act(state, epsilon)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "        # For every C time, update Q\n",
    "        if t % C:\n",
    "#             print(\"Update weights\")\n",
    "            agent.update_target_net_weights()\n",
    "        \n",
    "        t += 1\n",
    "        score += reward\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Epsilon decay\n",
    "    epsilon = max(MIN_EPSILON, EPSILON_DECAY_RATE * epsilon)\n",
    "    scores.append(score)\n",
    "    scores_window.append(scores)\n",
    "    \n",
    "    print(\"\\rEpisode %s \\t Average Score: %s \\t Epsilon: %s\" % (i, np.mean(scores_window), epsilon), end=\"\")\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"\\rEpisode %s \\t Average Score: %s \\t Epsilon: %s\" % (i, np.mean(scores_window), epsilon))\n",
    "        \n",
    "    if np.mean(scores_window) >= 200:\n",
    "        print(\"\\nEnvironment solved in %s episodes! \\tAverage Score: %s\" % (i, np.mean(scores_window)))\n",
    "        torch.save(agent.main_net.state_dict(), \"checkpoint.pth\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve the cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_agent = DQNAgent(env, loss_fcn=loss_fn)\n",
    "trained_agent.main_net.load_state_dict(torch.load(\"./checkpoint.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "\n",
    "done = False\n",
    "total_return = 0\n",
    "timestep = 0\n",
    "\n",
    "while not done:\n",
    "    env.render()\n",
    "    action = trained_agent.act(observation, epsilon=0) # your agent here (this takes random actions)\n",
    "    next_observation, reward, done, info = env.step(action)\n",
    "    \n",
    "    total_return += reward\n",
    "    timestep += 1\n",
    "\n",
    "    if done:\n",
    "        print(\"Episode finished\")\n",
    "        observation = env.reset()\n",
    "        \n",
    "    observation = next_observation \n",
    "        \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci",
   "language": "python",
   "name": "datasci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
